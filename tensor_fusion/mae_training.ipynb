{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b684eea4-53bf-41f3-bc3d-6293380241e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "import numpy as np\n",
    "from model import SubNet, TextSubNet, TFN\n",
    "#from datasets import MultimodalDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "79ed6fe3-6a66-4975-88dd-958054a597c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cmu_mosi_dataset(path='../../dataset/cmu-mosi/mosi_20_seq_data.pkl'):\n",
    "    \n",
    "    file = open(path, 'rb')\n",
    "    data = pickle.load(file)\n",
    "    file.close()\n",
    "    \n",
    "    # features: (batch_size, seq_length, input_size)\n",
    "    # audio and vision features are averaged across time to (batch_size, input_size)\n",
    "    # labels: (batch_size, 1)\n",
    "    text = torch.tensor(data['train']['text'], dtype=torch.float32)\n",
    "    audio = torch.tensor(data['train']['audio'], dtype=torch.float32).mean(dim=1)\n",
    "    vision = torch.tensor(data['train']['vision'], dtype=torch.float32).mean(dim=1)\n",
    "    labels = torch.tensor(data['train']['labels'], dtype=torch.float32).squeeze(1)\n",
    "    train_set = MultimodalDataset(text, audio, vision, labels)\n",
    "\n",
    "    text = torch.tensor(data['valid']['text'], dtype=torch.float32)\n",
    "    audio = torch.tensor(data['valid']['audio'], dtype=torch.float32).mean(dim=1)\n",
    "    vision = torch.tensor(data['valid']['vision'], dtype=torch.float32).mean(dim=1)\n",
    "    labels = torch.tensor(data['valid']['labels'], dtype=torch.float32).squeeze(1)\n",
    "    valid_set = MultimodalDataset(text, audio, vision, labels)\n",
    "    \n",
    "    text = torch.tensor(data['test']['text'], dtype=torch.float32)\n",
    "    audio = torch.tensor(data['test']['audio'], dtype=torch.float32).mean(dim=1)\n",
    "    vision = torch.tensor(data['test']['vision'], dtype=torch.float32).mean(dim=1)\n",
    "    labels = torch.tensor(data['test']['labels'], dtype=torch.float32).squeeze(1)\n",
    "    test_set = MultimodalDataset(text, audio, vision, labels)\n",
    "    \n",
    "    return train_set, valid_set, test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "949413b0-e1e8-4d5c-8154-d6c0530a2da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultimodalDataset(Dataset):\n",
    "    '''\n",
    "    Dataset for CMU-MOSI\n",
    "    '''\n",
    "    def __init__(self, text, audio, vision, labels):\n",
    "        '''\n",
    "        args:\n",
    "            text: text modality feature of shape (N, seq. length, text_input_size)\n",
    "            audio: audio modality feature of shape (N, seq. length, audio_input_size)\n",
    "            vision: vision modality feature of shape (N, seq. length, vision_input_size)\n",
    "            labels: labels of shape (N, 1) and ranges from -3 to 3\n",
    "        '''\n",
    "        self.text = text\n",
    "        self.audio = audio\n",
    "        self.vision = vision\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        '''\n",
    "        Returns an individual data composed of (text, audio, vision, label)\n",
    "\n",
    "        Returns:\n",
    "            text: text modality feature of shape (seq. length, text_input_size)\n",
    "            audio: audio modality feature of shape (audio_input_size)\n",
    "            vision: vision modality feature of shape (vision_input_size)\n",
    "            label: a scalar label that ranges from -3 to 3\n",
    "        '''\n",
    "        text = self.text[idx]\n",
    "        audio = self.audio[idx]\n",
    "        vision = self.vision[idx]\n",
    "        label = self.labels[idx]\n",
    "        return text, audio, vision, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "34c993f7-48eb-4ed3-84ed-656dd5f31bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set, valid_set, test_set = get_cmu_mosi_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "97792882-72c9-4a43-8b85-ebef742aa0eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "epochs = 50\n",
    "# settings from https://github.com/Justin1904/TensorFusionNetworks/blob/master/train.py\n",
    "input_dims = (5, 20, 300)\n",
    "hidden_dims = (4, 16, 128)\n",
    "text_out = 64\n",
    "dropouts = (0.3, 0.3, 0.3, 0.3)\n",
    "post_fusion_dim = 32\n",
    "model = TFN(input_dims, hidden_dims, text_out, dropouts, post_fusion_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "63cbd265-5bd8-43b6-b162-9d0afaac6652",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "valid_dataloader = DataLoader(valid_set, batch_size=len(valid_set))\n",
    "test_dataloader = DataLoader(test_set, batch_size=len(test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b48406cc-82d3-4667-8420-1dfbc5de4a5a",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "52.85571324825287\n",
      "1.2533843517303467\n",
      "Epoch 1\n",
      "42.93867760896683\n",
      "1.1115158796310425\n",
      "Epoch 2\n",
      "37.203013837337494\n",
      "1.0741556882858276\n",
      "Epoch 3\n",
      "35.8412299156189\n",
      "1.0383533239364624\n",
      "Epoch 4\n",
      "32.361928820610046\n",
      "1.1015052795410156\n",
      "Epoch 5\n",
      "34.682821810245514\n",
      "1.039508581161499\n",
      "Epoch 6\n",
      "27.491903245449066\n",
      "0.9781687259674072\n",
      "Epoch 7\n",
      "26.280299723148346\n",
      "0.9494391679763794\n",
      "Epoch 8\n",
      "25.401515871286392\n",
      "0.9347696900367737\n",
      "Epoch 9\n",
      "24.185295909643173\n",
      "0.9859669208526611\n",
      "Epoch 10\n",
      "23.77444839477539\n",
      "0.9533861875534058\n",
      "Epoch 11\n",
      "22.39775425195694\n",
      "Epoch    12: reducing learning rate of group 0 to 1.0000e-04.\n",
      "0.9566575884819031\n",
      "Epoch 12\n",
      "19.511213064193726\n",
      "0.9454945921897888\n",
      "Epoch 13\n",
      "18.544819474220276\n",
      "0.9480131268501282\n",
      "Epoch 14\n",
      "19.425233066082\n",
      "Epoch    15: reducing learning rate of group 0 to 1.0000e-05.\n",
      "0.9447203874588013\n",
      "Epoch 15\n",
      "18.77895399928093\n",
      "0.9439171552658081\n",
      "Epoch 16\n",
      "18.106388211250305\n",
      "0.9436770677566528\n",
      "Epoch 17\n",
      "18.676059514284134\n",
      "Epoch    18: reducing learning rate of group 0 to 1.0000e-06.\n",
      "0.9434993267059326\n",
      "Epoch 18\n",
      "18.22844171524048\n",
      "0.9440346956253052\n",
      "Epoch 19\n",
      "18.016422659158707\n",
      "0.9441255331039429\n",
      "Epoch 20\n",
      "17.968571186065674\n",
      "Epoch    21: reducing learning rate of group 0 to 1.0000e-07.\n",
      "0.9438321590423584\n",
      "Epoch 21\n",
      "18.241883873939514\n",
      "0.9444210529327393\n",
      "Epoch 22\n",
      "18.242259234189987\n",
      "0.9430758953094482\n",
      "Epoch 23\n",
      "18.158489257097244\n",
      "Epoch    24: reducing learning rate of group 0 to 1.0000e-08.\n",
      "0.9444615244865417\n",
      "Epoch 24\n",
      "18.373193740844727\n",
      "0.9440272450447083\n",
      "Epoch 25\n",
      "18.706962168216705\n",
      "0.9448580741882324\n",
      "Epoch 26\n",
      "18.720344096422195\n",
      "0.943780243396759\n",
      "Epoch 27\n",
      "18.397103160619736\n",
      "0.9445803165435791\n",
      "Epoch 28\n",
      "18.420233815908432\n",
      "0.9445750117301941\n",
      "Epoch 29\n",
      "18.086071252822876\n",
      "0.9437848329544067\n",
      "Epoch 30\n",
      "17.976484209299088\n",
      "0.9438692927360535\n",
      "Epoch 31\n",
      "18.381614297628403\n",
      "0.9441569447517395\n",
      "Epoch 32\n",
      "18.057763159275055\n",
      "0.9429619908332825\n",
      "Epoch 33\n",
      "18.46068948507309\n",
      "0.9448683261871338\n",
      "Epoch 34\n",
      "18.361600130796432\n",
      "0.9443223476409912\n",
      "Epoch 35\n",
      "18.566946268081665\n",
      "0.9447829127311707\n",
      "Epoch 36\n",
      "17.947621256113052\n",
      "0.9434375166893005\n",
      "Epoch 37\n",
      "18.32223778963089\n",
      "0.944532036781311\n",
      "Epoch 38\n",
      "18.178652182221413\n",
      "0.9433501362800598\n",
      "Epoch 39\n",
      "18.301265850663185\n",
      "0.9441113471984863\n",
      "Epoch 40\n",
      "18.18992480635643\n",
      "0.9430990815162659\n",
      "Epoch 41\n",
      "18.21069970726967\n",
      "0.9439958333969116\n",
      "Epoch 42\n",
      "18.059197559952736\n",
      "0.944610059261322\n",
      "Epoch 43\n",
      "18.081650257110596\n",
      "0.9438716173171997\n",
      "Epoch 44\n",
      "17.98208847641945\n",
      "0.9432345628738403\n",
      "Epoch 45\n",
      "18.22853723168373\n",
      "0.9446185827255249\n",
      "Epoch 46\n",
      "18.440645575523376\n",
      "0.9444394707679749\n",
      "Epoch 47\n",
      "18.410906940698624\n",
      "0.9436984062194824\n",
      "Epoch 48\n",
      "19.014651775360107\n",
      "0.9440747499465942\n",
      "Epoch 49\n",
      "17.953027367591858\n",
      "0.9448652267456055\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.L1Loss()\n",
    "optimizer = optim.Adam(list(model.parameters())[2:])\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=2, verbose=True)\n",
    "for e in range(epochs):\n",
    "    print('Epoch {}'.format(e))\n",
    "    train_loss = 0.0\n",
    "    model.train()\n",
    "    for text, audio, vision, label in train_dataloader:\n",
    "        model.zero_grad()\n",
    "        output = model(audio, vision, text)\n",
    "        loss = criterion(output, label)\n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        optimizer.step()\n",
    "    print(train_loss)\n",
    "    \n",
    "    model.eval()\n",
    "    for text, audio, vision, label in valid_dataloader:\n",
    "        output = model(audio, vision, text)\n",
    "        valid_loss = criterion(output, label).item()\n",
    "    scheduler.step(valid_loss)\n",
    "    print(valid_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6ea8d84-7e1f-4908-bec8-334a7e87e765",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6fc1777d-7e89-49db-9bdf-4c7acc9118bc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_cmu_mosi(batch_size=32, epochs=100, lr=.001, max_rank=5, rank_adaptive=False,  \n",
    "                   warmup_epochs=50, kl_multiplier=1e-4, no_kl_epochs=5, accelerated=True):\n",
    "\n",
    "    # load dataset file\n",
    "    file = open('../../dataset/cmu-mosi/mosi_20_seq_data.pkl', 'rb')\n",
    "    data = pickle.load(file)\n",
    "    file.close()\n",
    "\n",
    "    # prepare the datasets and data loaders\n",
    "    train_set = MultimodalDataset(data['train']['text'], data['train']['audio'],\n",
    "                                  data['train']['vision'], data['train']['labels'])\n",
    "    valid_set = MultimodalDataset(data['valid']['text'], data['valid']['audio'],\n",
    "                                  data['valid']['vision'], data['valid']['labels'])\n",
    "\n",
    "    train_dataloader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "    valid_dataloader = DataLoader(valid_set, batch_size=len(valid_set))\n",
    "\n",
    "    # set up model\n",
    "    input_sizes = (train_set[0][0]['audio'].shape[0], train_set[0][0]['vision'].shape[0],\n",
    "                   train_set[0][0]['text'].shape[1])\n",
    "    hidden_sizes = (32, 32, 128)\n",
    "    output_size = 1\n",
    "    \n",
    "    # model = CP_Tensor_Fusion_Network(input_sizes, hidden_sizes, output_size, max_rank,\n",
    "    #                                  rank_adaptive)\n",
    "    \n",
    "    model = TFN(input_dims=input_sizes, hidden_dims=hidden_sizes, text_out=128, dropouts=(0.3, 0.3, 0.3, 0.3), \n",
    "                post_fusion_dim=1)\n",
    "    \n",
    "    # set up training\n",
    "    DTYPE = torch.FloatTensor\n",
    "    optimizer = optim.Adam(list(model.parameters())[2:], lr=lr)\n",
    "    #criterion = nn.MSELoss()\n",
    "    criterion = nn.L1Loss()\n",
    "    \n",
    "    # train and validate\n",
    "    for e in range(1, epochs + 1):\n",
    "        # train\n",
    "        #tic = time.time()\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        for batch in train_dataloader:\n",
    "            model.zero_grad()\n",
    "\n",
    "            features, label = batch\n",
    "            \n",
    "            x_a = Variable(features['audio'].float().type(DTYPE), requires_grad=False)\n",
    "            x_v = Variable(features['vision'].float().type(DTYPE), requires_grad=False)\n",
    "            x_t = Variable(features['text'].float().type(DTYPE), requires_grad=False)\n",
    "            y = Variable(label.view(-1, 1).float().type(DTYPE), requires_grad=False)\n",
    "            \n",
    "            output = model(x_a, x_v, x_t)\n",
    "            loss = criterion(output, y)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "            \n",
    "        \n",
    "        print('Train Loss {:.3f}'.format(train_loss))\n",
    "        '''\n",
    "        print(model.audio_subnet.linear_2.weight.grad.mean())\n",
    "        print(model.video_subnet.linear_2.weight.grad.mean())\n",
    "        print(model.text_subnet.linear_1.weight.grad.mean())\n",
    "        print(model.tensor_fusion_layer.weight_tensor.factors[0].grad.mean())\n",
    "        '''\n",
    "\n",
    "        # validate\n",
    "        model.eval()\n",
    "        for batch in valid_dataloader:\n",
    "            features, label = batch\n",
    "            x_a = Variable(features['audio'].float().type(DTYPE), requires_grad=False)\n",
    "            x_v = Variable(features['vision'].float().type(DTYPE), requires_grad=False)\n",
    "            x_t = Variable(features['text'].float().type(DTYPE), requires_grad=False)\n",
    "            y = Variable(label.view(-1, 1).float().type(DTYPE), requires_grad=False)\n",
    "\n",
    "            output = model(x_a, x_v, x_t)\n",
    "        \n",
    "        valid_mse = nn.functional.l1_loss(output, y).item()\n",
    "        print(\"Valid MSE {:.3f}\".format(valid_mse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c3ba38f7-669c-4ffb-8086-acf7c8aa13a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss 56.048\n",
      "Valid MSE 1.432\n",
      "Train Loss 55.095\n",
      "Valid MSE 1.426\n",
      "Train Loss 54.541\n",
      "Valid MSE 1.421\n",
      "Train Loss 54.881\n",
      "Valid MSE 1.419\n",
      "Train Loss 54.105\n",
      "Valid MSE 1.417\n",
      "Train Loss 54.201\n",
      "Valid MSE 1.416\n",
      "Train Loss 54.426\n",
      "Valid MSE 1.413\n",
      "Train Loss 54.250\n",
      "Valid MSE 1.413\n",
      "Train Loss 53.920\n",
      "Valid MSE 1.413\n",
      "Train Loss 54.576\n",
      "Valid MSE 1.413\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/xr/p4pqzm995nbcl9w6d8lrb5zh0000gn/T/ipykernel_56057/2459496142.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_cmu_mosi\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/var/folders/xr/p4pqzm995nbcl9w6d8lrb5zh0000gn/T/ipykernel_56057/2366110158.py\u001b[0m in \u001b[0;36mtrain_cmu_mosi\u001b[0;34m(batch_size, epochs, lr, max_rank, rank_adaptive, warmup_epochs, kl_multiplier, no_kl_epochs, accelerated)\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDTYPE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequires_grad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_a\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_v\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/tensor_fusion/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Projects/bayesian-tensor-rank-determination/tensor_fusion/model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, audio_x, video_x, text_x)\u001b[0m\n\u001b[1;32m    135\u001b[0m         \u001b[0maudio_h\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maudio_subnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maudio_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m         \u001b[0mvideo_h\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvideo_subnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvideo_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m         \u001b[0mtext_h\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext_subnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m         \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maudio_h\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/tensor_fusion/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Projects/bayesian-tensor-rank-determination/tensor_fusion/model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtensor\u001b[0m \u001b[0mof\u001b[0m \u001b[0mshape\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msequence_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0min_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m         '''\n\u001b[0;32m---> 71\u001b[0;31m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinal_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m         \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfinal_states\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0my_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear_1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/tensor_fusion/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/tensor_fusion/lib/python3.9/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    689\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_forward_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    690\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 691\u001b[0;31m             result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n\u001b[0m\u001b[1;32m    692\u001b[0m                               self.dropout, self.training, self.bidirectional, self.batch_first)\n\u001b[1;32m    693\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_cmu_mosi()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "281c3ff2-dded-4afa-9a9b-2478e21cb8d7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
