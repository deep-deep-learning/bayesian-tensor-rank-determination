{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b684eea4-53bf-41f3-bc3d-6293380241e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "import numpy as np\n",
    "from model import SubNet, TextSubNet, TFN, LMF, AdaptiveRankFusion\n",
    "from torch.distributions.half_cauchy import HalfCauchy\n",
    "from torch.distributions.normal import Normal\n",
    "from datasets import get_cmu_mosi_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "34c993f7-48eb-4ed3-84ed-656dd5f31bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set, valid_set, test_set = get_cmu_mosi_dataset(binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "97792882-72c9-4a43-8b85-ebef742aa0eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/christian_lee/anaconda3/envs/tensor_layers/lib/python3.8/site-packages/torch/nn/modules/rnn.py:62: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "epochs = 50\n",
    "# settings from https://github.com/Justin1904/TensorFusionNetworks/blob/master/train.py\n",
    "input_dims = (5, 20, 300)\n",
    "hidden_dims = (4, 16, 128)\n",
    "text_out = 64\n",
    "dropouts = (0.3, 0.3, 0.3, 0.3)\n",
    "post_fusion_dim = 32\n",
    "model = TFN(input_dims, hidden_dims, text_out, dropouts, post_fusion_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "63cbd265-5bd8-43b6-b162-9d0afaac6652",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=32\n",
    "train_dataloader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "valid_dataloader = DataLoader(valid_set, batch_size=len(valid_set))\n",
    "test_dataloader = DataLoader(test_set, batch_size=len(test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b48406cc-82d3-4667-8420-1dfbc5de4a5a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/christian_lee/anaconda3/envs/tensor_layers/lib/python3.8/site-packages/torch/nn/functional.py:1805: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51.58957976102829\n",
      "1.1529542207717896\n",
      "Epoch 1\n",
      "39.98873919248581\n",
      "1.1220371723175049\n",
      "Epoch 2\n",
      "36.850977063179016\n",
      "1.0471466779708862\n",
      "Epoch 3\n",
      "33.12352240085602\n",
      "1.015275239944458\n",
      "Epoch 4\n",
      "31.333762228488922\n",
      "1.1576157808303833\n",
      "Epoch 5\n",
      "31.159377694129944\n",
      "1.030088186264038\n",
      "Epoch 6\n",
      "28.39326113462448\n",
      "Epoch     7: reducing learning rate of group 0 to 1.0000e-04.\n",
      "1.0263044834136963\n",
      "Epoch 7\n",
      "26.078719526529312\n",
      "0.9703226685523987\n",
      "Epoch 8\n",
      "25.163460284471512\n",
      "0.96832275390625\n",
      "Epoch 9\n",
      "24.169587582349777\n",
      "0.9733483195304871\n",
      "Epoch 10\n",
      "24.36491909623146\n",
      "0.984216034412384\n",
      "Epoch 11\n",
      "23.912283927202225\n",
      "Epoch    12: reducing learning rate of group 0 to 1.0000e-05.\n",
      "0.9718354940414429\n",
      "Epoch 12\n",
      "23.52481299638748\n",
      "0.9737423658370972\n",
      "Epoch 13\n",
      "23.00118261575699\n",
      "0.9700931906700134\n",
      "Epoch 14\n",
      "23.31677833199501\n",
      "0.9659343957901001\n",
      "Epoch 15\n",
      "24.225109219551086\n",
      "0.9717851877212524\n",
      "Epoch 16\n",
      "23.423350244760513\n",
      "0.9671643972396851\n",
      "Epoch 17\n",
      "23.49833881855011\n",
      "Epoch    18: reducing learning rate of group 0 to 1.0000e-06.\n",
      "0.9698118567466736\n",
      "Epoch 18\n",
      "22.89182385802269\n",
      "0.969062328338623\n",
      "Epoch 19\n",
      "22.876979023218155\n",
      "0.9673333168029785\n",
      "Epoch 20\n",
      "23.14093628525734\n",
      "Epoch    21: reducing learning rate of group 0 to 1.0000e-07.\n",
      "0.9669705629348755\n",
      "Epoch 21\n",
      "23.72996237874031\n",
      "0.9706878066062927\n",
      "Epoch 22\n",
      "23.25182741880417\n",
      "0.968403697013855\n",
      "Epoch 23\n",
      "22.413132950663567\n",
      "0.965544581413269\n",
      "Epoch 24\n",
      "23.72746828198433\n",
      "0.97076016664505\n",
      "Epoch 25\n",
      "23.090075820684433\n",
      "0.9705105423927307\n",
      "Epoch 26\n",
      "23.729697436094284\n",
      "Epoch    27: reducing learning rate of group 0 to 1.0000e-08.\n",
      "0.9676216244697571\n",
      "Epoch 27\n",
      "23.321259915828705\n",
      "0.9691495895385742\n",
      "Epoch 28\n",
      "23.08038318157196\n",
      "0.9689145088195801\n",
      "Epoch 29\n",
      "23.3046216070652\n",
      "0.9685364961624146\n",
      "Epoch 30\n",
      "22.969385981559753\n",
      "0.9650918245315552\n",
      "Epoch 31\n",
      "23.54999601840973\n",
      "0.9669001698493958\n",
      "Epoch 32\n",
      "22.96922415494919\n",
      "0.969024658203125\n",
      "Epoch 33\n",
      "23.33345639705658\n",
      "0.967757523059845\n",
      "Epoch 34\n",
      "23.22039946913719\n",
      "0.9669368863105774\n",
      "Epoch 35\n",
      "23.199168860912323\n",
      "0.9695451855659485\n",
      "Epoch 36\n",
      "22.84316685795784\n",
      "0.9694777727127075\n",
      "Epoch 37\n",
      "23.801842749118805\n",
      "0.9701430797576904\n",
      "Epoch 38\n",
      "23.46989279985428\n",
      "0.9694908857345581\n",
      "Epoch 39\n",
      "23.02222567796707\n",
      "0.967971682548523\n",
      "Epoch 40\n",
      "22.91052833199501\n",
      "0.9708861708641052\n",
      "Epoch 41\n",
      "23.586570024490356\n",
      "0.9670401215553284\n",
      "Epoch 42\n",
      "23.355402886867523\n",
      "0.9676880836486816\n",
      "Epoch 43\n",
      "23.058048397302628\n",
      "0.9669484496116638\n",
      "Epoch 44\n",
      "23.152512162923813\n",
      "0.969781219959259\n",
      "Epoch 45\n",
      "22.77719148993492\n",
      "0.9695078134536743\n",
      "Epoch 46\n",
      "23.739845424890518\n",
      "0.9688222408294678\n",
      "Epoch 47\n",
      "23.242196708917618\n",
      "0.9667183756828308\n",
      "Epoch 48\n",
      "22.96525612473488\n",
      "0.9692946672439575\n",
      "Epoch 49\n",
      "23.993491291999817\n",
      "0.969179630279541\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.L1Loss()\n",
    "optimizer = optim.Adam(list(model.parameters())[2:])\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=2, verbose=True)\n",
    "for e in range(epochs):\n",
    "    print('Epoch {}'.format(e))\n",
    "    train_loss = 0.0\n",
    "    model.train()\n",
    "    for text, audio, vision, label in train_dataloader:\n",
    "        model.zero_grad()\n",
    "        output = model(audio, vision, text)\n",
    "        loss = criterion(output, label)\n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        optimizer.step()\n",
    "    print(train_loss)\n",
    "    \n",
    "    model.eval()\n",
    "    for text, audio, vision, label in valid_dataloader:\n",
    "        output = model(audio, vision, text)\n",
    "        valid_loss = criterion(output, label).item()\n",
    "    scheduler.step(valid_loss)\n",
    "    print(valid_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c6ea8d84-7e1f-4908-bec8-334a7e87e765",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/christian_lee/projects/bayesian-tensor-rank-determination/tensor_fusion/model.py:231: UserWarning: nn.init.xavier_normal is now deprecated in favor of nn.init.xavier_normal_.\n",
      "  xavier_normal(self.audio_factor)\n",
      "/home/christian_lee/projects/bayesian-tensor-rank-determination/tensor_fusion/model.py:232: UserWarning: nn.init.xavier_normal is now deprecated in favor of nn.init.xavier_normal_.\n",
      "  xavier_normal(self.video_factor)\n",
      "/home/christian_lee/projects/bayesian-tensor-rank-determination/tensor_fusion/model.py:233: UserWarning: nn.init.xavier_normal is now deprecated in favor of nn.init.xavier_normal_.\n",
      "  xavier_normal(self.text_factor)\n",
      "/home/christian_lee/projects/bayesian-tensor-rank-determination/tensor_fusion/model.py:234: UserWarning: nn.init.xavier_normal is now deprecated in favor of nn.init.xavier_normal_.\n",
      "  xavier_normal(self.fusion_weights)\n"
     ]
    }
   ],
   "source": [
    "rank = 4\n",
    "output_dim = 1\n",
    "model = LMF(input_dims, hidden_dims, text_out, dropouts, output_dim, rank, use_softmax=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "45bfe104-0bbc-4083-9432-d308a4bc5a61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "22.86905947327614\n",
      "1.000284194946289\n",
      "Epoch 1\n",
      "20.729330733418465\n",
      "1.0048730373382568\n",
      "Epoch 2\n",
      "20.46320030093193\n",
      "0.9808865785598755\n",
      "Epoch 3\n",
      "18.937021389603615\n",
      "0.9868036508560181\n",
      "Epoch 4\n",
      "18.58921778202057\n",
      "1.0054031610488892\n",
      "Epoch 5\n",
      "17.24204032123089\n",
      "Epoch     6: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch     6: reducing learning rate of group 1 to 1.0000e-04.\n",
      "1.012037992477417\n",
      "Epoch 6\n",
      "16.150738149881363\n",
      "0.9593955874443054\n",
      "Epoch 7\n",
      "14.970886915922165\n",
      "0.9665323495864868\n",
      "Epoch 8\n",
      "14.458706766366959\n",
      "0.9764308929443359\n",
      "Epoch 9\n",
      "14.091897323727608\n",
      "Epoch    10: reducing learning rate of group 0 to 5.0000e-06.\n",
      "Epoch    10: reducing learning rate of group 1 to 1.0000e-05.\n",
      "0.9663721323013306\n",
      "Epoch 10\n",
      "13.62071168422699\n",
      "0.965540885925293\n",
      "Epoch 11\n",
      "13.827472314238548\n",
      "0.9655867218971252\n",
      "Epoch 12\n",
      "13.69283601641655\n",
      "Epoch    13: reducing learning rate of group 0 to 5.0000e-07.\n",
      "Epoch    13: reducing learning rate of group 1 to 1.0000e-06.\n",
      "0.9673418998718262\n",
      "Epoch 13\n",
      "13.770007252693176\n",
      "0.9671180248260498\n",
      "Epoch 14\n",
      "13.540986143052578\n",
      "0.9673351049423218\n",
      "Epoch 15\n",
      "14.118044331669807\n",
      "Epoch    16: reducing learning rate of group 0 to 5.0000e-08.\n",
      "Epoch    16: reducing learning rate of group 1 to 1.0000e-07.\n",
      "0.9656857252120972\n",
      "Epoch 16\n",
      "13.856385320425034\n",
      "0.9665480852127075\n",
      "Epoch 17\n",
      "13.959072723984718\n",
      "0.9662286639213562\n",
      "Epoch 18\n",
      "14.009803891181946\n",
      "Epoch    19: reducing learning rate of group 0 to 5.0000e-09.\n",
      "Epoch    19: reducing learning rate of group 1 to 1.0000e-08.\n",
      "0.9675585627555847\n",
      "Epoch 19\n",
      "13.975851371884346\n",
      "0.9661593437194824\n",
      "Epoch 20\n",
      "14.027985572814941\n",
      "0.9667764902114868\n",
      "Epoch 21\n",
      "14.01363579928875\n",
      "0.9664937853813171\n",
      "Epoch 22\n",
      "13.957672581076622\n",
      "0.9664810299873352\n",
      "Epoch 23\n",
      "13.807520627975464\n",
      "0.966729462146759\n",
      "Epoch 24\n",
      "13.803470894694328\n",
      "0.9659863710403442\n",
      "Epoch 25\n",
      "14.084839582443237\n",
      "0.9668165445327759\n",
      "Epoch 26\n",
      "13.953190758824348\n",
      "0.9662301540374756\n",
      "Epoch 27\n",
      "14.091428399085999\n",
      "0.9658627510070801\n",
      "Epoch 28\n",
      "14.281571373343468\n",
      "0.9664732217788696\n",
      "Epoch 29\n",
      "13.63581308722496\n",
      "0.9660665988922119\n",
      "Epoch 30\n",
      "13.847106516361237\n",
      "0.9655248522758484\n",
      "Epoch 31\n",
      "13.72951826453209\n",
      "0.9672750234603882\n",
      "Epoch 32\n",
      "13.734538152813911\n",
      "0.9660537242889404\n",
      "Epoch 33\n",
      "13.777122110128403\n",
      "0.9680215716362\n",
      "Epoch 34\n",
      "13.800354078412056\n",
      "0.9663001298904419\n",
      "Epoch 35\n",
      "13.655103787779808\n",
      "0.9660874009132385\n",
      "Epoch 36\n",
      "13.993770703673363\n",
      "0.9661179184913635\n",
      "Epoch 37\n",
      "14.187026277184486\n",
      "0.9658401608467102\n",
      "Epoch 38\n",
      "14.08176276087761\n",
      "0.9669401049613953\n",
      "Epoch 39\n",
      "13.680228158831596\n",
      "0.9663575291633606\n",
      "Epoch 40\n",
      "13.426718458533287\n",
      "0.9661930799484253\n",
      "Epoch 41\n",
      "13.672035813331604\n",
      "0.967471182346344\n",
      "Epoch 42\n",
      "13.932603552937508\n",
      "0.966646671295166\n",
      "Epoch 43\n",
      "13.652762278914452\n",
      "0.9663758873939514\n",
      "Epoch 44\n",
      "13.60215250402689\n",
      "0.9663940072059631\n",
      "Epoch 45\n",
      "13.948273032903671\n",
      "0.9658129215240479\n",
      "Epoch 46\n",
      "14.158247947692871\n",
      "0.9672079682350159\n",
      "Epoch 47\n",
      "13.714168518781662\n",
      "0.9668121337890625\n",
      "Epoch 48\n",
      "14.531151160597801\n",
      "0.9664973616600037\n",
      "Epoch 49\n",
      "13.467280149459839\n",
      "0.965761125087738\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.L1Loss()\n",
    "factors = list(model.parameters())[:3]\n",
    "other = list(model.parameters())[3:]\n",
    "factor_lr = 0.0005\n",
    "lr = 0.001\n",
    "optimizer = optim.Adam([{\"params\": factors, \"lr\": factor_lr}, {\"params\": other, \"lr\": lr}])\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=2, verbose=True)\n",
    "for e in range(epochs):\n",
    "    print('Epoch {}'.format(e))\n",
    "    train_loss = 0.0\n",
    "    model.train()\n",
    "    for text, audio, vision, label in train_dataloader:\n",
    "        model.zero_grad()\n",
    "        output = model(audio, vision, text)\n",
    "        loss = criterion(output, label)\n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        optimizer.step()\n",
    "    print(train_loss)\n",
    "    \n",
    "    model.eval()\n",
    "    for text, audio, vision, label in valid_dataloader:\n",
    "        output = model(audio, vision, text)\n",
    "        valid_loss = criterion(output, label).item()\n",
    "    scheduler.step(valid_loss)\n",
    "    print(valid_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "281c3ff2-dded-4afa-9a9b-2478e21cb8d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaptiveRankFusionLayer(nn.Module):\n",
    "\n",
    "    def __init__(self, input_sizes, output_size, max_rank=10, eta=0.01):\n",
    "        '''\n",
    "        args:\n",
    "            input_sizes: a tuple of ints, (input_size_1, input_size_2, ..., input_size_M)\n",
    "            output_sizes: an int, output size of the fusion layer\n",
    "            dropout: a float, dropout probablity after fusion\n",
    "            max_rank: an int, maximum rank for the CP decomposition\n",
    "            eta: a float, hyperparameter for rank parameter distribution\n",
    "        '''\n",
    "        super(AdaptiveRankFusionLayer, self).__init__()\n",
    "\n",
    "        self.input_sizes = input_sizes\n",
    "        self.output_size = output_size\n",
    "        self.max_rank = max_rank\n",
    "        self.eta = eta\n",
    "\n",
    "        # CP decomposition factors for the weight tensor\n",
    "        self.factors = nn.ParameterList([nn.init.xavier_normal_(nn.Parameter(torch.empty(s, max_rank))) \n",
    "                                        for s in input_sizes+(output_size,)])\n",
    "        # rank parameter and its distribution for adaptive rank\n",
    "        self.rank_param = nn.Parameter(torch.rand((max_rank,)))\n",
    "        self.rank_param_dist = HalfCauchy(eta)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        '''\n",
    "        args:\n",
    "            inputs: a list of vectors, (input_1, input_2, ..., input_M)\n",
    "        return:\n",
    "            y = [(input_1 @ factor_1) (input_2 @ factor_2) ... (input_M @ factor_M)] @ factor_{M+1}.T\n",
    "        '''\n",
    "\n",
    "        y = 1.0\n",
    "        for i, x in enumerate(inputs):\n",
    "            y = y * (x @ self.factors[i])\n",
    "        y = y @ self.factors[-1].T\n",
    "\n",
    "        return y\n",
    "\n",
    "    def get_log_prior(self):\n",
    "        '''\n",
    "        return:\n",
    "            log_prior = log[HalfCauchy(rank_param | eta)] + log[Normal(factor_1 | 0, rank_param)]\n",
    "                    + log[Normal(factor_2 | 0, rank_param)] + ... + log[Normal(factor_{M+1} | 0, rank_param)]\n",
    "        '''\n",
    "        # clamp rank_param because <=0 is undefined \n",
    "        clamped_rank_param = self.rank_param.clamp(0.01)\n",
    "        log_prior = torch.sum(self.rank_param_dist.log_prob(clamped_rank_param))\n",
    "\n",
    "        # 0 mean normal distribution for the factors\n",
    "        factor_dist = Normal(0, clamped_rank_param)\n",
    "        for factor in self.factors:\n",
    "            log_prior = log_prior + torch.sum(factor_dist.log_prob(factor))\n",
    "        \n",
    "        return log_prior\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2cac9fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaptiveRankFusion(nn.Module):\n",
    "\n",
    "    def __init__(self, input_sizes, hidden_sizes, dropouts, output_size, max_rank=10, eta=0.01):\n",
    "        '''\n",
    "        args:\n",
    "            input_sizes: a tuple of ints, (audio_in, video_in, ... text_in)\n",
    "            hidden_sizes: a tuple of ints, (audio_hidden, video_hidden, ... text_hidden)\n",
    "            dropouts: a tuple of floats, (dropout_1, dropout_2, ..., dropout_M, post_fusion_dropout)\n",
    "            output_size: an int, output size for fusion layer\n",
    "            max_rank: an int, maximum rank for the CP decomposition\n",
    "        '''\n",
    "        super(AdaptiveRankFusion, self).__init__()\n",
    "        \n",
    "        # define the pre-fusion subnetworks\n",
    "        self.audio_subnet = SubNet(input_sizes[0], hidden_sizes[0], dropouts[0])\n",
    "        self.video_subnet = SubNet(input_sizes[1], hidden_sizes[1], dropouts[1])\n",
    "        self.text_subnet = TextSubNet(input_sizes[2], hidden_sizes[2], hidden_sizes[2]//2, dropout=dropouts[2])\n",
    "        \n",
    "        fusion_input_sizes = (hidden_sizes[0]+1, hidden_sizes[1]+1, hidden_sizes[2]//2+1)\n",
    "        # define fusion layer\n",
    "        self.fusion_layer = AdaptiveRankFusionLayer(input_sizes=fusion_input_sizes,\n",
    "                                                    output_size=output_size,\n",
    "                                                    max_rank=max_rank,\n",
    "                                                    eta=eta)\n",
    "        self.post_fusion_dropout = nn.Dropout(dropouts[-1])\n",
    "\n",
    "    def forward(self, audio_x, video_x, text_x):\n",
    "\n",
    "        audio_h = self.audio_subnet(audio_x)\n",
    "        video_h = self.video_subnet(video_x)\n",
    "        text_h = self.text_subnet(text_x)\n",
    "\n",
    "        batch_size = audio_h.shape[0]\n",
    "\n",
    "        audio_h = torch.cat((audio_h, torch.ones((batch_size, 1))), dim=1)\n",
    "        video_h = torch.cat((video_h, torch.ones((batch_size, 1))), dim=1)\n",
    "        text_h = torch.cat((text_h, torch.ones((batch_size, 1))), dim=1)\n",
    "\n",
    "        output = self.fusion_layer([audio_h, video_h, text_h])\n",
    "        output = self.post_fusion_dropout(output)\n",
    "        \n",
    "        return output\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "63b77167",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/christian_lee/anaconda3/envs/tensor_layers/lib/python3.8/site-packages/torch/nn/modules/rnn.py:62: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    }
   ],
   "source": [
    "input_dims = (5, 20, 300)\n",
    "hidden_dims = (4, 16, 128)\n",
    "output_size = (1)\n",
    "dropouts = (0.2, 0.2, 0.2, 0.2)\n",
    "model = AdaptiveRankFusion(input_dims, hidden_dims, dropouts, output_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e91ce4c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "1547.292766571045\n",
      "0.3096248209476471\n",
      "Epoch 1\n",
      "1292.0612697601318\n",
      "-1.2837207317352295\n",
      "Epoch 2\n",
      "721.7102184295654\n",
      "-10.459827423095703\n",
      "Epoch 3\n",
      "-954.6527261734009\n",
      "-42.06126403808594\n",
      "Epoch 4\n",
      "-12695.144991874695\n",
      "-273.0089111328125\n",
      "Epoch 5\n",
      "-71993.96398925781\n",
      "-930.1858520507812\n",
      "Epoch 6\n",
      "-273316.80923461914\n",
      "-6793.36376953125\n",
      "Epoch 7\n",
      "-1391102.5603027344\n",
      "-31626.0234375\n",
      "Epoch 8\n",
      "-9069991.73803711\n",
      "-377204.4375\n",
      "Epoch 9\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-c7806787cb95>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mtrain_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensor_layers/lib/python3.8/site-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m                 \u001b[0mprofile_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Optimizer.step#{}.step\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprofile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensor_layers/lib/python3.8/site-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensor_layers/lib/python3.8/site-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    105\u001b[0m                     \u001b[0mstate_steps\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'step'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m             F.adam(params_with_grad,\n\u001b[0m\u001b[1;32m    108\u001b[0m                    \u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m                    \u001b[0mexp_avgs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensor_layers/lib/python3.8/site-packages/torch/optim/_functional.py\u001b[0m in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps)\u001b[0m\n\u001b[1;32m     92\u001b[0m             \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmax_exp_avg_sqs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias_correction2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m             \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias_correction2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0mstep_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlr\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mbias_correction1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "criterion = nn.BCEWithLogitsLoss()\n",
    "factor_lr = 0.0001\n",
    "lr = 0.001\n",
    "subnet_params = list(model.audio_subnet.parameters()) + list(model.video_subnet.parameters()) + list(model.text_subnet.parameters())\n",
    "optimizer = optim.Adam([{\"params\": subnet_params, \"lr\": lr}, \n",
    "                        {\"params\": list(model.fusion_layer.parameters()), \"lr\": factor_lr}])\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=2, verbose=True)\n",
    "epochs = 50\n",
    "for e in range(epochs):\n",
    "    print('Epoch {}'.format(e))\n",
    "    train_loss = 0.0\n",
    "    model.train()\n",
    "    for text, audio, vision, label in train_dataloader:\n",
    "        model.zero_grad()\n",
    "        output = model(audio, vision, text)\n",
    "        loss = criterion(output, label) - 0.1 * model.fusion_layer.get_log_prior()\n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        optimizer.step()\n",
    "    print(train_loss)\n",
    "    \n",
    "    model.eval()\n",
    "    for text, audio, vision, label in valid_dataloader:\n",
    "        output = model(audio, vision, text)\n",
    "        valid_loss = criterion(output, label).item()\n",
    "    scheduler.step(valid_loss)\n",
    "    print(valid_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a6ca96c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([0.6895, 0.4420, 0.6921, 0.6713, 0.6768, 0.3938, 0.4589, 0.5104, 0.4994,\n",
       "        0.4866], requires_grad=True)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fusion_layer.rank_param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45bfe104-0bbc-4083-9432-d308a4bc5a61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "22.86905947327614\n",
      "1.000284194946289\n",
      "Epoch 1\n",
      "20.729330733418465\n",
      "1.0048730373382568\n",
      "Epoch 2\n",
      "20.46320030093193\n",
      "0.9808865785598755\n",
      "Epoch 3\n",
      "18.937021389603615\n",
      "0.9868036508560181\n",
      "Epoch 4\n",
      "18.58921778202057\n",
      "1.0054031610488892\n",
      "Epoch 5\n",
      "17.24204032123089\n",
      "Epoch     6: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch     6: reducing learning rate of group 1 to 1.0000e-04.\n",
      "1.012037992477417\n",
      "Epoch 6\n",
      "16.150738149881363\n",
      "0.9593955874443054\n",
      "Epoch 7\n",
      "14.970886915922165\n",
      "0.9665323495864868\n",
      "Epoch 8\n",
      "14.458706766366959\n",
      "0.9764308929443359\n",
      "Epoch 9\n",
      "14.091897323727608\n",
      "Epoch    10: reducing learning rate of group 0 to 5.0000e-06.\n",
      "Epoch    10: reducing learning rate of group 1 to 1.0000e-05.\n",
      "0.9663721323013306\n",
      "Epoch 10\n",
      "13.62071168422699\n",
      "0.965540885925293\n",
      "Epoch 11\n",
      "13.827472314238548\n",
      "0.9655867218971252\n",
      "Epoch 12\n",
      "13.69283601641655\n",
      "Epoch    13: reducing learning rate of group 0 to 5.0000e-07.\n",
      "Epoch    13: reducing learning rate of group 1 to 1.0000e-06.\n",
      "0.9673418998718262\n",
      "Epoch 13\n",
      "13.770007252693176\n",
      "0.9671180248260498\n",
      "Epoch 14\n",
      "13.540986143052578\n",
      "0.9673351049423218\n",
      "Epoch 15\n",
      "14.118044331669807\n",
      "Epoch    16: reducing learning rate of group 0 to 5.0000e-08.\n",
      "Epoch    16: reducing learning rate of group 1 to 1.0000e-07.\n",
      "0.9656857252120972\n",
      "Epoch 16\n",
      "13.856385320425034\n",
      "0.9665480852127075\n",
      "Epoch 17\n",
      "13.959072723984718\n",
      "0.9662286639213562\n",
      "Epoch 18\n",
      "14.009803891181946\n",
      "Epoch    19: reducing learning rate of group 0 to 5.0000e-09.\n",
      "Epoch    19: reducing learning rate of group 1 to 1.0000e-08.\n",
      "0.9675585627555847\n",
      "Epoch 19\n",
      "13.975851371884346\n",
      "0.9661593437194824\n",
      "Epoch 20\n",
      "14.027985572814941\n",
      "0.9667764902114868\n",
      "Epoch 21\n",
      "14.01363579928875\n",
      "0.9664937853813171\n",
      "Epoch 22\n",
      "13.957672581076622\n",
      "0.9664810299873352\n",
      "Epoch 23\n",
      "13.807520627975464\n",
      "0.966729462146759\n",
      "Epoch 24\n",
      "13.803470894694328\n",
      "0.9659863710403442\n",
      "Epoch 25\n",
      "14.084839582443237\n",
      "0.9668165445327759\n",
      "Epoch 26\n",
      "13.953190758824348\n",
      "0.9662301540374756\n",
      "Epoch 27\n",
      "14.091428399085999\n",
      "0.9658627510070801\n",
      "Epoch 28\n",
      "14.281571373343468\n",
      "0.9664732217788696\n",
      "Epoch 29\n",
      "13.63581308722496\n",
      "0.9660665988922119\n",
      "Epoch 30\n",
      "13.847106516361237\n",
      "0.9655248522758484\n",
      "Epoch 31\n",
      "13.72951826453209\n",
      "0.9672750234603882\n",
      "Epoch 32\n",
      "13.734538152813911\n",
      "0.9660537242889404\n",
      "Epoch 33\n",
      "13.777122110128403\n",
      "0.9680215716362\n",
      "Epoch 34\n",
      "13.800354078412056\n",
      "0.9663001298904419\n",
      "Epoch 35\n",
      "13.655103787779808\n",
      "0.9660874009132385\n",
      "Epoch 36\n",
      "13.993770703673363\n",
      "0.9661179184913635\n",
      "Epoch 37\n",
      "14.187026277184486\n",
      "0.9658401608467102\n",
      "Epoch 38\n",
      "14.08176276087761\n",
      "0.9669401049613953\n",
      "Epoch 39\n",
      "13.680228158831596\n",
      "0.9663575291633606\n",
      "Epoch 40\n",
      "13.426718458533287\n",
      "0.9661930799484253\n",
      "Epoch 41\n",
      "13.672035813331604\n",
      "0.967471182346344\n",
      "Epoch 42\n",
      "13.932603552937508\n",
      "0.966646671295166\n",
      "Epoch 43\n",
      "13.652762278914452\n",
      "0.9663758873939514\n",
      "Epoch 44\n",
      "13.60215250402689\n",
      "0.9663940072059631\n",
      "Epoch 45\n",
      "13.948273032903671\n",
      "0.9658129215240479\n",
      "Epoch 46\n",
      "14.158247947692871\n",
      "0.9672079682350159\n",
      "Epoch 47\n",
      "13.714168518781662\n",
      "0.9668121337890625\n",
      "Epoch 48\n",
      "14.531151160597801\n",
      "0.9664973616600037\n",
      "Epoch 49\n",
      "13.467280149459839\n",
      "0.965761125087738\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.L1Loss()\n",
    "factors = list(model.parameters())[:3]\n",
    "other = list(model.parameters())[3:]\n",
    "factor_lr = 0.0005\n",
    "lr = 0.001\n",
    "optimizer = optim.Adam([{\"params\": factors, \"lr\": factor_lr}, {\"params\": other, \"lr\": lr}])\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=2, verbose=True)\n",
    "for e in range(epochs):\n",
    "    print('Epoch {}'.format(e))\n",
    "    train_loss = 0.0\n",
    "    model.train()\n",
    "    for text, audio, vision, label in train_dataloader:\n",
    "        model.zero_grad()\n",
    "        output = model(audio, vision, text)\n",
    "        loss = criterion(output, label)\n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        optimizer.step()\n",
    "    print(train_loss)\n",
    "    \n",
    "    model.eval()\n",
    "    for text, audio, vision, label in valid_dataloader:\n",
    "        output = model(audio, vision, text)\n",
    "        valid_loss = criterion(output, label).item()\n",
    "    scheduler.step(valid_loss)\n",
    "    print(valid_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b77d2bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1e77fab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_sizes = (64, 64, 128)\n",
    "output_size = (1)\n",
    "\n",
    "layer = AdaptiveRankFusionLayer(input_sizes, output_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4baf96b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ParameterList(\n",
       "    (0): Parameter containing: [torch.FloatTensor of size 64x10]\n",
       "    (1): Parameter containing: [torch.FloatTensor of size 64x10]\n",
       "    (2): Parameter containing: [torch.FloatTensor of size 128x10]\n",
       "    (3): Parameter containing: [torch.FloatTensor of size 1x10]\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer.factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bc9af754",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-329.0527, grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer.get_log_prior()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ed9a84f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
