{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c522630-d3bb-461f-8961-25d773d5fbd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from abc import abstractmethod, ABC\n",
    "from tensor_layers import truncated_normal\n",
    "from tensor_layers import low_rank_tensors\n",
    "from tensor_layers import TensorizedLinear\n",
    "import torch.distributions as td\n",
    "\n",
    "import tensorly as tl\n",
    "tl.set_backend('pytorch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f31e9f7a-d332-4dc4-b9a9-8e2071386ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CP_Linear(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_sizes, output_size, init_rank=20, rank_adaptive=True,\n",
    "                 prior_type='log_uniform', em_stepsize=1.0, init_method='nn', eta=None):\n",
    "        \n",
    "        super(CP_Linear, self).__init__()\n",
    "        \n",
    "        self.input_sizes = input_sizes\n",
    "        self.output_size = output_size\n",
    "        \n",
    "        self.dims = input_sizes + (output_size,)\n",
    "        self.order = len(self.dims)\n",
    "        self.init_rank = init_rank\n",
    "        self.prior_type = prior_type\n",
    "        \n",
    "        self._init_factors()\n",
    "        \n",
    "        if rank_adaptive:\n",
    "            self._init_factor_distributions()\n",
    "            self._init_rank_priors_and_prior_distributions()\n",
    "        \n",
    "    def _init_factors(self):\n",
    "        '''\n",
    "        assumming 'nn' init from Cole's implementation\n",
    "        '''\n",
    "        \n",
    "        self.target_stddev = np.sqrt(2/np.prod(self.input_sizes))\n",
    "        \n",
    "        # factor_stddev = {target_stddev / sqrt(init_rank)}^(1 / order)\n",
    "        self.factor_stddev = torch.pow(self.target_stddev / \n",
    "                                       torch.sqrt(torch.tensor(1.0 * self.init_rank)),\n",
    "                                       1.0 / self.order)\n",
    "        \n",
    "        # factor_init_dist = truncated normal distribution\n",
    "        factor_init_dist = truncated_normal.TruncatedNormal(loc=0.0,\n",
    "                                           scale=self.factor_stddev,\n",
    "                                           a=-3.0*self.factor_stddev,\n",
    "                                           b=3.0*self.factor_stddev)\n",
    "        \n",
    "        # factors are sampled from truncated normal distribution \n",
    "        # with shape (dim_i, init_rank) for i=1,2,...,D\n",
    "        self.factors = nn.ParameterList(\n",
    "            [nn.Parameter(factor_init_dist.sample([x, self.init_rank])) \n",
    "             for x in self.dims])\n",
    "    \n",
    "    def _init_factor_distributions(self):\n",
    "        '''\n",
    "        assuming learned_scale == False from Cole's implementation\n",
    "        i.e., we are not learning the variance\n",
    "        '''\n",
    "        factor_scale_multiplier = 1e-9\n",
    "        \n",
    "        # factor_scales = (dim_i, init_rank) matrix with all the elements\n",
    "        #                 equal to the factor_scale_multiplier for i=1,2,...,D\n",
    "        self.factor_scales = [factor_scale_multiplier * torch.ones(factor.shape)\n",
    "                              for factor in self.factors]\n",
    "        \n",
    "        # factor_distributions = Independent(Normal(loc=factors[i], scale=factor_scales[i]), 2) \n",
    "        #                        for i=1,2,...,D\n",
    "        self.factor_distributions = []\n",
    "        for factor, factor_scale in zip(self.factors, self.factor_scales):\n",
    "            self.factor_distributions.append(\n",
    "                td.Independent(td.Normal(loc=factor,\n",
    "                                         scale=factor_scale),\n",
    "                                reinterpreted_batch_ndims=2))\n",
    "            \n",
    "    def _init_rank_priors_and_prior_distributions(self):\n",
    "        \n",
    "        self.rank_parameter = torch.sqrt(\n",
    "            self.get_rank_parameters_update().clone().detach()).view([1,self.init_rank])\n",
    "\n",
    "        self.factor_prior_distributions = []\n",
    "\n",
    "        for x in self.dims:\n",
    "            zero_mean = torch.zeros([x, self.init_rank])\n",
    "            base_dist = td.Normal(loc=zero_mean,scale=self.rank_parameter)\n",
    "            independent_dist = td.Independent(base_dist,reinterpreted_batch_ndims=2)\n",
    "            self.factor_prior_distributions.append(independent_dist)\n",
    "            \n",
    "    def get_rank_parameters_update(self):\n",
    "        \n",
    "        def half_cauchy():\n",
    "\n",
    "            M = torch.sum(torch.stack([torch.sum(torch.square(x.mean) \n",
    "                                                 + torch.square(x.stddev), dim=0) \n",
    "                                       for x in self.factor_distributions]),dim=0)\n",
    "\n",
    "            D = 1.0 * sum(self.dims)\n",
    "\n",
    "            update = (M - D * self.eta**2 + torch.sqrt(torch.square(M) + (2.0 * D + 8.0) * torch.square(torch.tensor(self.eta)) * M +torch.pow(torch.tensor(self.eta), 4.0) * torch.square(torch.tensor(D)))) / (2.0 * D + 4.0)\n",
    "\n",
    "            return update\n",
    "\n",
    "        def log_uniform():\n",
    "\n",
    "            M = torch.sum(torch.stack([torch.sum(torch.square(x.mean) \n",
    "                                                 + torch.square(x.stddev), dim=0)\n",
    "                                       for x in self.factor_distributions]),dim=0)\n",
    "\n",
    "            D = 1.0 * (sum(self.dims) + 1.0)\n",
    "\n",
    "            update = M / D\n",
    "\n",
    "            return update\n",
    "\n",
    "        if self.prior_type == 'log_uniform':\n",
    "            return log_uniform()\n",
    "        elif self.prior_type == 'half_cauchy':\n",
    "            return half_cauchy()\n",
    "        else:\n",
    "            raise ValueError(\"Prior type not supported\")\n",
    "\n",
    "    def update_rank_parameters(self):\n",
    "\n",
    "        with torch.no_grad():\n",
    "            rank_update = self.get_rank_parameters_update()\n",
    "            sqrt_parameter_update = torch.sqrt((1 - self.em_stepsize) * self.rank_parameter.data**2 + self.em_stepsize * rank_update)\n",
    "            self.rank_parameter.data.sub_(self.rank_parameter.data)\n",
    "            self.rank_parameter.data.add_(sqrt_parameter_update.to(self.rank_parameter.device))\n",
    "            \n",
    "    def get_rank_variance(self):\n",
    "        return torch.square(torch.relu(self.rank_parameter))\n",
    "    \n",
    "    def prune_ranks(self, threshold=1e-5):\n",
    "        mask = self.get_rank_variance() < threshold\n",
    "        \n",
    "        # re-define the factors by removing columns from all the factors \n",
    "        # if rank adaptive\n",
    "        # also re-define factor distributions and rank priors and rank prior distributions\n",
    "        \n",
    "        \n",
    "        return self.get_rank_variance() < threshold\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9d1dd5a8-5c70-431e-bc20-f78e7f76a722",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = CP_Linear((10, 20, 30), 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "bfae9322-d25a-423a-805e-3bb370ba25e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ParameterList(\n",
       "    (0): Parameter containing: [torch.FloatTensor of size 10x20]\n",
       "    (1): Parameter containing: [torch.FloatTensor of size 20x20]\n",
       "    (2): Parameter containing: [torch.FloatTensor of size 30x20]\n",
       "    (3): Parameter containing: [torch.FloatTensor of size 3x20]\n",
       ")"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer.factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8ee815c5-eba6-4816-aaf0-8da8a02a496f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2780, 0.2373, 0.2650, 0.2288, 0.2519, 0.2620, 0.2554, 0.2148, 0.2193,\n",
       "         0.2417, 0.2414, 0.2323, 0.2310, 0.2832, 0.2792, 0.2771, 0.2530, 0.2779,\n",
       "         0.2337, 0.2838]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer.rank_parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6a650439-4142-4dd3-8aaf-a10bef3d031d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2780, 0.2373, 0.2650, 0.2288, 0.2519, 0.2620, 0.2554, 0.2148, 0.2193,\n",
       "         0.2417, 0.2414, 0.2323, 0.2310, 0.2832, 0.2792, 0.2771, 0.2530, 0.2779,\n",
       "         0.2337, 0.2838],\n",
       "        [0.2780, 0.2373, 0.2650, 0.2288, 0.2519, 0.2620, 0.2554, 0.2148, 0.2193,\n",
       "         0.2417, 0.2414, 0.2323, 0.2310, 0.2832, 0.2792, 0.2771, 0.2530, 0.2779,\n",
       "         0.2337, 0.2838],\n",
       "        [0.2780, 0.2373, 0.2650, 0.2288, 0.2519, 0.2620, 0.2554, 0.2148, 0.2193,\n",
       "         0.2417, 0.2414, 0.2323, 0.2310, 0.2832, 0.2792, 0.2771, 0.2530, 0.2779,\n",
       "         0.2337, 0.2838],\n",
       "        [0.2780, 0.2373, 0.2650, 0.2288, 0.2519, 0.2620, 0.2554, 0.2148, 0.2193,\n",
       "         0.2417, 0.2414, 0.2323, 0.2310, 0.2832, 0.2792, 0.2771, 0.2530, 0.2779,\n",
       "         0.2337, 0.2838],\n",
       "        [0.2780, 0.2373, 0.2650, 0.2288, 0.2519, 0.2620, 0.2554, 0.2148, 0.2193,\n",
       "         0.2417, 0.2414, 0.2323, 0.2310, 0.2832, 0.2792, 0.2771, 0.2530, 0.2779,\n",
       "         0.2337, 0.2838],\n",
       "        [0.2780, 0.2373, 0.2650, 0.2288, 0.2519, 0.2620, 0.2554, 0.2148, 0.2193,\n",
       "         0.2417, 0.2414, 0.2323, 0.2310, 0.2832, 0.2792, 0.2771, 0.2530, 0.2779,\n",
       "         0.2337, 0.2838],\n",
       "        [0.2780, 0.2373, 0.2650, 0.2288, 0.2519, 0.2620, 0.2554, 0.2148, 0.2193,\n",
       "         0.2417, 0.2414, 0.2323, 0.2310, 0.2832, 0.2792, 0.2771, 0.2530, 0.2779,\n",
       "         0.2337, 0.2838],\n",
       "        [0.2780, 0.2373, 0.2650, 0.2288, 0.2519, 0.2620, 0.2554, 0.2148, 0.2193,\n",
       "         0.2417, 0.2414, 0.2323, 0.2310, 0.2832, 0.2792, 0.2771, 0.2530, 0.2779,\n",
       "         0.2337, 0.2838],\n",
       "        [0.2780, 0.2373, 0.2650, 0.2288, 0.2519, 0.2620, 0.2554, 0.2148, 0.2193,\n",
       "         0.2417, 0.2414, 0.2323, 0.2310, 0.2832, 0.2792, 0.2771, 0.2530, 0.2779,\n",
       "         0.2337, 0.2838],\n",
       "        [0.2780, 0.2373, 0.2650, 0.2288, 0.2519, 0.2620, 0.2554, 0.2148, 0.2193,\n",
       "         0.2417, 0.2414, 0.2323, 0.2310, 0.2832, 0.2792, 0.2771, 0.2530, 0.2779,\n",
       "         0.2337, 0.2838]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer.factor_prior_distributions[0].stddev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7123fbbb-4f36-44fb-ad88-12ca2fab1ff7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
