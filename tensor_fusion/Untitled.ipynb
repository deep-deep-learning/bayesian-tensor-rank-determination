{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "4c522630-d3bb-461f-8961-25d773d5fbd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from abc import abstractmethod, ABC\n",
    "from tensor_layers import truncated_normal\n",
    "from tensor_layers import low_rank_tensors\n",
    "from tensor_layers import TensorizedLinear\n",
    "import torch.distributions as td\n",
    "\n",
    "import tensorly as tl\n",
    "tl.set_backend('pytorch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "f31e9f7a-d332-4dc4-b9a9-8e2071386ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CP_Linear(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_sizes, output_size, max_rank=20, prior_type='log_uniform', \n",
    "                 em_stepsize=1.0, init_method='nn',eta=None):\n",
    "        \n",
    "        super(CP_Linear, self).__init__()\n",
    "        \n",
    "        self.input_sizes = input_sizes\n",
    "        self.output_size = output_size\n",
    "        \n",
    "        self.dims = input_sizes + (output_size,)\n",
    "        self.order = len(self.dims)\n",
    "        self.max_rank = max_rank\n",
    "        self.prior_type = prior_type\n",
    "        \n",
    "        self._init_factors()\n",
    "        self._init_factor_distributions()\n",
    "        self._init_rank_prior()\n",
    "        \n",
    "    def _init_factors(self):\n",
    "        '''\n",
    "        assumming 'nn' init from Cole's implementation\n",
    "        '''\n",
    "        \n",
    "        self.target_stddev = np.sqrt(2/np.prod(self.input_sizes))\n",
    "        \n",
    "        # factor_stddev = {target_stddev / sqrt(max_rank)}^(1 / order)\n",
    "        self.factor_stddev = torch.pow(self.target_stddev / \n",
    "                                       torch.sqrt(torch.tensor(1.0 * self.max_rank)),\n",
    "                                       1.0 / self.order)\n",
    "        \n",
    "        # factor_init_dist = truncated normal distribution\n",
    "        factor_init_dist = truncated_normal.TruncatedNormal(loc=0.0,\n",
    "                                           scale=self.factor_stddev,\n",
    "                                           a=-3.0*self.factor_stddev,\n",
    "                                           b=3.0*self.factor_stddev)\n",
    "        \n",
    "        # factors are sampled from truncated normal distribution \n",
    "        # with shape (dim_i, max_rank) for i=1,2,...,D\n",
    "        self.factors = nn.ParameterList(\n",
    "            [nn.Parameter(factor_init_dist.sample([x, self.max_rank])) \n",
    "             for x in self.dims])\n",
    "    \n",
    "    def _init_factor_distributions(self):\n",
    "        '''\n",
    "        assuming learned_scale == False from Cole's implementation\n",
    "        i.e., we are not learning the variance\n",
    "        '''\n",
    "        factor_scale_multiplier = 1e-9\n",
    "        \n",
    "        # factor_scales = (dim_i, max_rank) matrix with all the elements\n",
    "        #                 equal to the factor_scale_multiplier for i=1,2,...,D\n",
    "        self.factor_scales = [factor_scale_multiplier * torch.ones(factor.shape)\n",
    "                              for factor in self.factors]\n",
    "        \n",
    "        # factor_distributions = Independent(Normal(loc=factors[i], scale=factor_scales[i]), 2) \n",
    "        #                        for i=1,2,...,D\n",
    "        self.factor_distributions = []\n",
    "        for factor, factor_scale in zip(self.factors, self.factor_scales):\n",
    "            self.factor_distributions.append(\n",
    "                td.Independent(td.Normal(loc=factor,\n",
    "                                         scale=factor_scale),\n",
    "                                reinterpreted_batch_ndims=2))\n",
    "            \n",
    "    def _init_rank_prior(self):\n",
    "        \n",
    "        self.rank_parameter = torch.sqrt(\n",
    "            self.get_rank_parameters_update().clone().detach()).view([1,self.max_rank])\n",
    "\n",
    "        self.factor_prior_distributions = []\n",
    "\n",
    "        for x in self.dims:\n",
    "            zero_mean = torch.zeros([x, self.max_rank])\n",
    "            base_dist = td.Normal(loc=zero_mean,scale=self.rank_parameter)\n",
    "            independent_dist = td.Independent(base_dist,reinterpreted_batch_ndims=2)\n",
    "            self.factor_prior_distributions.append(independent_dist)\n",
    "            \n",
    "    def get_rank_parameters_update(self):\n",
    "        \n",
    "        def half_cauchy():\n",
    "\n",
    "            M = torch.sum(torch.stack([torch.sum(torch.square(x.mean) \n",
    "                                                 + torch.square(x.stddev), dim=0) \n",
    "                                       for x in self.factor_distributions]),dim=0)\n",
    "\n",
    "            D = 1.0 * sum(self.dims)\n",
    "\n",
    "            update = (M - D * self.eta**2 + torch.sqrt(torch.square(M) + (2.0 * D + 8.0) * torch.square(torch.tensor(self.eta)) * M +torch.pow(torch.tensor(self.eta), 4.0) * torch.square(torch.tensor(D)))) / (2.0 * D + 4.0)\n",
    "\n",
    "            return update\n",
    "\n",
    "        def log_uniform():\n",
    "\n",
    "            M = torch.sum(torch.stack([torch.sum(torch.square(x.mean) \n",
    "                                                 + torch.square(x.stddev), dim=0)\n",
    "                                       for x in self.factor_distributions]),dim=0)\n",
    "\n",
    "            D = 1.0 * (sum(self.dims) + 1.0)\n",
    "\n",
    "            update = M / D\n",
    "\n",
    "            return update\n",
    "\n",
    "        if self.prior_type == 'log_uniform':\n",
    "            return log_uniform()\n",
    "        elif self.prior_type == 'half_cauchy':\n",
    "            return half_cauchy()\n",
    "        else:\n",
    "            raise ValueError(\"Prior type not supported\")\n",
    "\n",
    "    def update_rank_parameters(self):\n",
    "\n",
    "        with torch.no_grad():\n",
    "            rank_update = self.get_rank_parameters_update()\n",
    "            sqrt_parameter_update = torch.sqrt((1 - self.em_stepsize) * self.rank_parameter.data**2 + self.em_stepsize * rank_update)\n",
    "            self.rank_parameter.data.sub_(self.rank_parameter.data)\n",
    "            self.rank_parameter.data.add_(sqrt_parameter_update.to(self.rank_parameter.device))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "4928098b-e793-489e-a60f-b58fc1b2e15a",
   "metadata": {},
   "outputs": [],
   "source": [
    "l = TensorizedLinear(100, 10, tensor_type='CP', shape=(10,10,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "52ec1033-daba-4478-8125-26f92ae90032",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[0.2609, 0.3344, 0.3115, 0.3825, 0.3885, 0.3099, 0.2874, 0.3719, 0.2414,\n",
       "         0.2997, 0.3410, 0.3306, 0.2563, 0.3023, 0.2519, 0.2998, 0.3102, 0.3000,\n",
       "         0.3021, 0.3274]])"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l.tensor.rank_parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "2d00fc86-ebde-479e-af85-ed3f49d3db76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000e-09, 1.0000e-09, 1.0000e-09, 1.0000e-09, 1.0000e-09, 1.0000e-09,\n",
       "         1.0000e-09, 1.0000e-09, 1.0000e-09, 1.0000e-09, 1.0000e-09, 1.0000e-09,\n",
       "         1.0000e-09, 1.0000e-09, 1.0000e-09, 1.0000e-09, 1.0000e-09, 1.0000e-09,\n",
       "         1.0000e-09, 1.0000e-09],\n",
       "        [1.0000e-09, 1.0000e-09, 1.0000e-09, 1.0000e-09, 1.0000e-09, 1.0000e-09,\n",
       "         1.0000e-09, 1.0000e-09, 1.0000e-09, 1.0000e-09, 1.0000e-09, 1.0000e-09,\n",
       "         1.0000e-09, 1.0000e-09, 1.0000e-09, 1.0000e-09, 1.0000e-09, 1.0000e-09,\n",
       "         1.0000e-09, 1.0000e-09],\n",
       "        [1.0000e-09, 1.0000e-09, 1.0000e-09, 1.0000e-09, 1.0000e-09, 1.0000e-09,\n",
       "         1.0000e-09, 1.0000e-09, 1.0000e-09, 1.0000e-09, 1.0000e-09, 1.0000e-09,\n",
       "         1.0000e-09, 1.0000e-09, 1.0000e-09, 1.0000e-09, 1.0000e-09, 1.0000e-09,\n",
       "         1.0000e-09, 1.0000e-09],\n",
       "        [1.0000e-09, 1.0000e-09, 1.0000e-09, 1.0000e-09, 1.0000e-09, 1.0000e-09,\n",
       "         1.0000e-09, 1.0000e-09, 1.0000e-09, 1.0000e-09, 1.0000e-09, 1.0000e-09,\n",
       "         1.0000e-09, 1.0000e-09, 1.0000e-09, 1.0000e-09, 1.0000e-09, 1.0000e-09,\n",
       "         1.0000e-09, 1.0000e-09],\n",
       "        [1.0000e-09, 1.0000e-09, 1.0000e-09, 1.0000e-09, 1.0000e-09, 1.0000e-09,\n",
       "         1.0000e-09, 1.0000e-09, 1.0000e-09, 1.0000e-09, 1.0000e-09, 1.0000e-09,\n",
       "         1.0000e-09, 1.0000e-09, 1.0000e-09, 1.0000e-09, 1.0000e-09, 1.0000e-09,\n",
       "         1.0000e-09, 1.0000e-09],\n",
       "        [1.0000e-09, 1.0000e-09, 1.0000e-09, 1.0000e-09, 1.0000e-09, 1.0000e-09,\n",
       "         1.0000e-09, 1.0000e-09, 1.0000e-09, 1.0000e-09, 1.0000e-09, 1.0000e-09,\n",
       "         1.0000e-09, 1.0000e-09, 1.0000e-09, 1.0000e-09, 1.0000e-09, 1.0000e-09,\n",
       "         1.0000e-09, 1.0000e-09],\n",
       "        [1.0000e-09, 1.0000e-09, 1.0000e-09, 1.0000e-09, 1.0000e-09, 1.0000e-09,\n",
       "         1.0000e-09, 1.0000e-09, 1.0000e-09, 1.0000e-09, 1.0000e-09, 1.0000e-09,\n",
       "         1.0000e-09, 1.0000e-09, 1.0000e-09, 1.0000e-09, 1.0000e-09, 1.0000e-09,\n",
       "         1.0000e-09, 1.0000e-09],\n",
       "        [1.0000e-09, 1.0000e-09, 1.0000e-09, 1.0000e-09, 1.0000e-09, 1.0000e-09,\n",
       "         1.0000e-09, 1.0000e-09, 1.0000e-09, 1.0000e-09, 1.0000e-09, 1.0000e-09,\n",
       "         1.0000e-09, 1.0000e-09, 1.0000e-09, 1.0000e-09, 1.0000e-09, 1.0000e-09,\n",
       "         1.0000e-09, 1.0000e-09],\n",
       "        [1.0000e-09, 1.0000e-09, 1.0000e-09, 1.0000e-09, 1.0000e-09, 1.0000e-09,\n",
       "         1.0000e-09, 1.0000e-09, 1.0000e-09, 1.0000e-09, 1.0000e-09, 1.0000e-09,\n",
       "         1.0000e-09, 1.0000e-09, 1.0000e-09, 1.0000e-09, 1.0000e-09, 1.0000e-09,\n",
       "         1.0000e-09, 1.0000e-09],\n",
       "        [1.0000e-09, 1.0000e-09, 1.0000e-09, 1.0000e-09, 1.0000e-09, 1.0000e-09,\n",
       "         1.0000e-09, 1.0000e-09, 1.0000e-09, 1.0000e-09, 1.0000e-09, 1.0000e-09,\n",
       "         1.0000e-09, 1.0000e-09, 1.0000e-09, 1.0000e-09, 1.0000e-09, 1.0000e-09,\n",
       "         1.0000e-09, 1.0000e-09]])"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l.tensor.factor_distributions[0].stddev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "9d1dd5a8-5c70-431e-bc20-f78e7f76a722",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = CP_Linear((10, 20, 30), 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "bfae9322-d25a-423a-805e-3bb370ba25e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-8.5084e-02, -4.6769e-01, -1.0849e-01, -2.4412e-01, -1.5046e-01,\n",
       "          7.4886e-02,  4.7293e-01,  1.0978e-02,  1.0956e-01,  4.2085e-01,\n",
       "          3.1141e-01, -1.5022e-01,  5.0437e-01,  2.1918e-01, -3.3649e-01,\n",
       "         -6.7118e-02, -3.3188e-01,  7.3080e-02,  8.0571e-02,  1.1317e-02],\n",
       "        [-1.0309e-01, -4.4172e-02,  4.7485e-01, -3.3470e-02,  1.3069e-01,\n",
       "          1.4269e-02,  2.0047e-01, -6.8038e-02,  2.8591e-01,  6.9216e-02,\n",
       "          1.2770e-02,  2.3743e-01, -3.7915e-01,  1.8009e-01,  1.3312e-02,\n",
       "         -2.0657e-01,  4.0994e-02, -4.5407e-01,  1.7976e-01,  2.2039e-01],\n",
       "        [ 6.3784e-01, -2.4571e-01, -5.0475e-01, -2.2820e-01, -6.1975e-02,\n",
       "         -1.3783e-01,  1.0974e-01, -3.0980e-02, -6.3791e-01, -1.8114e-01,\n",
       "         -5.2292e-01, -5.6264e-01,  1.5986e-01,  4.6694e-01, -3.6123e-01,\n",
       "         -2.0816e-01,  4.5117e-01,  2.2959e-01,  7.4908e-03,  9.1826e-02],\n",
       "        [ 2.8955e-01,  7.7498e-02,  6.2985e-02,  1.7664e-01,  3.1129e-02,\n",
       "          1.1922e-02,  5.4064e-01,  1.5170e-01, -4.6638e-01, -3.8126e-01,\n",
       "         -7.5348e-03, -8.1954e-02,  1.8160e-01, -4.0562e-01, -6.7887e-02,\n",
       "          1.2245e-01,  1.9246e-02, -9.6591e-02, -1.8742e-01,  1.3554e-01],\n",
       "        [ 1.0007e-01, -3.0560e-01,  5.3460e-02, -2.2066e-01, -4.8533e-02,\n",
       "          2.8237e-01, -4.2016e-02, -9.5061e-02, -4.7139e-01,  5.0069e-02,\n",
       "          8.6668e-02, -2.6293e-01, -5.0268e-01,  1.3298e-01, -8.5067e-02,\n",
       "         -3.6927e-01, -5.0708e-01,  1.7449e-02,  2.6922e-01, -2.8510e-01],\n",
       "        [ 2.0077e-02, -1.8274e-01, -3.8260e-01,  3.5371e-01,  1.7275e-01,\n",
       "         -5.3259e-01, -7.7577e-02, -2.0593e-02,  1.9578e-01, -1.4086e-01,\n",
       "          1.1648e-01,  2.1371e-01,  4.0561e-02, -1.3053e-01,  2.4999e-01,\n",
       "         -3.6209e-01,  1.5417e-01,  2.3140e-01,  1.0878e-01, -1.7400e-02],\n",
       "        [ 3.1316e-01, -1.6673e-01,  7.7625e-02,  1.6858e-01,  3.2162e-03,\n",
       "         -2.7175e-02,  2.3533e-01,  3.3339e-01,  1.8504e-01,  1.1320e-01,\n",
       "         -3.6570e-01, -8.0627e-02,  3.1806e-01, -2.9114e-01, -1.8202e-01,\n",
       "         -7.9517e-02,  1.3626e-01, -1.0255e-01,  2.4038e-01,  4.2751e-01],\n",
       "        [ 4.5958e-02,  2.1474e-02,  2.8567e-01, -2.9078e-01, -9.9239e-02,\n",
       "         -4.7391e-01, -5.1751e-02, -2.3830e-01, -4.1756e-01, -1.4582e-01,\n",
       "         -1.4636e-01,  1.3082e-03, -6.7237e-02,  4.6105e-01, -1.7907e-01,\n",
       "         -6.8972e-02,  3.7453e-01, -1.0353e-01,  2.5705e-01,  2.7795e-01],\n",
       "        [ 1.4858e-01, -1.8850e-01,  8.8369e-02,  9.4975e-02, -4.2680e-01,\n",
       "          1.0268e-01, -1.8823e-01, -8.6489e-02, -1.5716e-01,  4.7550e-01,\n",
       "         -1.0417e-01,  1.1519e-01,  1.7027e-01, -2.4320e-01, -2.2174e-01,\n",
       "          3.7582e-01,  1.4417e-01,  1.6362e-01, -6.7955e-03, -9.2861e-02],\n",
       "        [-4.2221e-03,  2.3256e-01,  2.5683e-02,  1.7601e-03,  1.1992e-01,\n",
       "         -1.9650e-01,  9.2164e-02, -6.1085e-04,  4.1446e-02,  7.5932e-02,\n",
       "         -1.8231e-01, -8.6985e-03,  3.2472e-01,  2.9061e-01, -8.2769e-02,\n",
       "          2.4007e-01,  4.6284e-01, -4.9699e-01, -3.8540e-02,  9.3731e-02]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer.factor_distributions[0].mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "5a49b11b-4227-48fd-95d9-f7ae8453dc55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer.factor_prior_distributions[0].mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9f9231cb-82a0-488e-b095-c48da3a1004e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Low_Rank_Tensor(nn.Module):\n",
    "    \n",
    "    def __init__(self, dims, max_rank, prior_type, em_stepsize, init_method,\n",
    "                target_stddev, eta, learned_scale):\n",
    "        \n",
    "        super(Low_Rank_Tensor, self).__init__()\n",
    "        \n",
    "        self.eps = 1e-12\n",
    "        \n",
    "        self.dims = dims\n",
    "        self.order = len(dims)\n",
    "        self.max_rank = max_rank\n",
    "        self.prior_type = prior_type\n",
    "        self.em_stepsize = em_stepsize\n",
    "        self.init_method = init_method\n",
    "        self.target_stddev = target_stddev\n",
    "        self.learned_scale = learned_scale\n",
    "        \n",
    "        self.trainable_variables = []\n",
    "        \n",
    "        self._build_factors()\n",
    "        self._build_factor_distributions()\n",
    "        self._build_low_rank_prior()\n",
    "        \n",
    "    def add_variable(self, init_value, trainable=True):\n",
    "        \n",
    "        new_variable = nn.Parameter(init_value.clone().detach(), requires_grad=trainable)\n",
    "        self.trainable_variables.append(new_variable)\n",
    "        \n",
    "        return new_variable\n",
    "    \n",
    "    @abstractmethod\n",
    "    def _build_factors(self):\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def _build_factor_distributions(self):\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def _build_low_rank_prior(self):\n",
    "        pass\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "c3e6b8e8-be18-4ac3-be5e-c2a4507ffdc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CP(Low_Rank_Tensor):\n",
    "    \n",
    "    def __init__(self, dims, max_rank, prior_type, em_stepsize, init_method,\n",
    "                target_stddev, eta, learned_scale=True):\n",
    "        \n",
    "        super().__init__(dims, max_rank, prior_type, em_stepsize, init_method,\n",
    "                         target_stddev, eta, learned_scale)\n",
    "        \n",
    "    def _build_factors(self):\n",
    "        \n",
    "        # assume that init_method is 'nn'\n",
    "        \n",
    "        \n",
    "        # factor_stddev = {target_stddev / sqrt(max_rank)}^(1 / order)\n",
    "        self.factor_stddev = torch.pow(self.target_stddev / \n",
    "                                       torch.sqrt(torch.tensor(1.0*self.max_rank)),\n",
    "                                       1.0 / self.order)\n",
    "        \n",
    "        # init_dist = truncated normal distribution\n",
    "        init_dist = truncated_normal.TruncatedNormal(loc=0.0, scale=self.factor_stddev,\n",
    "                                                    a=-3.0*self.factor_stddev,\n",
    "                                                    b=3.0*self.factor_stddev)\n",
    "        \n",
    "        # factors = sampled from truncated normal distribution\n",
    "        #           with shape (dim_i, max_rank) for i=1,2,...,D\n",
    "        self.weights = torch.ones([self.max_rank])\n",
    "        self.factors = [init_dist.sample(sample_shape=[x, self.max_rank]) \n",
    "                        for x in self.dims]\n",
    "        \n",
    "        self.weights = None\n",
    "        self.factors = [self.add_variable(x) for x in self.factors]\n",
    "        \n",
    "    def _build_factor_distributions(self):\n",
    "        \n",
    "        factor_scale_multiplier = 1e-9\n",
    "        \n",
    "        # factor_scales = (dim_i, max_rank) matrix with all the elements\n",
    "        #                 equal to the factor_scale_multiplier for i=1,2,...,D\n",
    "        factor_scales = [self.add_variable(factor_scale_multiplier * \n",
    "                                           torch.ones(factor.shape),\n",
    "                                           trainable=self.learned_scale)\n",
    "                        for factor in self.factors]\n",
    "        \n",
    "        # factor_distributions = Independent(Normal(loc=factors[i], scale=factor_scales[i]), 2) \n",
    "        #                        for i=1,2,...,D\n",
    "        self.factor_distributions = []\n",
    "        for factor, factor_scale in zip(self.factors, factor_scales):\n",
    "            self.factor_distributions.append(\n",
    "                td.Independent(base_distribution=td.Normal(loc=factor, scale=factor_scale),\n",
    "                               reinterpreted_batch_ndims=2))\n",
    "            \n",
    "    def _build_low_rank_prior(self):\n",
    "        # lambda \n",
    "        self.rank_parameter = self.add_variable(torch.sqrt(self.get_rank_parameters_update().clone().detach()).view([1, self.max_rank]),\n",
    "                                                trainable=False)\n",
    "        \n",
    "        self.factor_prior_distributions = []\n",
    "        for x in self.dims:\n",
    "            zero_mean = torch.zeros([x, self.max_rank])\n",
    "            base_dist = td.Normal(loc=zero_mean, scale=self.rank_parameter)\n",
    "            self.factor_prior_distributions.append(td.Independent(base_dist, reinterpreted_batch_ndims=2))\n",
    "        \n",
    "    def get_rank_parameters_update(self):\n",
    "\n",
    "        def log_uniform():\n",
    "\n",
    "            M = torch.sum(to rch.stack([torch.sum(torch.square(x.mean) + \n",
    "                                                 torch.square(x.stddev), dim=0)\n",
    "                                       for x in self.factor_distributions]),dim=0)\n",
    "\n",
    "            D = 1.0 * (sum(self.dims) + 1.0)\n",
    "\n",
    "            update = M / D\n",
    "\n",
    "            return update\n",
    "        \n",
    "        return log_uniform()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "02675990-f17f-4d15-8533-4f2d1eff6162",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = Tensorized_Linear((2, 2, 2), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "2412d28e-4b02-45ba-826c-22f36711e482",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 20])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer.tensor.rank_parameter.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "af5bbbc3-14cc-4bb4-b0ed-b1e1e21c8a01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Independent(Normal(loc: torch.Size([2, 20]), scale: torch.Size([2, 20])), 2),\n",
       " Independent(Normal(loc: torch.Size([2, 20]), scale: torch.Size([2, 20])), 2),\n",
       " Independent(Normal(loc: torch.Size([2, 20]), scale: torch.Size([2, 20])), 2),\n",
       " Independent(Normal(loc: torch.Size([1, 20]), scale: torch.Size([1, 20])), 2)]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer.tensor.factor_distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "82a23f13-1fd5-421f-8503-f1482f263c04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Independent(Normal(loc: torch.Size([2, 20]), scale: torch.Size([2, 20])), 2),\n",
       " Independent(Normal(loc: torch.Size([2, 20]), scale: torch.Size([2, 20])), 2),\n",
       " Independent(Normal(loc: torch.Size([2, 20]), scale: torch.Size([2, 20])), 2),\n",
       " Independent(Normal(loc: torch.Size([1, 20]), scale: torch.Size([1, 20])), 2)]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer.tensor.factor_prior_distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20eb589c-d088-4100-b420-8826cfcc39fd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
