{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f86c40f2-cd11-424e-8c27-1861278352e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/tensor_fusion/lib/python3.9/site-packages/tltorch/factorized_tensors/core.py:145: UserWarning: Creating a subclass of FactorizedTensor TensorizedTensor with no name.\n",
      "  warnings.warn(f'Creating a subclass of FactorizedTensor {cls.__name__} with no name.')\n"
     ]
    }
   ],
   "source": [
    "from datasets import get_cmu_mosi_dataset\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import tltorch\n",
    "from tltorch.factorized_layers.factorized_linear import FactorizedLinear\n",
    "from tltorch.factorized_tensors import TensorizedTensor\n",
    "from layers import FactorizedLinearCP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ffa3b4b5-cf0a-4a46-a7f3-cf5533adde06",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set, valid_set, test_set = get_cmu_mosi_dataset(binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "25ba4171-5b04-481d-8756-809468914f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "batch = train_set[:batch_size][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "80f98a75-c763-40e7-888b-585da217e02f",
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn = nn.LSTM(input_size=300, hidden_size=128, num_layers=1, bidirectional=False, batch_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "041fcaf1-6016-487e-aeb9-10d95b179ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "output, (h_n, c_n) = rnn(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bdecc66c-2163-47a3-a40f-471d6a167e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_list = list(rnn.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "638e1e15-e1ed-44d0-97d5-3c290abf67ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "W_ii = param_list[0][:128]\n",
    "W_if = param_list[0][128:256]\n",
    "W_ig = param_list[0][256:384]\n",
    "W_io = param_list[0][384:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2599a0e1-f4e3-4bec-b718-44ca01ccaf06",
   "metadata": {},
   "outputs": [],
   "source": [
    "W_hi = param_list[1][:128]\n",
    "W_hf = param_list[1][128:256]\n",
    "W_hg = param_list[1][256:384]\n",
    "W_ho = param_list[1][384:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d8e3d78f-3c50-47b8-a85f-2efd0f6c3c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "b_ii = param_list[2][:128]\n",
    "b_if = param_list[2][128:256]\n",
    "b_ig = param_list[2][256:384]\n",
    "b_io = param_list[2][384:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c9cb844c-f525-459a-9be9-abd2fe5ad931",
   "metadata": {},
   "outputs": [],
   "source": [
    "b_hi = param_list[3][:128]\n",
    "b_hf = param_list[3][128:256]\n",
    "b_hg = param_list[3][256:384]\n",
    "b_ho = param_list[3][384:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e12724bb-a6c8-4699-a66c-85848dae3ee7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorizing (in, out)=((128, 300)) -> (((4, 4, 8), (4, 5, 15)))\n"
     ]
    }
   ],
   "source": [
    "tensorized_shape = tltorch.utils.get_tensorized_shape(128, 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c5895b2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/tensor_fusion/lib/python3.9/site-packages/tensorly/backend/core.py:885: UserWarning: In partial_svd: converting to NumPy. Check SVD_FUNS for available alternatives if you want to avoid this.\n",
      "  warnings.warn('In partial_svd: converting to NumPy.'\n"
     ]
    }
   ],
   "source": [
    "layer_ii = FactorizedLinearCP(tensorized_shape[1], tensorized_shape[0], bias=True)\n",
    "layer_ii.from_matrix(W_ii.data, b_ii.data)\n",
    "layer_if = FactorizedLinearCP(tensorized_shape[1], tensorized_shape[0], bias=True)\n",
    "layer_if.from_matrix(W_if.data, b_if.data)\n",
    "layer_ig = FactorizedLinearCP(tensorized_shape[1], tensorized_shape[0], bias=True)\n",
    "layer_ig.from_matrix(W_ig.data, b_ig.data)\n",
    "layer_io = FactorizedLinearCP(tensorized_shape[1], tensorized_shape[0], bias=True)\n",
    "layer_io.from_matrix(W_io.data, b_io.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4e49b117",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorizing (in, out)=((128, 128)) -> (((4, 4, 8), (4, 4, 8)))\n"
     ]
    }
   ],
   "source": [
    "tensorized_shape = tltorch.utils.get_tensorized_shape(128, 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "077fc098",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_hi = FactorizedLinearCP(tensorized_shape[1], tensorized_shape[0], bias=True)\n",
    "layer_hi.from_matrix(W_hi.data, b_hi.data)\n",
    "layer_hf = FactorizedLinearCP(tensorized_shape[1], tensorized_shape[0], bias=True)\n",
    "layer_hf.from_matrix(W_hf.data, b_hf.data)\n",
    "layer_hg = FactorizedLinearCP(tensorized_shape[1], tensorized_shape[0], bias=True)\n",
    "layer_hg.from_matrix(W_hg.data, b_hg.data)\n",
    "layer_ho = FactorizedLinearCP(tensorized_shape[1], tensorized_shape[0], bias=True)\n",
    "layer_ho.from_matrix(W_ho.data, b_ho.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c0cbca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "c = torch.zeros((batch_size, 128))\n",
    "h = torch.zeros((batch_size, 128))\n",
    "for seq in range(20):\n",
    "    i = torch.sigmoid(layer_ii(batch[:,seq,:]))\n",
    "    f = torch.sigmoid(layer_if(batch[:,seq,:]))\n",
    "    g = torch.tanh(layer_ig(batch[:,seq,:]))\n",
    "    o = torch.sigmoid(layer_io(batch[:,seq,:]))\n",
    "    c = f * c + i * g\n",
    "    h = o * torch.tanh(c)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0bac7db2-28f6-42fd-9efc-2a192973edea",
   "metadata": {},
   "outputs": [],
   "source": [
    "c = torch.zeros((batch_size, 128))\n",
    "h = torch.zeros((batch_size, 128))\n",
    "for seq in range(20):\n",
    "    i = torch.sigmoid(batch[:,seq,:] @ W_ii.T + b_ii + h @ W_hi.T + b_hi)\n",
    "    f = torch.sigmoid(batch[:,seq,:] @ W_if.T + b_if + h @ W_hf.T + b_hf)\n",
    "    g = torch.tanh(batch[:,seq,:] @ W_ig.T + b_ig + h @ W_hg.T + b_hg)\n",
    "    o = torch.sigmoid(batch[:,seq,:] @ W_io.T + b_io + h @ W_ho.T + b_ho)\n",
    "    c = f * c + i * g\n",
    "    h = o * torch.tanh(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cb4c68f5-be2d-481c-8d8a-d7615f8d46ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearCP(FactorizedLinear):\n",
    "    \n",
    "    def __init__(self, in_tensorized_features, out_tensorized_features, bias=False,\n",
    "                 max_rank=10, device=None, dtype=None):\n",
    "        '''\n",
    "        args:\n",
    "            in_tensorized_features: a tuple of ints, (in_size_1, in_size_2, ..., in_size_n) \n",
    "            out_tensorized_features: a tuple of ints, (out_size_1, out_size_2, ..., out_size_m)\n",
    "            bias: a boolean, True for bias False for no bias\n",
    "            max_rank: maximum rank for CP decomposition of weight\n",
    "        '''\n",
    "        \n",
    "        super(LinearCP, self).__init__(in_tensorized_features, out_tensorized_features, bias,\n",
    "                                               factorization='cp', rank=max_rank, n_layers=1, \n",
    "                                               device=device, dtype=dtype)\n",
    "        self.max_rank = max_rank\n",
    "        self.n_input_factors = len(in_tensorized_features)\n",
    "        self.n_output_factors = len(out_tensorized_features)\n",
    "    \n",
    "    def from_matrix(self, matrix, bias=None):\n",
    "        \n",
    "        self.weight = TensorizedTensor.from_matrix(matrix, \n",
    "                                                   self.out_tensorized_features, \n",
    "                                                   self.in_tensorized_features, \n",
    "                                                   self.max_rank, \n",
    "                                                   factorization='CP')\n",
    "        if bias is None:\n",
    "            self.bias = bias\n",
    "        else:\n",
    "            self.bias = nn.Parameter(bias)       \n",
    "        \n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        X @ W.T + b\n",
    "        \n",
    "        factors are in the order of [out_factors, in_factors]\n",
    "        '''\n",
    "        \n",
    "        # tensorize input\n",
    "        output = x.reshape((x.shape[0],) + self.in_tensorized_features)\n",
    "        print(output.shape)\n",
    "        \n",
    "        # forward propagate with input factors\n",
    "        output = torch.einsum('na...,ar->n...r', output, self.weight.factors[self.n_output_factors])\n",
    "        for factor in self.weight.factors[self.n_output_factors+1:]:\n",
    "            output = torch.einsum('na...r,ar->n...r', output, factor)\n",
    "            \n",
    "        # forward propagate with output factors\n",
    "        for factor in self.weight.factors[:self.n_output_factors-1]:\n",
    "            output = torch.einsum('n...r,ar->n...ar', output, factor)\n",
    "        output = torch.einsum('n...r,ar->n...a', output, self.weight.factors[self.n_output_factors-1])\n",
    "        \n",
    "        # vectorize output\n",
    "        output = output.reshape((x.shape[0], self.out_features))\n",
    "        \n",
    "        # add bias\n",
    "        if self.bias is not None:\n",
    "            output = output + self.bias\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "02e392b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = LinearCP(tensorized_shape[1], tensorized_shape[0], bias=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d2f8dfb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/tensor_fusion/lib/python3.9/site-packages/tensorly/backend/core.py:885: UserWarning: In partial_svd: converting to NumPy. Check SVD_FUNS for available alternatives if you want to avoid this.\n",
      "  warnings.warn('In partial_svd: converting to NumPy.'\n"
     ]
    }
   ],
   "source": [
    "layer.from_matrix(W_ii.data, b_ii.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dc3d1a2c-c444-49c8-b628-c62aa53a4ad5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 4, 5, 15])\n"
     ]
    }
   ],
   "source": [
    "out = layer(batch[:,0,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5f5834d8-731f-4475-9bd0-0b8a2cbda087",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_ = batch[:,0,:] @ W_ii.T + b_ii"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2d26486a-4b99-4ace-9aaf-fbbc5a10cc43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "         True, True, True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "         True, True, True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "         True, True, True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "         True, True, True, True, True, True, True, True]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.isclose(out, out_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9700889f-5eb5-45a2-b49e-a0a4fce7e3dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearCP_from_matrix(FactorizedLinear):\n",
    "    \n",
    "    def __init__(self, in_tensorized_features, out_tensorized_features, matrix, bias=None, has_bias=False,\n",
    "                 max_rank=10, device=None, dtype=None):\n",
    "        \n",
    "        super(LinearCP_from_matrix, self).__init__(in_tensorized_features, out_tensorized_features, has_bias,\n",
    "                                               factorization='cp', rank=max_rank, n_layers=1, \n",
    "                                               device=device, dtype=dtype)\n",
    "        \n",
    "        # replace weight and bias \n",
    "        self.weight = TensorizedTensor.from_matrix(matrix, out_tensorized_features, in_tensorized_features, max_rank, factorization='CP')\n",
    "        self.bias = nn.Parameter(bias)\n",
    "        self.n_input_factors = len(in_tensorized_features)\n",
    "        self.n_output_factors = len(out_tensorized_features)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        X @ W.T\n",
    "        '''\n",
    "        \n",
    "        # tensorize input\n",
    "        output = x.reshape((x.shape[0],) + self.in_tensorized_features)\n",
    "        \n",
    "        # forward propagate with input factors\n",
    "        output = torch.einsum('na...,ar->n...r', output, self.weight.factors[self.n_output_factors])\n",
    "        for factor in self.weight.factors[self.n_output_factors+1:]:\n",
    "            output = torch.einsum('na...r,ar->n...r', output, factor)\n",
    "            \n",
    "        # forward propagate with output factors\n",
    "        for factor in self.weight.factors[:self.n_output_factors-1]:\n",
    "            output = torch.einsum('n...r,ar->n...ar', output, factor)\n",
    "        output = torch.einsum('n...r,ar->n...a', output, self.weight.factors[self.n_output_factors-1])\n",
    "        \n",
    "        # vectorize output\n",
    "        output = output.reshape((x.shape[0], self.out_features))\n",
    "        \n",
    "        # add bias\n",
    "        if self.bias is not None:\n",
    "            output = output + self.bias\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cb4c68f5-be2d-481c-8d8a-d7615f8d46ae",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'FactorizedLinear' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/xr/p4pqzm995nbcl9w6d8lrb5zh0000gn/T/ipykernel_61142/1953702912.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mLinearCP\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFactorizedLinear\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     def __init__(self, in_tensorized_features, out_tensorized_features, bias=False,\n\u001b[1;32m      4\u001b[0m                  max_rank=10, device=None, dtype=None):\n\u001b[1;32m      5\u001b[0m         '''\n",
      "\u001b[0;31mNameError\u001b[0m: name 'FactorizedLinear' is not defined"
     ]
    }
   ],
   "source": [
    "class LinearCP(FactorizedLinear):\n",
    "    \n",
    "    def __init__(self, in_tensorized_features, out_tensorized_features, bias=False,\n",
    "                 max_rank=10, device=None, dtype=None):\n",
    "        '''\n",
    "        args:\n",
    "            in_tensorized_features: a tuple of ints, (in_size_1, in_size_2, ..., in_size_n) \n",
    "            out_tensorized_features: a tuple of ints, (out_size_1, out_size_2, ..., out_size_m)\n",
    "            bias: a boolean, True for bias False for no bias\n",
    "            max_rank: maximum rank for CP decomposition of weight\n",
    "        '''\n",
    "        \n",
    "        super(LinearCP, self).__init__(in_tensorized_features, out_tensorized_features, bias,\n",
    "                                               factorization='cp', rank=max_rank, n_layers=1, \n",
    "                                               device=device, dtype=dtype)\n",
    "        self.max_rank = max_rank\n",
    "        self.n_input_factors = len(in_tensorized_features)\n",
    "        self.n_output_factors = len(out_tensorized_features)\n",
    "    \n",
    "    def from_matrix(self, matrix, bias=None):\n",
    "        \n",
    "        self.weight = TensorizedTensor.from_matrix(matrix, \n",
    "                                                   self.out_tensorized_features, \n",
    "                                                   self.in_tensorized_features, \n",
    "                                                   self.max_rank, \n",
    "                                                   factorization='CP')\n",
    "        if bias is None:\n",
    "            self.bias = bias\n",
    "        else:\n",
    "            self.bias = nn.Parameter(bias)       \n",
    "        \n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        X @ W.T + b\n",
    "        \n",
    "        factors are in the order of [out_factors, in_factors]\n",
    "        '''\n",
    "        \n",
    "        # tensorize input\n",
    "        output = x.reshape((x.shape[0],) + self.in_tensorized_features)\n",
    "        print(output.shape)\n",
    "        \n",
    "        # forward propagate with input factors\n",
    "        output = torch.einsum('na...,ar->n...r', output, self.weight.factors[self.n_output_factors])\n",
    "        for factor in self.weight.factors[self.n_output_factors+1:]:\n",
    "            output = torch.einsum('na...r,ar->n...r', output, factor)\n",
    "            \n",
    "        # forward propagate with output factors\n",
    "        for factor in self.weight.factors[:self.n_output_factors-1]:\n",
    "            output = torch.einsum('n...r,ar->n...ar', output, factor)\n",
    "        output = torch.einsum('n...r,ar->n...a', output, self.weight.factors[self.n_output_factors-1])\n",
    "        \n",
    "        # vectorize output\n",
    "        output = output.reshape((x.shape[0], self.out_features))\n",
    "        \n",
    "        # add bias\n",
    "        if self.bias is not None:\n",
    "            output = output + self.bias\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "258b0ba7-78ea-401c-afa7-2aa2c8c35409",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = LinearCP((2,3),(4, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "0027cd8e-f11f-4f1a-919a-23dc530221e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 2, 3])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn((5, 6))\n",
    "output = layer(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "b9ce882f-01c8-47c1-8d43-1acdf3867250",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_ = x @ layer.weight.to_matrix().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "a9fbe245-e2c7-48cd-b98d-719087d54914",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
       "        [ True,  True,  True,  True,  True, False,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
       "        [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
       "        [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
       "        [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True]])"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.isclose(output, output_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "4ebc09a0-1f9c-47e9-a172-d9d4a175f890",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 10])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer.weight.factors[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b433f04c-9014-423d-a7f9-3a5d3f28915c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer.weight.order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e46f2e05-238f-4ef0-9e91-5d5ef9ab6f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cp_times_matrix_fwd(tensor, matrix):\n",
    "    \"\"\"\n",
    "    Multiplies a tensorly CP tensorized matrix and an input matrix\n",
    "    \n",
    "    X @ W\n",
    "    \"\"\"\n",
    "    \n",
    "    order = len(tensor.tensorized_shape[0])\n",
    "    saved_tensors = []\n",
    "\n",
    "    # tensorize the input\n",
    "    output = matrix.reshape((matrix.shape[0],) + tensor.tensorized_shape[0])\n",
    "    saved_tensors.append(output)\n",
    "\n",
    "    # forward propagate with input factors\n",
    "    output = torch.einsum('na...,ar->n...r', output, tensor.factors[0])\n",
    "    saved_tensors.append(output)\n",
    "    for factor in tensor.factors[1:order]:\n",
    "        output = torch.einsum('na...r,ar->n...r', output, factor)\n",
    "        saved_tensors.append(output)\n",
    "\n",
    "    # forward propagate with output factors\n",
    "    for factor in tensor.factors[order:tensor.order-1]:\n",
    "        output = torch.einsum('n...r,ar->n...ar', output, factor)\n",
    "        saved_tensors.append(output)\n",
    "    output = torch.einsum('n...r,ar->n...a', output, tensor.factors[-1])\n",
    "    \n",
    "    # vectorize the output\n",
    "    output = output.reshape((matrix.shape[0], tensor.shape[1]))\n",
    "    \n",
    "    return output, saved_tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eb5a5bc-a61d-417e-b583-5b24772e668e",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### LSTM\n",
    "\n",
    "For each element in the input sequence, each layer computes the following function:\n",
    "\n",
    "$i_t = \\sigma(W_{ii}x_t + b_{ii} + W_{hi}h_{t-1} +b_{hi})$  \n",
    "$f_t = \\sigma(W_{if}x_t + b_{if} + W_{hf}h_{t-1} +b_{hf})$  \n",
    "$g_t = \\tanh(W_{ig}x_t + b_{ig} + W_{hg}h_{t-1} +b_{hg})$  \n",
    "$o_t = \\sigma(W_{io}x_t + b_{io} + W_{ho}h_{t-1} + b_{ho})$  \n",
    "$c_t = f_t \\odot c_{t-1} + i_t \\odot g_t$  \n",
    "$h_t = o_t \\odot \\tanh(c_t)$  \n",
    "\n",
    "where $h_t$ is the hidden state at time $t$, $c_t$ is the cell state at time $t$, $x_t$ is the input at time $t$, $h_{t-1}$ is the hidden state of the layer at time $t-1$ or the initial hidden state at time $o$, and $i_t, f_t, o_t$ are the input, forget,cell and output gates, respectively. $\\sigma$ is the sigmoid function, and $\\odot$ is the Hadamard product.\n",
    "\n",
    "In a multilayer LSTM, the input $x_t^{(l)}$ of the $l$-th layer $(l \\geq 2)$ is the hidden state $h_t^{(l-1)}$ of the previous layer multiplied by dropout $\\delta_t^{(l-1)}$ where each $\\delta_t^{(l-1)}$ is a Bernoulli random variable which is 0 with probability dropout.\n",
    "\n",
    "If proj_size > 0 is specified, LSTM with projections will be used. This changes the LSTM cell in the following way. First, the diemnsion of $h_t$ will be changed from hidden_size to proj_size (dimensions of $W_{hi}$ will change accordingly). Second, the output hidden state of each layer will be multiplied by a learnable projection matrix: $h_t = W_{hr}h_t$. Note that as a consequence of this, the output of LSTM network will be of different shape as well.\n",
    "\n",
    "##### Parameters\n",
    "\n",
    "* **input_size** - the number of expected features in the input $x$\n",
    "* **hidden_size** - the number of features in the hidden state $h$\n",
    "* **num_layers** - the number of recurrent layers. E.g., setting num_layers=2 would mean stacking two LSTMs together to form a stacked LSTM, with the second LSTM taking in outputs of the first LSTM and computing the final results. Default: 1\n",
    "* **bias** - if False, then the layer does not use bias weights $b_ih$ and $b_hh$. Default: True\n",
    "* **batch_first** - if True, then the input ad output tensors are provided as (batch,seq,feature) instead of (seq,batch,feature). Note that this does not apply to hidden or cell states. Default: False\n",
    "* **dropout** - if non-zero, introduces a Dropout layer on the outputs of each LSTM layer except the last layer, with dropout pro ability equal to drpout. Default: 0\n",
    "* **bidirectional** - if True, becomes a bidirectional LSTM. Default: False\n",
    "* **proj_size** - if > 0, will use LSTM with projections of corresponding size. Default: 0\n",
    "\n",
    "\n",
    "##### Inputs: input, (h_0,c_0)\n",
    "\n",
    "* **input**: tensor of shape $(L,N,H_{in})$ when batch_first=False or $(N,L,H_{in})$ when batch_first=True containing the features of the input sequence.  \n",
    "* **h_0**: tensor of shape $(D*$num_layers$,N,H_{out})$ containing the initial hidden state for each element in the batch. Defaults to zeros if (h_0,c_0) is not provided.\n",
    "* **c_0**: tensor of shape $(D*$num_layers$,N,H_{cell})$ containing the initial cell state for each element in the batch. Defaults to zeros if (h_0,c_0) is not provided.\n",
    "\n",
    "where:\n",
    "\n",
    "\n",
    "$N$ = batch size  \n",
    "$L$ = sequence length  \n",
    "$D$ = 2 if bidirectional=True otherwise 1  \n",
    "$H_{in}$ = input_size  \n",
    "$H_{cell}$ = hidden_size  \n",
    "$H_{out}$ = proj_size if proj_size > 0 otherwise hidden_size\n",
    "\n",
    "\n",
    "##### Outputs: output, (h_n,c_n)\n",
    "\n",
    "* **output**: tensor of shape $(L,N,D*H_{out})$ when batch_first=False or $(N,L,D*H_{out})$ when batch_first=True containing the output features $(h_t)$ from the last layer of the LSTMk for each $t$.\n",
    "* **h_n**: tensor of shape $(D*$num_layers$,N,H_{out})$ containing the final hidden state for eah element in the batch.\n",
    "* **c_n**: tensor of shape $(D*$num_layers$,N,H_{cell})$ containing the final cell state for each element in the batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1445733d-a495-4548-9b00-c1401911ee9e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc-autonumbering": false,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
