{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/christian_lee/anaconda3/envs/tensor_layers/lib/python3.8/site-packages/tltorch/factorized_tensors/core.py:145: UserWarning: Creating a subclass of FactorizedTensor TensorizedTensor with no name.\n",
      "  warnings.warn(f'Creating a subclass of FactorizedTensor {cls.__name__} with no name.')\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import tltorch\n",
    "import numpy as np\n",
    "from tensor_fusion.model import AdaptiveRankFactorizedTextSubNet, SubNet\n",
    "from tensor_fusion.fusion_layer import AdaptiveRankFusionLayer\n",
    "device='cuda'\n",
    "DTYPE=torch.float64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.01\n",
      "0.01\n"
     ]
    }
   ],
   "source": [
    "TextSubNet = AdaptiveRankFactorizedTextSubNet(300, 128, 64, dropout=0.0, prior_type='half_cauchy', eta=0.01, device=device, dtype=DTYPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.randn((4, 20, 300), device=device, dtype=DTYPE)\n",
    "out = TextSubNet(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 64])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = AdaptiveRankFactorizedLinear(300, 128, max_rank=10, tensor_type='CP', prior_type='half_cauchy', eta=0.01, device=device, dtype=DTYPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.randn((1, 300), device=device, dtype=DTYPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = layer(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = layer.weight_tensor.get_full().reshape(300, 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_ = a @ W "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.allclose(out_, out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaptiveRankFactorizedLSTM(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, bias=True, dropout=0.0,\n",
    "                 max_rank=20, tensor_type='CP', prior_type='half_cauchy', eta=None,\n",
    "                 device=None, dtype=None):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        self.layer_ih = AdaptiveRankFactorizedLinear(input_size, hidden_size*4, bias, dropout, \n",
    "                                                     max_rank, tensor_type, prior_type, eta,\n",
    "                                                     device, dtype)\n",
    "        self.layer_hh = AdaptiveRankFactorizedLinear(hidden_size, hidden_size*4, bias, dropout,\n",
    "                                                     max_rank, tensor_type, prior_type, eta,\n",
    "                                                     device, dtype)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        output = []\n",
    "        batch_size = x.shape[0]\n",
    "        \n",
    "        c = torch.zeros((batch_size, self.hidden_size), device=x.device, dtype=x.dtype)\n",
    "        h = torch.zeros((batch_size, self.hidden_size), device=x.device, dtype=x.dtype)\n",
    "        for seq in range(20):\n",
    "            ih = self.layer_ih(x[:,seq,:])\n",
    "            hh = self.layer_hh(h)\n",
    "            i, f, g, o = torch.split(ih + hh, self.hidden_size, 1)\n",
    "            i = torch.sigmoid(i)\n",
    "            f = torch.sigmoid(f)\n",
    "            g = torch.tanh(g)\n",
    "            o = torch.sigmoid(o)\n",
    "            c = f * c + i * g\n",
    "            h = o * torch.tanh(c)\n",
    "            output.append(h.unsqueeze(1))\n",
    "            \n",
    "        output = torch.cat(output, dim=1)\n",
    "        \n",
    "        return output, (h, c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn = AdaptiveRankFactorizedLSTM(300, 128, eta=0.01, device=device, dtype=DTYPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.randn((4, 20, 300), device=device, dtype=DTYPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "output, (h_n, c_n) = rnn(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 20, 128])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 128])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h_n.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 128])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c_n.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9b70b155ba0c7c9d97f63e43fc99bb262c536cbfb221d6f6caf716892d6e29b8"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 ('tensor_layers')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
