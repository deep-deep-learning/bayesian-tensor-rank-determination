{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "5e213020-c6a8-4371-8254-e191084e5f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import pickle\n",
    "\n",
    "from tensor_layers import low_rank_tensors, TensorizedLinear\n",
    "\n",
    "from model import SubNet, TextSubNet\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcc6e532-2f87-4925-96d9-4e2bcc02fd70",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Layers and Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "188fa7ba-2d5d-4a38-8f7a-3bdaf9f25c95",
   "metadata": {
    "tags": []
   },
   "source": [
    "## CP Linear Layer with Accelerated Forward and Backward Propagation\n",
    "\n",
    "\n",
    "$y = \\left(\\prod_{m=1}^M x_m W_m\\right) \\otimes W_{M+1}$\\\n",
    "\\\n",
    "For $m=1,2,...,M$ where $M$ is the total number of input modalities,\\\n",
    "$x_m$ is an input from modality $m$ with shape (batch_size, input_size[$m$]),\\\n",
    "$W_m$ is a weight tensor factor for modality $m$ with shape (input_size[$m$], max_rank), and\\\n",
    "$W_{M+1}$ is a weight tensor factor for the output ($M+1^{th}$) mode with shape (output_size, max_rank)\\"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "938db601-9ef9-4cd0-acab-aa00f7e3c80b",
   "metadata": {},
   "source": [
    "### With Adaptive Rank Weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "138caa74-3cda-4ea5-80af-2015306adde2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Adaptive_Rank_CP_Linear(nn.Module):\n",
    "\n",
    "    def __init__(self, input_sizes, output_size, max_rank=20, em_stepsize=1.0, \n",
    "                 prior_type='log_uniform', eta=None):\n",
    "\n",
    "        super(Adaptive_Rank_CP_Linear, self).__init__()\n",
    "\n",
    "        self.input_sizes = input_sizes\n",
    "        self.output_size = output_size\n",
    "        \n",
    "        shape = input_sizes + (output_size,)\n",
    "        target_stddev = np.sqrt(2 / np.prod(self.input_sizes))\n",
    "        self.weight_tensor = getattr(low_rank_tensors, 'CP')(shape, prior_type=prior_type, em_stepsize=em_stepsize,\n",
    "                                                      max_rank=max_rank, initialization_method='nn', \n",
    "                                                      target_stddev=target_stddev, learned_scale=False, \n",
    "                                                      eta=eta)\n",
    "\n",
    "    def forward(self, inputs, rank_update=True):\n",
    "        \n",
    "        if self.training and rank_update:\n",
    "            self.weight_tensor.update_rank_parameters()\n",
    "        \n",
    "        y = torch.ones(size=(1,))\n",
    "        for i, x in enumerate(inputs):\n",
    "            y = y * (x @ self.weight_tensor.factors[i])\n",
    "        y = y @ self.weight_tensor.factors[-1].T\n",
    "\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e6d49b2-c53e-4562-8743-642a0dc879dd",
   "metadata": {},
   "source": [
    "### With Fixed Rank Weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3175c850-be95-4da9-8c4d-ee604c510a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Fixed_Rank_CP_Linear(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_sizes, output_size, rank=20):\n",
    "        \n",
    "        super(Fixed_Rank_CP_Linear, self).__init__()\n",
    "        \n",
    "        self.input_sizes = input_sizes\n",
    "        self.output_size = output_size\n",
    "        self.rank = rank\n",
    "        \n",
    "        self.weight_tensor_factors = self.initialize_weight_tensor_factors()\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        \n",
    "        # y = ((x_1 @ W_1)(x_2 @ W_2)...(x_M @ W_M)) @ W_y.T\n",
    "        y = torch.ones(size=(1,))\n",
    "        for i, x in enumerate(inputs):\n",
    "            y = y * (x @ self.weight_tensor_factors[i])\n",
    "        y = y @ self.weight_tensor_factors[-1].T\n",
    "\n",
    "        return y\n",
    "        \n",
    "    def initialize_weight_tensor_factors(self):\n",
    "        \n",
    "        factors = []\n",
    "        for m, input_size in enumerate(self.input_sizes):\n",
    "            factors.append(nn.Parameter(torch.empty(input_size, self.rank)))\n",
    "            nn.init.xavier_normal_(factors[m])\n",
    "        factors.append(nn.Parameter(torch.empty(self.output_size, self.rank)))\n",
    "        nn.init.xavier_normal_(factors[-1])\n",
    "            \n",
    "        return nn.ParameterList(factors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "043377b3-275f-413e-9128-7be0384ae17d",
   "metadata": {},
   "source": [
    "### Tensor Fusion Network with Accelerated CP Linear Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "fdf72173-1b62-4d0e-86cc-eaa052f89349",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CP_Tensor_Fusion_Network(nn.Module):\n",
    "    '''\n",
    "    Implements the Tensor Fusion Networks for multimodal sentiment analysis as is described in:\n",
    "    Zadeh, Amir, et al. \"Tensor fusion network for multimodal sentiment analysis.\" EMNLP 2017 Oral.\n",
    "    with rank-adaptive tensorized training from Hawkins, Cole and Zheng Zhang \"Bayesian tensorized\n",
    "    neural networks with automatic rank selection.\" Neurocomputing 2021.\n",
    "    '''\n",
    "\n",
    "    def __init__(self, input_sizes, hidden_sizes, output_size, max_rank, \n",
    "                 rank_adaptive=True):\n",
    "        '''\n",
    "        Args:\n",
    "            input_sizes - a length-3 tuple that contains (x_1_size, x_2_size, x_3_size)\n",
    "            hidden_sizes - a length-3 tuple that contains (hidden_size_1, hidden_size_2, hidden_size_3)\n",
    "            output_size - an integer specifying the size of the output\n",
    "            max_rank - an integer specifying the maximum rank of weight tensor\n",
    "        '''\n",
    "\n",
    "        super(CP_Tensor_Fusion_Network, self).__init__()\n",
    "\n",
    "        self.input_sizes = input_sizes\n",
    "        self.hidden_sizes = hidden_sizes\n",
    "        self.output_size = output_size\n",
    "        self.max_rank = max_rank\n",
    "        self.rank_adaptive = rank_adaptive\n",
    "        \n",
    "        self.audio_subnet = SubNet(input_sizes[0], hidden_sizes[0], dropout=0.3)\n",
    "        self.video_subnet = SubNet(input_sizes[1], hidden_sizes[1], dropout=0.3)\n",
    "        self.text_subnet = TextSubNet(input_sizes[2], hidden_sizes[2], hidden_sizes[2], dropout=0.3)\n",
    "        \n",
    "        tensor_input_sizes = (hidden_sizes[0] + 1, hidden_sizes[1] + 1, hidden_sizes[2] + 1)\n",
    "        if rank_adaptive:\n",
    "            self.tensor_fusion_layer = Adaptive_Rank_CP_Linear(tensor_input_sizes, output_size,\n",
    "                                                               max_rank=max_rank, em_stepsize=1.0,\n",
    "                                                               prior_type='log_uniform', eta=None)\n",
    "        else:\n",
    "            self.tensor_fusion_layer = Fixed_Rank_CP_Linear(tensor_input_sizes, output_size,\n",
    "                                                            rank=max_rank)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        \n",
    "        # subnet outputs\n",
    "        z_audio = self.audio_subnet(inputs[0])\n",
    "        z_video = self.video_subnet(inputs[1])\n",
    "        z_text = self.text_subnet(inputs[2])\n",
    "\n",
    "        batch_size = z_audio.data.shape[0]\n",
    "\n",
    "        if z_audio.is_cuda:\n",
    "            DTYPE = torch.cuda.FloatTensor\n",
    "        else:\n",
    "            DTYPE = torch.FloatTensor\n",
    "\n",
    "        # 1 in concatenated to each subnet outputs\n",
    "        z_audio = torch.cat((Variable(torch.ones(batch_size, 1).type(DTYPE), requires_grad=False), z_audio), dim=1)\n",
    "        z_video = torch.cat((Variable(torch.ones(batch_size, 1).type(DTYPE), requires_grad=False), z_video), dim=1)\n",
    "        z_text = torch.cat((Variable(torch.ones(batch_size, 1).type(DTYPE), requires_grad=False), z_text), dim=1)\n",
    "\n",
    "        output = self.tensor_fusion_layer([z_audio, z_video, z_text])\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "116b003c-b5eb-4330-9838-7f4dbc95e879",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## CP Linear Layer with Naive Forward and Backward Propagation\n",
    "\n",
    "$ y = \\mathcal X \\mathcal W $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "cee1c009-fa7f-4b2e-9335-d817b6508219",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TFN(nn.Module):\n",
    "\n",
    "    def __init__(self, input_sizes, hidden_sizes, output_size, rank, rank_adaptive):\n",
    "        \n",
    "        super(TFN, self).__init__()\n",
    "\n",
    "        # dimensions are specified in the order of audio, video and text\n",
    "        self.input_sizes = input_sizes\n",
    "        self.hidden_sizes = hidden_sizes\n",
    "        self.output_size = output_size\n",
    "        self.rank = rank\n",
    "\n",
    "        # define the pre-fusion subnetworks\n",
    "        self.audio_subnet = SubNet(input_sizes[0], hidden_sizes[0], dropout=0.3)\n",
    "        self.video_subnet = SubNet(input_sizes[1], hidden_sizes[1], dropout=0.3)\n",
    "        self.text_subnet = TextSubNet(input_sizes[2], hidden_sizes[2], hidden_sizes[2], dropout=0.3)\n",
    "        \n",
    "        tensor_input_sizes = (hidden_sizes[0] + 1, hidden_sizes[1] + 1, hidden_sizes[2] + 1)\n",
    "        if rank_adaptive:\n",
    "            shape = tensor_input_sizes + (output_size,)\n",
    "            self.post_fusion_layer = TensorizedLinear(in_features=np.prod(tensor_input_sizes), \n",
    "                                                      out_features=output_size, \n",
    "                                                      shape=shape, \n",
    "                                                      tensor_type='CP', max_rank=rank)\n",
    "        else:\n",
    "            self.post_fusion_layer = nn.Linear(np.prod(tensor_input_sizes), output_size)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        '''\n",
    "        Args:\n",
    "            audio_x: tensor of shape (batch_size, audio_in)\n",
    "            video_x: tensor of shape (batch_size, video_in)\n",
    "            text_x: tensor of shape (batch_size, sequence_len, text_in)\n",
    "        '''\n",
    "        audio_h = self.audio_subnet(inputs[0])\n",
    "        video_h = self.video_subnet(inputs[1])\n",
    "        text_h = self.text_subnet(inputs[2])\n",
    "        batch_size = audio_h.data.shape[0]\n",
    "\n",
    "        # next we perform \"tensor fusion\", which is essentially appending 1s to the tensors and take Kronecker product\n",
    "        if audio_h.is_cuda:\n",
    "            DTYPE = torch.cuda.FloatTensor\n",
    "        else:\n",
    "            DTYPE = torch.FloatTensor\n",
    "        \n",
    "        # append 1s\n",
    "        _audio_h = torch.cat((Variable(torch.ones(batch_size, 1).type(DTYPE),\n",
    "                                       requires_grad=False), audio_h), dim=1)\n",
    "        _video_h = torch.cat((Variable(torch.ones(batch_size, 1).type(DTYPE),\n",
    "                                       requires_grad=False), video_h), dim=1)\n",
    "        _text_h = torch.cat((Variable(torch.ones(batch_size, 1).type(DTYPE),\n",
    "                                      requires_grad=False), text_h), dim=1)\n",
    "        \n",
    "        \n",
    "        \n",
    "        # fusion_tensor will have shape (batch_size, (audio_hidden + 1) * (video_hidden + 1) * (text_out + 1))\n",
    "        fusion_tensor = torch.bmm(_audio_h.unsqueeze(2), _video_h.unsqueeze(1))\n",
    "        fusion_tensor = fusion_tensor.view(-1, (self.hidden_sizes[0] + 1) * (self.hidden_sizes[1] + 1), 1)\n",
    "        fusion_tensor = torch.bmm(fusion_tensor, _text_h.unsqueeze(1)).view(batch_size, -1)\n",
    "                                  \n",
    "        output = self.post_fusion_layer(fusion_tensor)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c47104d-f828-43b5-8fdc-09028be08062",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1d8d208-744e-4823-9fd0-6043fd4db9ef",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## CMU-MOSI Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e8d7d660-3aaf-4dc8-a7fb-8068f1ff9d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultimodalDataset(Dataset):\n",
    "    '''\n",
    "    Dataset for CMU-MOSI\n",
    "    '''\n",
    "    def __init__(self, text, audio, vision, labels):\n",
    "        '''\n",
    "        args:\n",
    "            text: text modality feature of shape (N, seq. length, text_input_size)\n",
    "            audio: audio modality feature of shape (N, seq. length, audio_input_size)\n",
    "            vision: vision modality feature of shape (N, seq. length, vision_input_size)\n",
    "            labels: labels of shape (N, 1) and ranges from -3 to 3\n",
    "        '''\n",
    "        self.text = text\n",
    "        self.audio = audio\n",
    "        self.vision = vision\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        '''\n",
    "        Returns an individual data composed of (features, label)\n",
    "        where features is a dictionary {'text': , 'audio':, 'vision':}\n",
    "        Returns:\n",
    "            features['text']: text modality feature of shape (seq. length, text_input_size)\n",
    "            features['audio']: audio modality feature of shape (audio_input_size)\n",
    "            features['vision']: vision modality feature of shape (vision_input_size)\n",
    "            label: a scalar label that ranges from -3 to 3\n",
    "        '''\n",
    "        features = dict()\n",
    "        features['text'] = self.text[idx]\n",
    "        # audio and vision features are averaged across time\n",
    "        features['audio'] = np.mean(self.audio[idx], axis=0)\n",
    "        features['vision'] = np.mean(self.vision[idx], axis=0)\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        return features, label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3a5c3d3-0502-463f-bd9a-b83aaa564312",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## KL Loss for Rank Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "14886dba-5314-4610-a863-b30497366244",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_kl_loss(model, kl_multiplier, no_kl_epochs, warmup_epochs, epoch):\n",
    "    kl_loss = 0.0\n",
    "    for layer in model.modules():\n",
    "        if hasattr(layer, \"tensor\"):\n",
    "            kl_loss += layer.tensor.get_kl_divergence_to_prior()\n",
    "        elif hasattr(layer, \"weight_tensor\"):\n",
    "            kl_loss += layer.weight_tensor.get_kl_divergence_to_prior()\n",
    "            \n",
    "    kl_mult = kl_multiplier * torch.clamp(torch.tensor(((epoch - no_kl_epochs) / \n",
    "                                                        warmup_epochs)), 0.0, 1.0)\n",
    "    \n",
    "    return kl_loss*kl_mult"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54e6bbe5-1265-4173-a62c-83a21db62994",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "cfc00954-c881-4528-819d-cf5fa1338325",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_CMU_mosi(batch_size=32, epochs=100, lr=.001, max_rank=20, rank_adaptive=True,  \n",
    "                   warmup_epochs=50, kl_multiplier=1e-4, no_kl_epochs=5, accelerated=True):\n",
    "\n",
    "    # load dataset file\n",
    "    file = open('../../dataset/CMU-MOSI/mosi_20_seq_data.pkl', 'rb')\n",
    "    data = pickle.load(file)\n",
    "    file.close()\n",
    "\n",
    "    # prepare the datasets and data loaders\n",
    "    train_set = MultimodalDataset(data['train']['text'], data['train']['audio'],\n",
    "                                  data['train']['vision'], data['train']['labels'])\n",
    "    valid_set = MultimodalDataset(data['valid']['text'], data['valid']['audio'],\n",
    "                                  data['valid']['vision'], data['valid']['labels'])\n",
    "\n",
    "    train_dataloader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "    valid_dataloader = DataLoader(valid_set, batch_size=len(valid_set))\n",
    "\n",
    "    # set up model\n",
    "    input_sizes = (train_set[0][0]['audio'].shape[0], train_set[0][0]['vision'].shape[0],\n",
    "                   train_set[0][0]['text'].shape[1])\n",
    "    hidden_sizes = (32, 32, 128)\n",
    "    output_size = 1\n",
    "    \n",
    "    if accelerated:\n",
    "        model = CP_Tensor_Fusion_Network(input_sizes, hidden_sizes, output_size, max_rank,\n",
    "                                         rank_adaptive)\n",
    "    else:\n",
    "        model = TFN(input_sizes, hidden_sizes, output_size, max_rank, rank_adaptive)\n",
    "\n",
    "    # set up training\n",
    "    DTYPE = torch.FloatTensor\n",
    "    optimizer = optim.Adam(list(model.parameters()), lr=lr)\n",
    "    criterion = nn.MSELoss(size_average=False)\n",
    "    \n",
    "    train_losses = []\n",
    "    train_times = []\n",
    "    valid_errors = []\n",
    "    \n",
    "    # train and validate\n",
    "    for e in range(1, epochs + 1):\n",
    "        # train\n",
    "        tic = time.time()\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        for batch in train_dataloader:\n",
    "            model.zero_grad()\n",
    "\n",
    "            features, label = batch\n",
    "            x_a = Variable(features['audio'].float().type(DTYPE), requires_grad=False)\n",
    "            x_v = Variable(features['vision'].float().type(DTYPE), requires_grad=False)\n",
    "            x_t = Variable(features['text'].float().type(DTYPE), requires_grad=False)\n",
    "            y = Variable(label.view(-1, 1).float().type(DTYPE), requires_grad=False)\n",
    "\n",
    "            output = model([x_a, x_v, x_t])\n",
    "\n",
    "            loss = criterion(output, y)\n",
    "\n",
    "            # rank loss for adaptive-rank model\n",
    "            if rank_adaptive:\n",
    "                loss += get_kl_loss(model, kl_multiplier, no_kl_epochs, warmup_epochs, e)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item() / len(train_set)\n",
    "        toc = time.time()\n",
    "        train_time = toc - tic\n",
    "        print(\"Epoch {}\".format(e))\n",
    "        print(\"Training Loss {:.3f}\".format(train_loss))\n",
    "        print(\"Training TIme {:.3f}\".format(train_time))\n",
    "        train_losses.append(train_loss)\n",
    "        train_times.append(train_time)\n",
    "        \n",
    "        # validate\n",
    "        model.eval()\n",
    "        for batch in valid_dataloader:\n",
    "            features, label = batch\n",
    "            x_a = Variable(features['audio'].float().type(DTYPE), requires_grad=False)\n",
    "            x_v = Variable(features['vision'].float().type(DTYPE), requires_grad=False)\n",
    "            x_t = Variable(features['text'].float().type(DTYPE), requires_grad=False)\n",
    "            y = Variable(label.view(-1, 1).float().type(DTYPE), requires_grad=False)\n",
    "\n",
    "            output = model([x_a, x_v, x_t])\n",
    "            \n",
    "        valid_mse = nn.functional.mse_loss(y, output).item()\n",
    "        print(\"Valid MSE {:.3f}\".format(valid_mse))\n",
    "        valid_errors.append(valid_mse)\n",
    "    \n",
    "    np.savetxt('Ada-{}-Acc-{}-train-losses.csv'.format(rank_adaptive, accelerated), train_losses, delimiter=',')\n",
    "    np.savetxt('Ada-{}-Acc-{}-train-times.csv'.format(rank_adaptive, accelerated), train_times, delimiter=',')\n",
    "    np.savetxt('Ada-{}-Acc-{}-valid-errors.csv'.format(rank_adaptive, accelerated), valid_errors, delimiter=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d49d03ca-a93e-4c71-bc97-0b1a605fbc52",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2b01bb7-78c4-4d15-b206-f146de80b95b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Experiment with Naive, Adaptive Rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "9dc1552b-948b-491d-9fc5-40965ca0dd48",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "Training Loss 2.203\n",
      "Training TIme 0.763\n",
      "Valid MSE 2.256\n",
      "Epoch 2\n",
      "Training Loss 1.571\n",
      "Training TIme 0.747\n",
      "Valid MSE 1.736\n",
      "Epoch 3\n",
      "Training Loss 1.369\n",
      "Training TIme 0.744\n",
      "Valid MSE 1.646\n",
      "Epoch 4\n",
      "Training Loss 1.176\n",
      "Training TIme 0.748\n",
      "Valid MSE 1.530\n",
      "Epoch 5\n",
      "Training Loss 1.018\n",
      "Training TIme 0.735\n",
      "Valid MSE 1.472\n",
      "Epoch 6\n",
      "Training Loss 0.858\n",
      "Training TIme 0.735\n",
      "Valid MSE 1.560\n",
      "Epoch 7\n",
      "Training Loss 0.788\n",
      "Training TIme 0.740\n",
      "Valid MSE 1.730\n",
      "Epoch 8\n",
      "Training Loss 0.841\n",
      "Training TIme 0.738\n",
      "Valid MSE 1.489\n",
      "Epoch 9\n",
      "Training Loss 0.659\n",
      "Training TIme 0.736\n",
      "Valid MSE 1.550\n",
      "Epoch 10\n",
      "Training Loss 0.574\n",
      "Training TIme 0.743\n",
      "Valid MSE 1.568\n",
      "Epoch 11\n",
      "Training Loss 0.478\n",
      "Training TIme 0.738\n",
      "Valid MSE 1.657\n",
      "Epoch 12\n",
      "Training Loss 0.442\n",
      "Training TIme 0.737\n",
      "Valid MSE 1.679\n",
      "Epoch 13\n",
      "Training Loss 0.386\n",
      "Training TIme 0.733\n",
      "Valid MSE 1.653\n",
      "Epoch 14\n",
      "Training Loss 0.343\n",
      "Training TIme 0.753\n",
      "Valid MSE 1.614\n",
      "Epoch 15\n",
      "Training Loss 0.297\n",
      "Training TIme 0.736\n",
      "Valid MSE 1.706\n",
      "Epoch 16\n",
      "Training Loss 0.306\n",
      "Training TIme 0.737\n",
      "Valid MSE 1.678\n",
      "Epoch 17\n",
      "Training Loss 0.274\n",
      "Training TIme 0.735\n",
      "Valid MSE 1.615\n",
      "Epoch 18\n",
      "Training Loss 0.253\n",
      "Training TIme 0.736\n",
      "Valid MSE 1.669\n",
      "Epoch 19\n",
      "Training Loss 0.241\n",
      "Training TIme 0.735\n",
      "Valid MSE 1.611\n",
      "Epoch 20\n",
      "Training Loss 0.212\n",
      "Training TIme 0.739\n",
      "Valid MSE 1.605\n",
      "Epoch 21\n",
      "Training Loss 0.205\n",
      "Training TIme 0.736\n",
      "Valid MSE 1.605\n",
      "Epoch 22\n",
      "Training Loss 0.197\n",
      "Training TIme 0.746\n",
      "Valid MSE 1.589\n",
      "Epoch 23\n",
      "Training Loss 0.188\n",
      "Training TIme 0.749\n",
      "Valid MSE 1.555\n",
      "Epoch 24\n",
      "Training Loss 0.192\n",
      "Training TIme 0.743\n",
      "Valid MSE 1.549\n",
      "Epoch 25\n",
      "Training Loss 0.202\n",
      "Training TIme 0.740\n",
      "Valid MSE 1.560\n",
      "Epoch 26\n",
      "Training Loss 0.195\n",
      "Training TIme 0.740\n",
      "Valid MSE 1.564\n",
      "Epoch 27\n",
      "Training Loss 0.190\n",
      "Training TIme 0.746\n",
      "Valid MSE 1.552\n",
      "Epoch 28\n",
      "Training Loss 0.194\n",
      "Training TIme 0.735\n",
      "Valid MSE 1.591\n",
      "Epoch 29\n",
      "Training Loss 0.207\n",
      "Training TIme 0.737\n",
      "Valid MSE 1.557\n",
      "Epoch 30\n",
      "Training Loss 0.206\n",
      "Training TIme 0.742\n",
      "Valid MSE 1.550\n",
      "Epoch 31\n",
      "Training Loss 0.205\n",
      "Training TIme 0.735\n",
      "Valid MSE 1.514\n",
      "Epoch 32\n",
      "Training Loss 0.202\n",
      "Training TIme 0.737\n",
      "Valid MSE 1.608\n",
      "Epoch 33\n",
      "Training Loss 0.212\n",
      "Training TIme 0.736\n",
      "Valid MSE 1.574\n",
      "Epoch 34\n",
      "Training Loss 0.208\n",
      "Training TIme 0.735\n",
      "Valid MSE 1.537\n",
      "Epoch 35\n",
      "Training Loss 0.199\n",
      "Training TIme 0.731\n",
      "Valid MSE 1.525\n",
      "Epoch 36\n",
      "Training Loss 0.210\n",
      "Training TIme 0.733\n",
      "Valid MSE 1.532\n",
      "Epoch 37\n",
      "Training Loss 0.207\n",
      "Training TIme 0.741\n",
      "Valid MSE 1.582\n",
      "Epoch 38\n",
      "Training Loss 0.212\n",
      "Training TIme 0.739\n",
      "Valid MSE 1.521\n",
      "Epoch 39\n",
      "Training Loss 0.220\n",
      "Training TIme 0.738\n",
      "Valid MSE 1.602\n",
      "Epoch 40\n",
      "Training Loss 0.232\n",
      "Training TIme 0.743\n",
      "Valid MSE 1.607\n",
      "Epoch 41\n",
      "Training Loss 0.229\n",
      "Training TIme 0.739\n",
      "Valid MSE 1.551\n",
      "Epoch 42\n",
      "Training Loss 0.231\n",
      "Training TIme 0.738\n",
      "Valid MSE 1.587\n",
      "Epoch 43\n",
      "Training Loss 0.220\n",
      "Training TIme 0.747\n",
      "Valid MSE 1.518\n",
      "Epoch 44\n",
      "Training Loss 0.222\n",
      "Training TIme 0.736\n",
      "Valid MSE 1.537\n",
      "Epoch 45\n",
      "Training Loss 0.228\n",
      "Training TIme 0.738\n",
      "Valid MSE 1.570\n",
      "Epoch 46\n",
      "Training Loss 0.235\n",
      "Training TIme 0.737\n",
      "Valid MSE 1.537\n",
      "Epoch 47\n",
      "Training Loss 0.245\n",
      "Training TIme 0.738\n",
      "Valid MSE 1.589\n",
      "Epoch 48\n",
      "Training Loss 0.241\n",
      "Training TIme 0.740\n",
      "Valid MSE 1.506\n",
      "Epoch 49\n",
      "Training Loss 0.247\n",
      "Training TIme 0.740\n",
      "Valid MSE 1.581\n",
      "Epoch 50\n",
      "Training Loss 0.251\n",
      "Training TIme 0.741\n",
      "Valid MSE 1.568\n",
      "Epoch 51\n",
      "Training Loss 0.255\n",
      "Training TIme 0.745\n",
      "Valid MSE 1.524\n",
      "Epoch 52\n",
      "Training Loss 0.255\n",
      "Training TIme 0.753\n",
      "Valid MSE 1.582\n",
      "Epoch 53\n",
      "Training Loss 0.260\n",
      "Training TIme 0.762\n",
      "Valid MSE 1.502\n",
      "Epoch 54\n",
      "Training Loss 0.258\n",
      "Training TIme 0.746\n",
      "Valid MSE 1.510\n",
      "Epoch 55\n",
      "Training Loss 0.260\n",
      "Training TIme 0.743\n",
      "Valid MSE 1.576\n",
      "Epoch 56\n",
      "Training Loss 0.266\n",
      "Training TIme 0.743\n",
      "Valid MSE 1.530\n",
      "Epoch 57\n",
      "Training Loss 0.261\n",
      "Training TIme 0.745\n",
      "Valid MSE 1.549\n",
      "Epoch 58\n",
      "Training Loss 0.263\n",
      "Training TIme 0.744\n",
      "Valid MSE 1.564\n",
      "Epoch 59\n",
      "Training Loss 0.263\n",
      "Training TIme 0.759\n",
      "Valid MSE 1.507\n",
      "Epoch 60\n",
      "Training Loss 0.261\n",
      "Training TIme 0.744\n",
      "Valid MSE 1.560\n",
      "Epoch 61\n",
      "Training Loss 0.263\n",
      "Training TIme 0.753\n",
      "Valid MSE 1.529\n",
      "Epoch 62\n",
      "Training Loss 0.262\n",
      "Training TIme 0.752\n",
      "Valid MSE 1.576\n",
      "Epoch 63\n",
      "Training Loss 0.263\n",
      "Training TIme 0.754\n",
      "Valid MSE 1.498\n",
      "Epoch 64\n",
      "Training Loss 0.260\n",
      "Training TIme 0.772\n",
      "Valid MSE 1.508\n",
      "Epoch 65\n",
      "Training Loss 0.258\n",
      "Training TIme 0.759\n",
      "Valid MSE 1.549\n",
      "Epoch 66\n",
      "Training Loss 0.253\n",
      "Training TIme 0.748\n",
      "Valid MSE 1.505\n",
      "Epoch 67\n",
      "Training Loss 0.254\n",
      "Training TIme 0.749\n",
      "Valid MSE 1.534\n",
      "Epoch 68\n",
      "Training Loss 0.253\n",
      "Training TIme 0.749\n",
      "Valid MSE 1.531\n",
      "Epoch 69\n",
      "Training Loss 0.252\n",
      "Training TIme 0.763\n",
      "Valid MSE 1.552\n",
      "Epoch 70\n",
      "Training Loss 0.252\n",
      "Training TIme 0.750\n",
      "Valid MSE 1.540\n",
      "Epoch 71\n",
      "Training Loss 0.252\n",
      "Training TIme 0.753\n",
      "Valid MSE 1.502\n",
      "Epoch 72\n",
      "Training Loss 0.250\n",
      "Training TIme 0.748\n",
      "Valid MSE 1.544\n",
      "Epoch 73\n",
      "Training Loss 0.250\n",
      "Training TIme 0.748\n",
      "Valid MSE 1.477\n",
      "Epoch 74\n",
      "Training Loss 0.243\n",
      "Training TIme 0.762\n",
      "Valid MSE 1.492\n",
      "Epoch 75\n",
      "Training Loss 0.244\n",
      "Training TIme 0.749\n",
      "Valid MSE 1.509\n",
      "Epoch 76\n",
      "Training Loss 0.242\n",
      "Training TIme 0.750\n",
      "Valid MSE 1.517\n",
      "Epoch 77\n",
      "Training Loss 0.248\n",
      "Training TIme 0.752\n",
      "Valid MSE 1.522\n",
      "Epoch 78\n",
      "Training Loss 0.241\n",
      "Training TIme 0.758\n",
      "Valid MSE 1.511\n",
      "Epoch 79\n",
      "Training Loss 0.241\n",
      "Training TIme 0.751\n",
      "Valid MSE 1.531\n",
      "Epoch 80\n",
      "Training Loss 0.241\n",
      "Training TIme 0.774\n",
      "Valid MSE 1.505\n",
      "Epoch 81\n",
      "Training Loss 0.239\n",
      "Training TIme 0.754\n",
      "Valid MSE 1.517\n",
      "Epoch 82\n",
      "Training Loss 0.237\n",
      "Training TIme 0.747\n",
      "Valid MSE 1.504\n",
      "Epoch 83\n",
      "Training Loss 0.234\n",
      "Training TIme 0.749\n",
      "Valid MSE 1.512\n",
      "Epoch 84\n",
      "Training Loss 0.237\n",
      "Training TIme 0.749\n",
      "Valid MSE 1.504\n",
      "Epoch 85\n",
      "Training Loss 0.238\n",
      "Training TIme 0.763\n",
      "Valid MSE 1.481\n",
      "Epoch 86\n",
      "Training Loss 0.233\n",
      "Training TIme 0.752\n",
      "Valid MSE 1.496\n",
      "Epoch 87\n",
      "Training Loss 0.235\n",
      "Training TIme 0.745\n",
      "Valid MSE 1.513\n",
      "Epoch 88\n",
      "Training Loss 0.231\n",
      "Training TIme 0.751\n",
      "Valid MSE 1.468\n",
      "Epoch 89\n",
      "Training Loss 0.229\n",
      "Training TIme 0.752\n",
      "Valid MSE 1.525\n",
      "Epoch 90\n",
      "Training Loss 0.232\n",
      "Training TIme 0.780\n",
      "Valid MSE 1.448\n",
      "Epoch 91\n",
      "Training Loss 0.238\n",
      "Training TIme 0.743\n",
      "Valid MSE 1.479\n",
      "Epoch 92\n",
      "Training Loss 0.234\n",
      "Training TIme 0.750\n",
      "Valid MSE 1.512\n",
      "Epoch 93\n",
      "Training Loss 0.233\n",
      "Training TIme 0.755\n",
      "Valid MSE 1.480\n",
      "Epoch 94\n",
      "Training Loss 0.230\n",
      "Training TIme 0.751\n",
      "Valid MSE 1.500\n",
      "Epoch 95\n",
      "Training Loss 0.232\n",
      "Training TIme 0.751\n",
      "Valid MSE 1.498\n",
      "Epoch 96\n",
      "Training Loss 0.230\n",
      "Training TIme 0.752\n",
      "Valid MSE 1.494\n",
      "Epoch 97\n",
      "Training Loss 0.229\n",
      "Training TIme 0.751\n",
      "Valid MSE 1.453\n",
      "Epoch 98\n",
      "Training Loss 0.225\n",
      "Training TIme 0.761\n",
      "Valid MSE 1.493\n",
      "Epoch 99\n",
      "Training Loss 0.228\n",
      "Training TIme 0.754\n",
      "Valid MSE 1.517\n",
      "Epoch 100\n",
      "Training Loss 0.234\n",
      "Training TIme 0.749\n",
      "Valid MSE 1.507\n"
     ]
    }
   ],
   "source": [
    "train_CMU_mosi(batch_size=32, epochs=100, lr=.001, max_rank=20, rank_adaptive=True,  \n",
    "                   warmup_epochs=50, kl_multiplier=1e-4, no_kl_epochs=5, accelerated=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff0ba1c3-fc1e-47de-8e70-8806e0b6700b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Experiment with Accelerated, Adaptive Rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "176021d0-2b99-4c1e-9763-ad1feffb6ea7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "Training Loss 2.200\n",
      "Training TIme 0.452\n",
      "Valid MSE 2.279\n",
      "Epoch 2\n",
      "Training Loss 1.615\n",
      "Training TIme 0.408\n",
      "Valid MSE 1.888\n",
      "Epoch 3\n",
      "Training Loss 1.383\n",
      "Training TIme 0.401\n",
      "Valid MSE 1.852\n",
      "Epoch 4\n",
      "Training Loss 1.189\n",
      "Training TIme 0.403\n",
      "Valid MSE 1.563\n",
      "Epoch 5\n",
      "Training Loss 1.012\n",
      "Training TIme 0.406\n",
      "Valid MSE 1.459\n",
      "Epoch 6\n",
      "Training Loss 0.864\n",
      "Training TIme 0.405\n",
      "Valid MSE 1.488\n",
      "Epoch 7\n",
      "Training Loss 0.800\n",
      "Training TIme 0.407\n",
      "Valid MSE 1.494\n",
      "Epoch 8\n",
      "Training Loss 0.677\n",
      "Training TIme 0.406\n",
      "Valid MSE 1.544\n",
      "Epoch 9\n",
      "Training Loss 0.561\n",
      "Training TIme 0.409\n",
      "Valid MSE 1.589\n",
      "Epoch 10\n",
      "Training Loss 0.497\n",
      "Training TIme 0.399\n",
      "Valid MSE 1.618\n",
      "Epoch 11\n",
      "Training Loss 0.437\n",
      "Training TIme 0.403\n",
      "Valid MSE 1.668\n",
      "Epoch 12\n",
      "Training Loss 0.365\n",
      "Training TIme 0.408\n",
      "Valid MSE 1.580\n",
      "Epoch 13\n",
      "Training Loss 0.285\n",
      "Training TIme 0.398\n",
      "Valid MSE 1.663\n",
      "Epoch 14\n",
      "Training Loss 0.310\n",
      "Training TIme 0.412\n",
      "Valid MSE 1.614\n",
      "Epoch 15\n",
      "Training Loss 0.283\n",
      "Training TIme 0.408\n",
      "Valid MSE 1.563\n",
      "Epoch 16\n",
      "Training Loss 0.245\n",
      "Training TIme 0.399\n",
      "Valid MSE 1.621\n",
      "Epoch 17\n",
      "Training Loss 0.230\n",
      "Training TIme 0.403\n",
      "Valid MSE 1.719\n",
      "Epoch 18\n",
      "Training Loss 0.230\n",
      "Training TIme 0.397\n",
      "Valid MSE 1.575\n",
      "Epoch 19\n",
      "Training Loss 0.222\n",
      "Training TIme 0.408\n",
      "Valid MSE 1.622\n",
      "Epoch 20\n",
      "Training Loss 0.211\n",
      "Training TIme 0.401\n",
      "Valid MSE 1.603\n",
      "Epoch 21\n",
      "Training Loss 0.184\n",
      "Training TIme 0.402\n",
      "Valid MSE 1.645\n",
      "Epoch 22\n",
      "Training Loss 0.194\n",
      "Training TIme 0.405\n",
      "Valid MSE 1.549\n",
      "Epoch 23\n",
      "Training Loss 0.186\n",
      "Training TIme 0.402\n",
      "Valid MSE 1.646\n",
      "Epoch 24\n",
      "Training Loss 0.180\n",
      "Training TIme 0.403\n",
      "Valid MSE 1.622\n",
      "Epoch 25\n",
      "Training Loss 0.188\n",
      "Training TIme 0.406\n",
      "Valid MSE 1.552\n",
      "Epoch 26\n",
      "Training Loss 0.196\n",
      "Training TIme 0.406\n",
      "Valid MSE 1.688\n",
      "Epoch 27\n",
      "Training Loss 0.191\n",
      "Training TIme 0.403\n",
      "Valid MSE 1.568\n",
      "Epoch 28\n",
      "Training Loss 0.191\n",
      "Training TIme 0.404\n",
      "Valid MSE 1.599\n",
      "Epoch 29\n",
      "Training Loss 0.187\n",
      "Training TIme 0.405\n",
      "Valid MSE 1.551\n",
      "Epoch 30\n",
      "Training Loss 0.184\n",
      "Training TIme 0.405\n",
      "Valid MSE 1.580\n",
      "Epoch 31\n",
      "Training Loss 0.186\n",
      "Training TIme 0.398\n",
      "Valid MSE 1.525\n",
      "Epoch 32\n",
      "Training Loss 0.188\n",
      "Training TIme 0.403\n",
      "Valid MSE 1.552\n",
      "Epoch 33\n",
      "Training Loss 0.193\n",
      "Training TIme 0.403\n",
      "Valid MSE 1.562\n",
      "Epoch 34\n",
      "Training Loss 0.194\n",
      "Training TIme 0.398\n",
      "Valid MSE 1.552\n",
      "Epoch 35\n",
      "Training Loss 0.201\n",
      "Training TIme 0.401\n",
      "Valid MSE 1.573\n",
      "Epoch 36\n",
      "Training Loss 0.204\n",
      "Training TIme 0.403\n",
      "Valid MSE 1.545\n",
      "Epoch 37\n",
      "Training Loss 0.210\n",
      "Training TIme 0.405\n",
      "Valid MSE 1.554\n",
      "Epoch 38\n",
      "Training Loss 0.210\n",
      "Training TIme 0.409\n",
      "Valid MSE 1.573\n",
      "Epoch 39\n",
      "Training Loss 0.212\n",
      "Training TIme 0.404\n",
      "Valid MSE 1.536\n",
      "Epoch 40\n",
      "Training Loss 0.207\n",
      "Training TIme 0.405\n",
      "Valid MSE 1.564\n",
      "Epoch 41\n",
      "Training Loss 0.216\n",
      "Training TIme 0.402\n",
      "Valid MSE 1.601\n",
      "Epoch 42\n",
      "Training Loss 0.225\n",
      "Training TIme 0.405\n",
      "Valid MSE 1.558\n",
      "Epoch 43\n",
      "Training Loss 0.217\n",
      "Training TIme 0.406\n",
      "Valid MSE 1.530\n",
      "Epoch 44\n",
      "Training Loss 0.222\n",
      "Training TIme 0.406\n",
      "Valid MSE 1.577\n",
      "Epoch 45\n",
      "Training Loss 0.221\n",
      "Training TIme 0.402\n",
      "Valid MSE 1.566\n",
      "Epoch 46\n",
      "Training Loss 0.227\n",
      "Training TIme 0.411\n",
      "Valid MSE 1.526\n",
      "Epoch 47\n",
      "Training Loss 0.229\n",
      "Training TIme 0.406\n",
      "Valid MSE 1.505\n",
      "Epoch 48\n",
      "Training Loss 0.231\n",
      "Training TIme 0.410\n",
      "Valid MSE 1.559\n",
      "Epoch 49\n",
      "Training Loss 0.233\n",
      "Training TIme 0.414\n",
      "Valid MSE 1.539\n",
      "Epoch 50\n",
      "Training Loss 0.242\n",
      "Training TIme 0.402\n",
      "Valid MSE 1.564\n",
      "Epoch 51\n",
      "Training Loss 0.241\n",
      "Training TIme 0.407\n",
      "Valid MSE 1.539\n",
      "Epoch 52\n",
      "Training Loss 0.247\n",
      "Training TIme 0.410\n",
      "Valid MSE 1.511\n",
      "Epoch 53\n",
      "Training Loss 0.247\n",
      "Training TIme 0.407\n",
      "Valid MSE 1.540\n",
      "Epoch 54\n",
      "Training Loss 0.253\n",
      "Training TIme 0.409\n",
      "Valid MSE 1.525\n",
      "Epoch 55\n",
      "Training Loss 0.257\n",
      "Training TIme 0.405\n",
      "Valid MSE 1.540\n",
      "Epoch 56\n",
      "Training Loss 0.253\n",
      "Training TIme 0.401\n",
      "Valid MSE 1.529\n",
      "Epoch 57\n",
      "Training Loss 0.253\n",
      "Training TIme 0.402\n",
      "Valid MSE 1.521\n",
      "Epoch 58\n",
      "Training Loss 0.256\n",
      "Training TIme 0.412\n",
      "Valid MSE 1.519\n",
      "Epoch 59\n",
      "Training Loss 0.255\n",
      "Training TIme 0.401\n",
      "Valid MSE 1.529\n",
      "Epoch 60\n",
      "Training Loss 0.245\n",
      "Training TIme 0.404\n",
      "Valid MSE 1.549\n",
      "Epoch 61\n",
      "Training Loss 0.248\n",
      "Training TIme 0.403\n",
      "Valid MSE 1.519\n",
      "Epoch 62\n",
      "Training Loss 0.245\n",
      "Training TIme 0.407\n",
      "Valid MSE 1.494\n",
      "Epoch 63\n",
      "Training Loss 0.245\n",
      "Training TIme 0.402\n",
      "Valid MSE 1.553\n",
      "Epoch 64\n",
      "Training Loss 0.243\n",
      "Training TIme 0.397\n",
      "Valid MSE 1.537\n",
      "Epoch 65\n",
      "Training Loss 0.244\n",
      "Training TIme 0.406\n",
      "Valid MSE 1.513\n",
      "Epoch 66\n",
      "Training Loss 0.238\n",
      "Training TIme 0.406\n",
      "Valid MSE 1.509\n",
      "Epoch 67\n",
      "Training Loss 0.238\n",
      "Training TIme 0.401\n",
      "Valid MSE 1.536\n",
      "Epoch 68\n",
      "Training Loss 0.242\n",
      "Training TIme 0.408\n",
      "Valid MSE 1.530\n",
      "Epoch 69\n",
      "Training Loss 0.242\n",
      "Training TIme 0.413\n",
      "Valid MSE 1.536\n",
      "Epoch 70\n",
      "Training Loss 0.233\n",
      "Training TIme 0.409\n",
      "Valid MSE 1.496\n",
      "Epoch 71\n",
      "Training Loss 0.232\n",
      "Training TIme 0.412\n",
      "Valid MSE 1.526\n",
      "Epoch 72\n",
      "Training Loss 0.231\n",
      "Training TIme 0.408\n",
      "Valid MSE 1.486\n",
      "Epoch 73\n",
      "Training Loss 0.227\n",
      "Training TIme 0.411\n",
      "Valid MSE 1.487\n",
      "Epoch 74\n",
      "Training Loss 0.223\n",
      "Training TIme 0.411\n",
      "Valid MSE 1.471\n",
      "Epoch 75\n",
      "Training Loss 0.221\n",
      "Training TIme 0.406\n",
      "Valid MSE 1.494\n",
      "Epoch 76\n",
      "Training Loss 0.227\n",
      "Training TIme 0.405\n",
      "Valid MSE 1.548\n",
      "Epoch 77\n",
      "Training Loss 0.223\n",
      "Training TIme 0.404\n",
      "Valid MSE 1.491\n",
      "Epoch 78\n",
      "Training Loss 0.226\n",
      "Training TIme 0.413\n",
      "Valid MSE 1.459\n",
      "Epoch 79\n",
      "Training Loss 0.221\n",
      "Training TIme 0.408\n",
      "Valid MSE 1.478\n",
      "Epoch 80\n",
      "Training Loss 0.221\n",
      "Training TIme 0.407\n",
      "Valid MSE 1.502\n",
      "Epoch 81\n",
      "Training Loss 0.222\n",
      "Training TIme 0.406\n",
      "Valid MSE 1.501\n",
      "Epoch 82\n",
      "Training Loss 0.221\n",
      "Training TIme 0.406\n",
      "Valid MSE 1.523\n",
      "Epoch 83\n",
      "Training Loss 0.220\n",
      "Training TIme 0.412\n",
      "Valid MSE 1.453\n",
      "Epoch 84\n",
      "Training Loss 0.220\n",
      "Training TIme 0.415\n",
      "Valid MSE 1.459\n",
      "Epoch 85\n",
      "Training Loss 0.219\n",
      "Training TIme 0.417\n",
      "Valid MSE 1.520\n",
      "Epoch 86\n",
      "Training Loss 0.220\n",
      "Training TIme 0.413\n",
      "Valid MSE 1.524\n",
      "Epoch 87\n",
      "Training Loss 0.220\n",
      "Training TIme 0.403\n",
      "Valid MSE 1.505\n",
      "Epoch 88\n",
      "Training Loss 0.221\n",
      "Training TIme 0.410\n",
      "Valid MSE 1.547\n",
      "Epoch 89\n",
      "Training Loss 0.222\n",
      "Training TIme 0.414\n",
      "Valid MSE 1.483\n",
      "Epoch 90\n",
      "Training Loss 0.219\n",
      "Training TIme 0.415\n",
      "Valid MSE 1.504\n",
      "Epoch 91\n",
      "Training Loss 0.216\n",
      "Training TIme 0.412\n",
      "Valid MSE 1.487\n",
      "Epoch 92\n",
      "Training Loss 0.222\n",
      "Training TIme 0.416\n",
      "Valid MSE 1.490\n",
      "Epoch 93\n",
      "Training Loss 0.218\n",
      "Training TIme 0.407\n",
      "Valid MSE 1.504\n",
      "Epoch 94\n",
      "Training Loss 0.219\n",
      "Training TIme 0.415\n",
      "Valid MSE 1.507\n",
      "Epoch 95\n",
      "Training Loss 0.216\n",
      "Training TIme 0.412\n",
      "Valid MSE 1.488\n",
      "Epoch 96\n",
      "Training Loss 0.219\n",
      "Training TIme 0.413\n",
      "Valid MSE 1.535\n",
      "Epoch 97\n",
      "Training Loss 0.218\n",
      "Training TIme 0.411\n",
      "Valid MSE 1.488\n",
      "Epoch 98\n",
      "Training Loss 0.215\n",
      "Training TIme 0.415\n",
      "Valid MSE 1.511\n",
      "Epoch 99\n",
      "Training Loss 0.213\n",
      "Training TIme 0.417\n",
      "Valid MSE 1.497\n",
      "Epoch 100\n",
      "Training Loss 0.215\n",
      "Training TIme 0.412\n",
      "Valid MSE 1.497\n"
     ]
    }
   ],
   "source": [
    "train_CMU_mosi(batch_size=32, epochs=100, lr=.001, max_rank=20, rank_adaptive=True,  \n",
    "                   warmup_epochs=50, kl_multiplier=1e-4, no_kl_epochs=5, accelerated=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34fc1bd7-6cb0-4c5a-96da-275a0f096bce",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Experiment with Naive, Fixed Rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "45082ff5-77c9-40c5-ba98-1405a30a1def",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "Training Loss 2.049\n",
      "Training TIme 0.714\n",
      "Valid MSE 1.925\n",
      "Epoch 2\n",
      "Training Loss 1.463\n",
      "Training TIme 0.699\n",
      "Valid MSE 1.865\n",
      "Epoch 3\n",
      "Training Loss 1.209\n",
      "Training TIme 0.702\n",
      "Valid MSE 1.670\n",
      "Epoch 4\n",
      "Training Loss 1.085\n",
      "Training TIme 0.729\n",
      "Valid MSE 1.711\n",
      "Epoch 5\n",
      "Training Loss 0.924\n",
      "Training TIme 0.706\n",
      "Valid MSE 1.608\n",
      "Epoch 6\n",
      "Training Loss 0.786\n",
      "Training TIme 0.704\n",
      "Valid MSE 1.612\n",
      "Epoch 7\n",
      "Training Loss 0.719\n",
      "Training TIme 0.714\n",
      "Valid MSE 1.861\n",
      "Epoch 8\n",
      "Training Loss 0.627\n",
      "Training TIme 0.708\n",
      "Valid MSE 1.562\n",
      "Epoch 9\n",
      "Training Loss 0.539\n",
      "Training TIme 0.706\n",
      "Valid MSE 1.704\n",
      "Epoch 10\n",
      "Training Loss 0.443\n",
      "Training TIme 0.705\n",
      "Valid MSE 1.596\n",
      "Epoch 11\n",
      "Training Loss 0.389\n",
      "Training TIme 0.709\n",
      "Valid MSE 1.628\n",
      "Epoch 12\n",
      "Training Loss 0.372\n",
      "Training TIme 0.695\n",
      "Valid MSE 1.585\n",
      "Epoch 13\n",
      "Training Loss 0.323\n",
      "Training TIme 0.726\n",
      "Valid MSE 1.804\n",
      "Epoch 14\n",
      "Training Loss 0.309\n",
      "Training TIme 0.755\n",
      "Valid MSE 1.730\n",
      "Epoch 15\n",
      "Training Loss 0.293\n",
      "Training TIme 0.706\n",
      "Valid MSE 1.706\n",
      "Epoch 16\n",
      "Training Loss 0.280\n",
      "Training TIme 0.706\n",
      "Valid MSE 1.732\n",
      "Epoch 17\n",
      "Training Loss 0.217\n",
      "Training TIme 0.735\n",
      "Valid MSE 1.746\n",
      "Epoch 18\n",
      "Training Loss 0.202\n",
      "Training TIme 0.733\n",
      "Valid MSE 1.665\n",
      "Epoch 19\n",
      "Training Loss 0.169\n",
      "Training TIme 0.779\n",
      "Valid MSE 1.671\n",
      "Epoch 20\n",
      "Training Loss 0.172\n",
      "Training TIme 0.724\n",
      "Valid MSE 1.635\n",
      "Epoch 21\n",
      "Training Loss 0.148\n",
      "Training TIme 0.729\n",
      "Valid MSE 1.571\n",
      "Epoch 22\n",
      "Training Loss 0.132\n",
      "Training TIme 0.689\n",
      "Valid MSE 1.648\n",
      "Epoch 23\n",
      "Training Loss 0.126\n",
      "Training TIme 0.695\n",
      "Valid MSE 1.675\n",
      "Epoch 24\n",
      "Training Loss 0.129\n",
      "Training TIme 0.683\n",
      "Valid MSE 1.586\n",
      "Epoch 25\n",
      "Training Loss 0.113\n",
      "Training TIme 0.680\n",
      "Valid MSE 1.605\n",
      "Epoch 26\n",
      "Training Loss 0.113\n",
      "Training TIme 0.686\n",
      "Valid MSE 1.588\n",
      "Epoch 27\n",
      "Training Loss 0.100\n",
      "Training TIme 0.724\n",
      "Valid MSE 1.606\n",
      "Epoch 28\n",
      "Training Loss 0.110\n",
      "Training TIme 0.710\n",
      "Valid MSE 1.594\n",
      "Epoch 29\n",
      "Training Loss 0.094\n",
      "Training TIme 0.723\n",
      "Valid MSE 1.610\n",
      "Epoch 30\n",
      "Training Loss 0.098\n",
      "Training TIme 0.718\n",
      "Valid MSE 1.586\n",
      "Epoch 31\n",
      "Training Loss 0.089\n",
      "Training TIme 0.723\n",
      "Valid MSE 1.593\n",
      "Epoch 32\n",
      "Training Loss 0.087\n",
      "Training TIme 0.753\n",
      "Valid MSE 1.626\n",
      "Epoch 33\n",
      "Training Loss 0.088\n",
      "Training TIme 0.807\n",
      "Valid MSE 1.581\n",
      "Epoch 34\n",
      "Training Loss 0.075\n",
      "Training TIme 0.754\n",
      "Valid MSE 1.645\n",
      "Epoch 35\n",
      "Training Loss 0.070\n",
      "Training TIme 0.808\n",
      "Valid MSE 1.579\n",
      "Epoch 36\n",
      "Training Loss 0.078\n",
      "Training TIme 0.724\n",
      "Valid MSE 1.624\n",
      "Epoch 37\n",
      "Training Loss 0.076\n",
      "Training TIme 0.719\n",
      "Valid MSE 1.631\n",
      "Epoch 38\n",
      "Training Loss 0.078\n",
      "Training TIme 0.755\n",
      "Valid MSE 1.573\n",
      "Epoch 39\n",
      "Training Loss 0.066\n",
      "Training TIme 0.697\n",
      "Valid MSE 1.621\n",
      "Epoch 40\n",
      "Training Loss 0.072\n",
      "Training TIme 0.709\n",
      "Valid MSE 1.565\n",
      "Epoch 41\n",
      "Training Loss 0.075\n",
      "Training TIme 0.698\n",
      "Valid MSE 1.562\n",
      "Epoch 42\n",
      "Training Loss 0.073\n",
      "Training TIme 0.702\n",
      "Valid MSE 1.597\n",
      "Epoch 43\n",
      "Training Loss 0.066\n",
      "Training TIme 0.702\n",
      "Valid MSE 1.572\n",
      "Epoch 44\n",
      "Training Loss 0.060\n",
      "Training TIme 0.685\n",
      "Valid MSE 1.521\n",
      "Epoch 45\n",
      "Training Loss 0.064\n",
      "Training TIme 0.683\n",
      "Valid MSE 1.633\n",
      "Epoch 46\n",
      "Training Loss 0.062\n",
      "Training TIme 0.680\n",
      "Valid MSE 1.625\n",
      "Epoch 47\n",
      "Training Loss 0.066\n",
      "Training TIme 0.688\n",
      "Valid MSE 1.564\n",
      "Epoch 48\n",
      "Training Loss 0.062\n",
      "Training TIme 0.691\n",
      "Valid MSE 1.599\n",
      "Epoch 49\n",
      "Training Loss 0.063\n",
      "Training TIme 0.692\n",
      "Valid MSE 1.579\n",
      "Epoch 50\n",
      "Training Loss 0.063\n",
      "Training TIme 0.698\n",
      "Valid MSE 1.554\n",
      "Epoch 51\n",
      "Training Loss 0.052\n",
      "Training TIme 0.693\n",
      "Valid MSE 1.530\n",
      "Epoch 52\n",
      "Training Loss 0.052\n",
      "Training TIme 0.701\n",
      "Valid MSE 1.599\n",
      "Epoch 53\n",
      "Training Loss 0.057\n",
      "Training TIme 0.688\n",
      "Valid MSE 1.584\n",
      "Epoch 54\n",
      "Training Loss 0.053\n",
      "Training TIme 0.693\n",
      "Valid MSE 1.565\n",
      "Epoch 55\n",
      "Training Loss 0.053\n",
      "Training TIme 0.700\n",
      "Valid MSE 1.611\n",
      "Epoch 56\n",
      "Training Loss 0.056\n",
      "Training TIme 0.691\n",
      "Valid MSE 1.569\n",
      "Epoch 57\n",
      "Training Loss 0.053\n",
      "Training TIme 0.705\n",
      "Valid MSE 1.525\n",
      "Epoch 58\n",
      "Training Loss 0.051\n",
      "Training TIme 0.693\n",
      "Valid MSE 1.552\n",
      "Epoch 59\n",
      "Training Loss 0.050\n",
      "Training TIme 0.694\n",
      "Valid MSE 1.537\n",
      "Epoch 60\n",
      "Training Loss 0.046\n",
      "Training TIme 0.696\n",
      "Valid MSE 1.593\n",
      "Epoch 61\n",
      "Training Loss 0.051\n",
      "Training TIme 0.704\n",
      "Valid MSE 1.601\n",
      "Epoch 62\n",
      "Training Loss 0.046\n",
      "Training TIme 0.741\n",
      "Valid MSE 1.548\n",
      "Epoch 63\n",
      "Training Loss 0.044\n",
      "Training TIme 0.738\n",
      "Valid MSE 1.576\n",
      "Epoch 64\n",
      "Training Loss 0.048\n",
      "Training TIme 0.757\n",
      "Valid MSE 1.544\n",
      "Epoch 65\n",
      "Training Loss 0.050\n",
      "Training TIme 0.723\n",
      "Valid MSE 1.552\n",
      "Epoch 66\n",
      "Training Loss 0.049\n",
      "Training TIme 0.828\n",
      "Valid MSE 1.548\n",
      "Epoch 67\n",
      "Training Loss 0.052\n",
      "Training TIme 0.809\n",
      "Valid MSE 1.574\n",
      "Epoch 68\n",
      "Training Loss 0.046\n",
      "Training TIme 0.822\n",
      "Valid MSE 1.543\n",
      "Epoch 69\n",
      "Training Loss 0.041\n",
      "Training TIme 0.776\n",
      "Valid MSE 1.543\n",
      "Epoch 70\n",
      "Training Loss 0.049\n",
      "Training TIme 0.782\n",
      "Valid MSE 1.547\n",
      "Epoch 71\n",
      "Training Loss 0.047\n",
      "Training TIme 0.728\n",
      "Valid MSE 1.522\n",
      "Epoch 72\n",
      "Training Loss 0.044\n",
      "Training TIme 0.807\n",
      "Valid MSE 1.582\n",
      "Epoch 73\n",
      "Training Loss 0.043\n",
      "Training TIme 0.815\n",
      "Valid MSE 1.593\n",
      "Epoch 74\n",
      "Training Loss 0.045\n",
      "Training TIme 0.779\n",
      "Valid MSE 1.541\n",
      "Epoch 75\n",
      "Training Loss 0.043\n",
      "Training TIme 0.765\n",
      "Valid MSE 1.555\n",
      "Epoch 76\n",
      "Training Loss 0.040\n",
      "Training TIme 0.723\n",
      "Valid MSE 1.570\n",
      "Epoch 77\n",
      "Training Loss 0.046\n",
      "Training TIme 0.708\n",
      "Valid MSE 1.586\n",
      "Epoch 78\n",
      "Training Loss 0.047\n",
      "Training TIme 0.697\n",
      "Valid MSE 1.589\n",
      "Epoch 79\n",
      "Training Loss 0.037\n",
      "Training TIme 0.714\n",
      "Valid MSE 1.566\n",
      "Epoch 80\n",
      "Training Loss 0.045\n",
      "Training TIme 0.699\n",
      "Valid MSE 1.535\n",
      "Epoch 81\n",
      "Training Loss 0.043\n",
      "Training TIme 0.707\n",
      "Valid MSE 1.579\n",
      "Epoch 82\n",
      "Training Loss 0.040\n",
      "Training TIme 0.708\n",
      "Valid MSE 1.557\n",
      "Epoch 83\n",
      "Training Loss 0.042\n",
      "Training TIme 0.700\n",
      "Valid MSE 1.573\n",
      "Epoch 84\n",
      "Training Loss 0.043\n",
      "Training TIme 0.705\n",
      "Valid MSE 1.551\n",
      "Epoch 85\n",
      "Training Loss 0.044\n",
      "Training TIme 0.705\n",
      "Valid MSE 1.587\n",
      "Epoch 86\n",
      "Training Loss 0.045\n",
      "Training TIme 0.703\n",
      "Valid MSE 1.560\n",
      "Epoch 87\n",
      "Training Loss 0.043\n",
      "Training TIme 0.698\n",
      "Valid MSE 1.524\n",
      "Epoch 88\n",
      "Training Loss 0.041\n",
      "Training TIme 0.699\n",
      "Valid MSE 1.556\n",
      "Epoch 89\n",
      "Training Loss 0.039\n",
      "Training TIme 0.698\n",
      "Valid MSE 1.557\n",
      "Epoch 90\n",
      "Training Loss 0.043\n",
      "Training TIme 0.690\n",
      "Valid MSE 1.569\n",
      "Epoch 91\n",
      "Training Loss 0.041\n",
      "Training TIme 0.699\n",
      "Valid MSE 1.551\n",
      "Epoch 92\n",
      "Training Loss 0.037\n",
      "Training TIme 0.692\n",
      "Valid MSE 1.551\n",
      "Epoch 93\n",
      "Training Loss 0.038\n",
      "Training TIme 0.691\n",
      "Valid MSE 1.568\n",
      "Epoch 94\n",
      "Training Loss 0.038\n",
      "Training TIme 0.699\n",
      "Valid MSE 1.573\n",
      "Epoch 95\n",
      "Training Loss 0.041\n",
      "Training TIme 0.692\n",
      "Valid MSE 1.553\n",
      "Epoch 96\n",
      "Training Loss 0.041\n",
      "Training TIme 0.708\n",
      "Valid MSE 1.539\n",
      "Epoch 97\n",
      "Training Loss 0.037\n",
      "Training TIme 0.694\n",
      "Valid MSE 1.556\n",
      "Epoch 98\n",
      "Training Loss 0.039\n",
      "Training TIme 0.693\n",
      "Valid MSE 1.529\n",
      "Epoch 99\n",
      "Training Loss 0.037\n",
      "Training TIme 0.688\n",
      "Valid MSE 1.531\n",
      "Epoch 100\n",
      "Training Loss 0.036\n",
      "Training TIme 0.694\n",
      "Valid MSE 1.558\n"
     ]
    }
   ],
   "source": [
    "train_CMU_mosi(batch_size=32, epochs=100, lr=.001, max_rank=20, rank_adaptive=False,  \n",
    "                   warmup_epochs=50, kl_multiplier=1e-4, no_kl_epochs=5, accelerated=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb94463a-f282-48a1-8261-06cf358c544f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Experiment with Accelerated, Fixed Rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "78f37e30-66f7-41b0-a10a-6d85955cbd74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "Training Loss 2.105\n",
      "Training TIme 0.410\n",
      "Valid MSE 2.071\n",
      "Epoch 2\n",
      "Training Loss 1.618\n",
      "Training TIme 0.436\n",
      "Valid MSE 1.807\n",
      "Epoch 3\n",
      "Training Loss 1.284\n",
      "Training TIme 0.410\n",
      "Valid MSE 1.573\n",
      "Epoch 4\n",
      "Training Loss 1.126\n",
      "Training TIme 0.395\n",
      "Valid MSE 1.517\n",
      "Epoch 5\n",
      "Training Loss 0.955\n",
      "Training TIme 0.390\n",
      "Valid MSE 1.535\n",
      "Epoch 6\n",
      "Training Loss 0.823\n",
      "Training TIme 0.399\n",
      "Valid MSE 1.628\n",
      "Epoch 7\n",
      "Training Loss 0.695\n",
      "Training TIme 0.426\n",
      "Valid MSE 1.772\n",
      "Epoch 8\n",
      "Training Loss 0.616\n",
      "Training TIme 0.409\n",
      "Valid MSE 1.664\n",
      "Epoch 9\n",
      "Training Loss 0.543\n",
      "Training TIme 0.397\n",
      "Valid MSE 1.629\n",
      "Epoch 10\n",
      "Training Loss 0.492\n",
      "Training TIme 0.407\n",
      "Valid MSE 1.598\n",
      "Epoch 11\n",
      "Training Loss 0.407\n",
      "Training TIme 0.389\n",
      "Valid MSE 1.646\n",
      "Epoch 12\n",
      "Training Loss 0.325\n",
      "Training TIme 0.423\n",
      "Valid MSE 1.661\n",
      "Epoch 13\n",
      "Training Loss 0.299\n",
      "Training TIme 0.397\n",
      "Valid MSE 1.656\n",
      "Epoch 14\n",
      "Training Loss 0.239\n",
      "Training TIme 0.410\n",
      "Valid MSE 1.595\n",
      "Epoch 15\n",
      "Training Loss 0.202\n",
      "Training TIme 0.424\n",
      "Valid MSE 1.711\n",
      "Epoch 16\n",
      "Training Loss 0.202\n",
      "Training TIme 0.416\n",
      "Valid MSE 1.640\n",
      "Epoch 17\n",
      "Training Loss 0.175\n",
      "Training TIme 0.402\n",
      "Valid MSE 1.534\n",
      "Epoch 18\n",
      "Training Loss 0.170\n",
      "Training TIme 0.395\n",
      "Valid MSE 1.672\n",
      "Epoch 19\n",
      "Training Loss 0.148\n",
      "Training TIme 0.386\n",
      "Valid MSE 1.606\n",
      "Epoch 20\n",
      "Training Loss 0.125\n",
      "Training TIme 0.418\n",
      "Valid MSE 1.576\n",
      "Epoch 21\n",
      "Training Loss 0.118\n",
      "Training TIme 0.387\n",
      "Valid MSE 1.596\n",
      "Epoch 22\n",
      "Training Loss 0.121\n",
      "Training TIme 0.385\n",
      "Valid MSE 1.634\n",
      "Epoch 23\n",
      "Training Loss 0.108\n",
      "Training TIme 0.413\n",
      "Valid MSE 1.647\n",
      "Epoch 24\n",
      "Training Loss 0.100\n",
      "Training TIme 0.407\n",
      "Valid MSE 1.589\n",
      "Epoch 25\n",
      "Training Loss 0.104\n",
      "Training TIme 0.390\n",
      "Valid MSE 1.611\n",
      "Epoch 26\n",
      "Training Loss 0.086\n",
      "Training TIme 0.388\n",
      "Valid MSE 1.567\n",
      "Epoch 27\n",
      "Training Loss 0.099\n",
      "Training TIme 0.389\n",
      "Valid MSE 1.577\n",
      "Epoch 28\n",
      "Training Loss 0.088\n",
      "Training TIme 0.386\n",
      "Valid MSE 1.549\n",
      "Epoch 29\n",
      "Training Loss 0.082\n",
      "Training TIme 0.387\n",
      "Valid MSE 1.529\n",
      "Epoch 30\n",
      "Training Loss 0.086\n",
      "Training TIme 0.381\n",
      "Valid MSE 1.639\n",
      "Epoch 31\n",
      "Training Loss 0.076\n",
      "Training TIme 0.404\n",
      "Valid MSE 1.579\n",
      "Epoch 32\n",
      "Training Loss 0.075\n",
      "Training TIme 0.413\n",
      "Valid MSE 1.514\n",
      "Epoch 33\n",
      "Training Loss 0.069\n",
      "Training TIme 0.407\n",
      "Valid MSE 1.582\n",
      "Epoch 34\n",
      "Training Loss 0.064\n",
      "Training TIme 0.392\n",
      "Valid MSE 1.635\n",
      "Epoch 35\n",
      "Training Loss 0.065\n",
      "Training TIme 0.397\n",
      "Valid MSE 1.553\n",
      "Epoch 36\n",
      "Training Loss 0.061\n",
      "Training TIme 0.419\n",
      "Valid MSE 1.587\n",
      "Epoch 37\n",
      "Training Loss 0.068\n",
      "Training TIme 0.404\n",
      "Valid MSE 1.586\n",
      "Epoch 38\n",
      "Training Loss 0.065\n",
      "Training TIme 0.389\n",
      "Valid MSE 1.574\n",
      "Epoch 39\n",
      "Training Loss 0.077\n",
      "Training TIme 0.401\n",
      "Valid MSE 1.553\n",
      "Epoch 40\n",
      "Training Loss 0.060\n",
      "Training TIme 0.394\n",
      "Valid MSE 1.603\n",
      "Epoch 41\n",
      "Training Loss 0.059\n",
      "Training TIme 0.403\n",
      "Valid MSE 1.561\n",
      "Epoch 42\n",
      "Training Loss 0.059\n",
      "Training TIme 0.402\n",
      "Valid MSE 1.599\n",
      "Epoch 43\n",
      "Training Loss 0.060\n",
      "Training TIme 0.455\n",
      "Valid MSE 1.587\n",
      "Epoch 44\n",
      "Training Loss 0.062\n",
      "Training TIme 0.392\n",
      "Valid MSE 1.561\n",
      "Epoch 45\n",
      "Training Loss 0.063\n",
      "Training TIme 0.390\n",
      "Valid MSE 1.609\n",
      "Epoch 46\n",
      "Training Loss 0.061\n",
      "Training TIme 0.408\n",
      "Valid MSE 1.550\n",
      "Epoch 47\n",
      "Training Loss 0.061\n",
      "Training TIme 0.408\n",
      "Valid MSE 1.525\n",
      "Epoch 48\n",
      "Training Loss 0.054\n",
      "Training TIme 0.432\n",
      "Valid MSE 1.587\n",
      "Epoch 49\n",
      "Training Loss 0.052\n",
      "Training TIme 0.413\n",
      "Valid MSE 1.567\n",
      "Epoch 50\n",
      "Training Loss 0.053\n",
      "Training TIme 0.405\n",
      "Valid MSE 1.572\n",
      "Epoch 51\n",
      "Training Loss 0.048\n",
      "Training TIme 0.405\n",
      "Valid MSE 1.532\n",
      "Epoch 52\n",
      "Training Loss 0.051\n",
      "Training TIme 0.444\n",
      "Valid MSE 1.628\n",
      "Epoch 53\n",
      "Training Loss 0.052\n",
      "Training TIme 0.484\n",
      "Valid MSE 1.532\n",
      "Epoch 54\n",
      "Training Loss 0.044\n",
      "Training TIme 0.406\n",
      "Valid MSE 1.530\n",
      "Epoch 55\n",
      "Training Loss 0.050\n",
      "Training TIme 0.410\n",
      "Valid MSE 1.523\n",
      "Epoch 56\n",
      "Training Loss 0.046\n",
      "Training TIme 0.411\n",
      "Valid MSE 1.518\n",
      "Epoch 57\n",
      "Training Loss 0.053\n",
      "Training TIme 0.399\n",
      "Valid MSE 1.586\n",
      "Epoch 58\n",
      "Training Loss 0.052\n",
      "Training TIme 0.414\n",
      "Valid MSE 1.522\n",
      "Epoch 59\n",
      "Training Loss 0.052\n",
      "Training TIme 0.423\n",
      "Valid MSE 1.539\n",
      "Epoch 60\n",
      "Training Loss 0.050\n",
      "Training TIme 0.413\n",
      "Valid MSE 1.529\n",
      "Epoch 61\n",
      "Training Loss 0.043\n",
      "Training TIme 0.423\n",
      "Valid MSE 1.545\n",
      "Epoch 62\n",
      "Training Loss 0.044\n",
      "Training TIme 0.429\n",
      "Valid MSE 1.549\n",
      "Epoch 63\n",
      "Training Loss 0.043\n",
      "Training TIme 0.406\n",
      "Valid MSE 1.556\n",
      "Epoch 64\n",
      "Training Loss 0.047\n",
      "Training TIme 0.413\n",
      "Valid MSE 1.556\n",
      "Epoch 65\n",
      "Training Loss 0.044\n",
      "Training TIme 0.406\n",
      "Valid MSE 1.581\n",
      "Epoch 66\n",
      "Training Loss 0.042\n",
      "Training TIme 0.404\n",
      "Valid MSE 1.543\n",
      "Epoch 67\n",
      "Training Loss 0.042\n",
      "Training TIme 0.397\n",
      "Valid MSE 1.553\n",
      "Epoch 68\n",
      "Training Loss 0.045\n",
      "Training TIme 0.402\n",
      "Valid MSE 1.531\n",
      "Epoch 69\n",
      "Training Loss 0.040\n",
      "Training TIme 0.398\n",
      "Valid MSE 1.557\n",
      "Epoch 70\n",
      "Training Loss 0.039\n",
      "Training TIme 0.395\n",
      "Valid MSE 1.577\n",
      "Epoch 71\n",
      "Training Loss 0.041\n",
      "Training TIme 0.406\n",
      "Valid MSE 1.499\n",
      "Epoch 72\n",
      "Training Loss 0.039\n",
      "Training TIme 0.395\n",
      "Valid MSE 1.574\n",
      "Epoch 73\n",
      "Training Loss 0.040\n",
      "Training TIme 0.393\n",
      "Valid MSE 1.589\n",
      "Epoch 74\n",
      "Training Loss 0.040\n",
      "Training TIme 0.398\n",
      "Valid MSE 1.558\n",
      "Epoch 75\n",
      "Training Loss 0.042\n",
      "Training TIme 0.396\n",
      "Valid MSE 1.573\n",
      "Epoch 76\n",
      "Training Loss 0.044\n",
      "Training TIme 0.397\n",
      "Valid MSE 1.473\n",
      "Epoch 77\n",
      "Training Loss 0.040\n",
      "Training TIme 0.394\n",
      "Valid MSE 1.534\n",
      "Epoch 78\n",
      "Training Loss 0.039\n",
      "Training TIme 0.397\n",
      "Valid MSE 1.504\n",
      "Epoch 79\n",
      "Training Loss 0.039\n",
      "Training TIme 0.404\n",
      "Valid MSE 1.533\n",
      "Epoch 80\n",
      "Training Loss 0.041\n",
      "Training TIme 0.393\n",
      "Valid MSE 1.526\n",
      "Epoch 81\n",
      "Training Loss 0.036\n",
      "Training TIme 0.394\n",
      "Valid MSE 1.517\n",
      "Epoch 82\n",
      "Training Loss 0.038\n",
      "Training TIme 0.398\n",
      "Valid MSE 1.561\n",
      "Epoch 83\n",
      "Training Loss 0.036\n",
      "Training TIme 0.404\n",
      "Valid MSE 1.527\n",
      "Epoch 84\n",
      "Training Loss 0.037\n",
      "Training TIme 0.399\n",
      "Valid MSE 1.502\n",
      "Epoch 85\n",
      "Training Loss 0.036\n",
      "Training TIme 0.395\n",
      "Valid MSE 1.628\n",
      "Epoch 86\n",
      "Training Loss 0.039\n",
      "Training TIme 0.395\n",
      "Valid MSE 1.507\n",
      "Epoch 87\n",
      "Training Loss 0.041\n",
      "Training TIme 0.402\n",
      "Valid MSE 1.515\n",
      "Epoch 88\n",
      "Training Loss 0.038\n",
      "Training TIme 0.395\n",
      "Valid MSE 1.508\n",
      "Epoch 89\n",
      "Training Loss 0.036\n",
      "Training TIme 0.403\n",
      "Valid MSE 1.606\n",
      "Epoch 90\n",
      "Training Loss 0.041\n",
      "Training TIme 0.405\n",
      "Valid MSE 1.543\n",
      "Epoch 91\n",
      "Training Loss 0.036\n",
      "Training TIme 0.396\n",
      "Valid MSE 1.567\n",
      "Epoch 92\n",
      "Training Loss 0.036\n",
      "Training TIme 0.398\n",
      "Valid MSE 1.565\n",
      "Epoch 93\n",
      "Training Loss 0.041\n",
      "Training TIme 0.404\n",
      "Valid MSE 1.540\n",
      "Epoch 94\n",
      "Training Loss 0.038\n",
      "Training TIme 0.398\n",
      "Valid MSE 1.561\n",
      "Epoch 95\n",
      "Training Loss 0.038\n",
      "Training TIme 0.395\n",
      "Valid MSE 1.571\n",
      "Epoch 96\n",
      "Training Loss 0.034\n",
      "Training TIme 0.398\n",
      "Valid MSE 1.532\n",
      "Epoch 97\n",
      "Training Loss 0.035\n",
      "Training TIme 0.396\n",
      "Valid MSE 1.520\n",
      "Epoch 98\n",
      "Training Loss 0.037\n",
      "Training TIme 0.394\n",
      "Valid MSE 1.528\n",
      "Epoch 99\n",
      "Training Loss 0.038\n",
      "Training TIme 0.402\n",
      "Valid MSE 1.559\n",
      "Epoch 100\n",
      "Training Loss 0.042\n",
      "Training TIme 0.400\n",
      "Valid MSE 1.572\n"
     ]
    }
   ],
   "source": [
    "train_CMU_mosi(batch_size=32, epochs=100, lr=.001, max_rank=20, rank_adaptive=False,  \n",
    "                   warmup_epochs=50, kl_multiplier=1e-4, no_kl_epochs=5, accelerated=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d1c150b-7212-4c19-be0c-e034e067a440",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
