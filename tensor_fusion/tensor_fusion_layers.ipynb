{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5e213020-c6a8-4371-8254-e191084e5f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import pickle\n",
    "\n",
    "from tensor_layers import low_rank_tensors\n",
    "from tensor_layers import TensorizedLinear\n",
    "\n",
    "from model import SubNet, TextSubNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "138caa74-3cda-4ea5-80af-2015306adde2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Adaptive_Rank_CP_Linear(nn.Module):\n",
    "\n",
    "    def __init__(self, input_sizes, output_size, max_rank=20, em_stepsize=1.0, \n",
    "                 prior_type='log_uniform', eta=None):\n",
    "\n",
    "        super(Adaptive_Rank_CP_Linear, self).__init__()\n",
    "\n",
    "        self.input_sizes = input_sizes\n",
    "        self.output_size = output_size\n",
    "        \n",
    "        shape = input_sizes + (output_size,)\n",
    "        target_stddev = np.sqrt(2 / np.prod(self.input_sizes))\n",
    "        self.tensor = getattr(low_rank_tensors, 'CP')(shape, prior_type=prior_type, em_stepsize=em_stepsize,\n",
    "                                                      max_rank=max_rank, initialization_method='nn', \n",
    "                                                      target_stddev=target_stddev, learned_scale=False, \n",
    "                                                      eta=eta)\n",
    "\n",
    "    def forward(self, inputs, rank_update=True):\n",
    "        \n",
    "        if self.training and rank_update:\n",
    "            self.tensor.update_rank_parameters()\n",
    "        \n",
    "        # y = ((x_1 @ W_1)(x_2 @ W_2)...(x_M @ W_M)) @ W_y.T\n",
    "        y = torch.ones(size=(1,))\n",
    "        for i, x in enumerate(inputs):\n",
    "            y = y * (x @ self.tensor.factors[i])\n",
    "        y = y @ self.tensor.factors[-1].T\n",
    "\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3175c850-be95-4da9-8c4d-ee604c510a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Fixed_Rank_CP_Linear(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_sizes, output_size, rank=20):\n",
    "        \n",
    "        super(Fixed_Rank_CP_Linear, self).__init__()\n",
    "        \n",
    "        self.input_sizes = input_sizes\n",
    "        self.output_size = output_size\n",
    "        self.rank = rank\n",
    "        \n",
    "        self.tensor_factors = self.initialize_tensor_factors()\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        \n",
    "        # y = ((x_1 @ W_1)(x_2 @ W_2)...(x_M @ W_M)) @ W_y.T\n",
    "        y = torch.ones(size=(1,))\n",
    "        for i, x in enumerate(inputs):\n",
    "            y = y * (x @ self.tensor_factors[i])\n",
    "        y = y @ self.tensor_factors[-1].T\n",
    "\n",
    "        return y\n",
    "        \n",
    "    def initialize_tensor_factors(self):\n",
    "        \n",
    "        factors = []\n",
    "        for m, input_size in enumerate(self.input_sizes):\n",
    "            factors.append(nn.Parameter(torch.empty(input_size, self.rank)))\n",
    "            nn.init.xavier_normal_(factors[m])\n",
    "        factors.append(nn.Parameter(torch.empty(self.output_size, self.rank)))\n",
    "        nn.init.xavier_normal_(factors[-1])\n",
    "            \n",
    "        return nn.ParameterList(factors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "575294f8-70bc-4ad6-ab89-31b8aca0901c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Old_CP_Tensor_Fusion_Network(nn.Module):\n",
    "    '''\n",
    "    Implements the Tensor Fusion Networks for multimodal sentiment analysis as is described in:\n",
    "    Zadeh, Amir, et al. \"Tensor fusion network for multimodal sentiment analysis.\" EMNLP 2017 Oral.\n",
    "    with rank-adaptive tensorized training from Hawkins, Cole and Zheng Zhang \"Bayesian tensorized\n",
    "    neural networks with automatic rank selection.\" Neurocomputing 2021.\n",
    "    '''\n",
    "\n",
    "    def __init__(self, input_sizes, hidden_sizes, output_size, max_rank, em_stepsize=1.0, \n",
    "                 prior_type='log_uniform', eta=None):\n",
    "        '''\n",
    "        Args:\n",
    "            input_sizes - a length-3 tuple that contains (x_1_size, x_2_size, x_3_size)\n",
    "            hidden_sizes - a length-3 tuple that contains (hidden_size_1, hidden_size_2, hidden_size_3)\n",
    "            output_size - an integer specifying the size of the output\n",
    "            max_rank - an integer specifying the maximum rank of weight tensor\n",
    "        '''\n",
    "\n",
    "        super(Old_CP_Tensor_Fusion_Network, self).__init__()\n",
    "\n",
    "        self.input_sizes = input_sizes\n",
    "        self.hidden_sizes = hidden_sizes\n",
    "        self.output_size = output_size\n",
    "        self.max_rank = max_rank\n",
    "        self.rank_adaptive = rank_adaptive\n",
    "        \n",
    "        self.audio_subnet = SubNet(input_sizes[0], hidden_sizes[0], dropout=0.3)\n",
    "        self.video_subnet = SubNet(input_sizes[1], hidden_sizes[1], dropout=0.3)\n",
    "        self.text_subnet = TextSubNet(input_sizes[2], hidden_sizes[2], hidden_sizes[2], dropout=0.3)\n",
    "        \n",
    "        input_sizes = (hidden_sizes[0] + 1, hidden_sizes[1] + 1, hidden_sizes[2] + 1)\n",
    "        in_features = np.prod(input_sizes)\n",
    "        shape = input_sizes + (output_size,)\n",
    "        self.tensor_fusion_layer = TensorizedLinear(in_features, output_size, shape=shape,\n",
    "                                                    tensor_type='CP', max_rank=max_rank, em_stepsize=1.0, \n",
    "                                                    prior_type='log_uniform', eta=None)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        \n",
    "        # subnet outputs\n",
    "        z_audio = self.audio_subnet(inputs[0])\n",
    "        z_video = self.video_subnet(inputs[1])\n",
    "        z_text = self.text_subnet(inputs[2])\n",
    "\n",
    "        batch_size = z_audio.data.shape[0]\n",
    "\n",
    "        if z_audio.is_cuda:\n",
    "            DTYPE = torch.cuda.FloatTensor\n",
    "        else:\n",
    "            DTYPE = torch.FloatTensor\n",
    "\n",
    "        # 1 in concatenated to each subnet outputs\n",
    "        z_audio = torch.cat((Variable(torch.ones(batch_size, 1).type(DTYPE), requires_grad=False), z_audio), dim=1)\n",
    "        z_video = torch.cat((Variable(torch.ones(batch_size, 1).type(DTYPE), requires_grad=False), z_video), dim=1)\n",
    "        z_text = torch.cat((Variable(torch.ones(batch_size, 1).type(DTYPE), requires_grad=False), z_text), dim=1)\n",
    "\n",
    "        output = self.tensor_fusion_layer([z_audio, z_video, z_text])\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "fdf72173-1b62-4d0e-86cc-eaa052f89349",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CP_Tensor_Fusion_Network(nn.Module):\n",
    "    '''\n",
    "    Implements the Tensor Fusion Networks for multimodal sentiment analysis as is described in:\n",
    "    Zadeh, Amir, et al. \"Tensor fusion network for multimodal sentiment analysis.\" EMNLP 2017 Oral.\n",
    "    with rank-adaptive tensorized training from Hawkins, Cole and Zheng Zhang \"Bayesian tensorized\n",
    "    neural networks with automatic rank selection.\" Neurocomputing 2021.\n",
    "    '''\n",
    "\n",
    "    def __init__(self, input_sizes, hidden_sizes, output_size, max_rank,\n",
    "                 rank_adaptive=True, em_stepsize=1.0, prior_type='log_uniform',\n",
    "                 eta=None):\n",
    "        '''\n",
    "        Args:\n",
    "            input_sizes - a length-3 tuple that contains (x_1_size, x_2_size, x_3_size)\n",
    "            hidden_sizes - a length-3 tuple that contains (hidden_size_1, hidden_size_2, hidden_size_3)\n",
    "            output_size - an integer specifying the size of the output\n",
    "            max_rank - an integer specifying the maximum rank of weight tensor\n",
    "        '''\n",
    "\n",
    "        super(CP_Tensor_Fusion_Network, self).__init__()\n",
    "\n",
    "        self.input_sizes = input_sizes\n",
    "        self.hidden_sizes = hidden_sizes\n",
    "        self.output_size = output_size\n",
    "        self.max_rank = max_rank\n",
    "        self.rank_adaptive = rank_adaptive\n",
    "        \n",
    "        self.audio_subnet = SubNet(input_sizes[0], hidden_sizes[0], dropout=0.3)\n",
    "        self.video_subnet = SubNet(input_sizes[1], hidden_sizes[1], dropout=0.3)\n",
    "        self.text_subnet = TextSubNet(input_sizes[2], hidden_sizes[2], hidden_sizes[2], dropout=0.3)\n",
    "        \n",
    "        input_sizes = (hidden_sizes[0] + 1, hidden_sizes[1] + 1, hidden_sizes[2] + 1)\n",
    "        \n",
    "        if rank_adaptive:\n",
    "            self.tensor_fusion_layer = Adaptive_Rank_CP_Linear(input_sizes, output_size, max_rank, \n",
    "                                                               em_stepsize, prior_type, eta)\n",
    "        else:\n",
    "            self.tensor_fusion_layer = Fixed_Rank_CP_Linear(input_sizes, output_size, max_rank)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        \n",
    "        # subnet outputs\n",
    "        z_audio = self.audio_subnet(inputs[0])\n",
    "        z_video = self.video_subnet(inputs[1])\n",
    "        z_text = self.text_subnet(inputs[2])\n",
    "\n",
    "        batch_size = z_audio.data.shape[0]\n",
    "\n",
    "        if z_audio.is_cuda:\n",
    "            DTYPE = torch.cuda.FloatTensor\n",
    "        else:\n",
    "            DTYPE = torch.FloatTensor\n",
    "\n",
    "        # 1 in concatenated to each subnet outputs\n",
    "        z_audio = torch.cat((Variable(torch.ones(batch_size, 1).type(DTYPE), requires_grad=False), z_audio), dim=1)\n",
    "        z_video = torch.cat((Variable(torch.ones(batch_size, 1).type(DTYPE), requires_grad=False), z_video), dim=1)\n",
    "        z_text = torch.cat((Variable(torch.ones(batch_size, 1).type(DTYPE), requires_grad=False), z_text), dim=1)\n",
    "\n",
    "        output = self.tensor_fusion_layer([z_audio, z_video, z_text])\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e8d7d660-3aaf-4dc8-a7fb-8068f1ff9d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultimodalDataset(Dataset):\n",
    "    '''\n",
    "    Dataset for CMU-MOSI\n",
    "    '''\n",
    "    def __init__(self, text, audio, vision, labels):\n",
    "        '''\n",
    "        args:\n",
    "            text: text modality feature of shape (N, seq. length, text_input_size)\n",
    "            audio: audio modality feature of shape (N, seq. length, audio_input_size)\n",
    "            vision: vision modality feature of shape (N, seq. length, vision_input_size)\n",
    "            labels: labels of shape (N, 1) and ranges from -3 to 3\n",
    "        '''\n",
    "        self.text = text\n",
    "        self.audio = audio\n",
    "        self.vision = vision\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        '''\n",
    "        Returns an individual data composed of (features, label)\n",
    "        where features is a dictionary {'text': , 'audio':, 'vision':}\n",
    "        Returns:\n",
    "            features['text']: text modality feature of shape (seq. length, text_input_size)\n",
    "            features['audio']: audio modality feature of shape (audio_input_size)\n",
    "            features['vision']: vision modality feature of shape (vision_input_size)\n",
    "            label: a scalar label that ranges from -3 to 3\n",
    "        '''\n",
    "        features = dict()\n",
    "        features['text'] = self.text[idx]\n",
    "        # audio and vision features are averaged across time\n",
    "        features['audio'] = np.mean(self.audio[idx], axis=0)\n",
    "        features['vision'] = np.mean(self.vision[idx], axis=0)\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        return features, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "14886dba-5314-4610-a863-b30497366244",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_kl_loss(model, kl_multiplier, no_kl_epochs, warmup_epochs, epoch):\n",
    "    '''\n",
    "    kl loss for rank reduction\n",
    "    '''\n",
    "    kl_loss = 0.0\n",
    "    for layer in model.modules():\n",
    "        if hasattr(layer, \"tensor\"):\n",
    "            kl_loss += layer.tensor.get_kl_divergence_to_prior()\n",
    "            \n",
    "    kl_mult = kl_multiplier * torch.clamp(torch.tensor(((epoch - no_kl_epochs) / \n",
    "                                                        warmup_epochs)), 0.0, 1.0)\n",
    "    print(\"KL loss \",kl_loss.item())\n",
    "    \n",
    "    return kl_loss*kl_mult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "cfc00954-c881-4528-819d-cf5fa1338325",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_CMU_mosi(batch_size=32, epochs=100, lr=.001, max_rank=20, rank_adaptive=True,  \n",
    "                   warmup_epochs=50, kl_multiplier=1e-4, no_kl_epochs=5):\n",
    "\n",
    "    # load dataset file\n",
    "    file = open('../../dataset/cmu-mosi/mosi_20_seq_data.pkl', 'rb')\n",
    "    data = pickle.load(file)\n",
    "    file.close()\n",
    "\n",
    "    # prepare the datasets and data loaders\n",
    "    train_set = MultimodalDataset(data['train']['text'], data['train']['audio'],\n",
    "                                  data['train']['vision'], data['train']['labels'])\n",
    "    valid_set = MultimodalDataset(data['valid']['text'], data['valid']['audio'],\n",
    "                                  data['valid']['vision'], data['valid']['labels'])\n",
    "\n",
    "    train_dataloader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "    valid_dataloader = DataLoader(valid_set, batch_size=len(valid_set))\n",
    "\n",
    "    # set up model\n",
    "    input_sizes = (train_set[0][0]['audio'].shape[0], train_set[0][0]['vision'].shape[0],\n",
    "                   train_set[0][0]['text'].shape[1])\n",
    "    hidden_sizes = (32, 32, 128)\n",
    "    output_size = 1\n",
    "\n",
    "    # model = CP_Tensor_Fusion_Network(input_sizes, hidden_sizes, output_size, max_rank,\n",
    "    #                                 rank_adaptive)\n",
    "    \n",
    "    model = Old_CP_Tensor_Fusion_Network(input_sizes, hidden_sizes, output_size, max_rank)\n",
    "    \n",
    "    # set up training\n",
    "    DTYPE = torch.FloatTensor\n",
    "    optimizer = optim.Adam(list(model.parameters()), lr=lr)\n",
    "    criterion = nn.MSELoss(size_average=False)\n",
    "    \n",
    "    # train and validate\n",
    "    for e in range(1, epochs + 1):\n",
    "        # train\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        for batch in train_dataloader:\n",
    "            model.zero_grad()\n",
    "\n",
    "            features, label = batch\n",
    "            x_a = Variable(features['audio'].float().type(DTYPE), requires_grad=False)\n",
    "            x_v = Variable(features['vision'].float().type(DTYPE), requires_grad=False)\n",
    "            x_t = Variable(features['text'].float().type(DTYPE), requires_grad=False)\n",
    "            y = Variable(label.view(-1, 1).float().type(DTYPE), requires_grad=False)\n",
    "\n",
    "            output = model([x_a, x_v, x_t])\n",
    "\n",
    "            loss = criterion(output, y)\n",
    "            \n",
    "            # rank loss for adaptive-rank model\n",
    "            if rank_adaptive:\n",
    "                loss += get_kl_loss(model, kl_multiplier, no_kl_epochs, warmup_epochs, e)\n",
    "            \n",
    "            train_loss += loss.item() / len(train_set)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        print(\"Epoch {}\".format(e))\n",
    "        print(\"Training Loss {:.4f}\".format(train_loss))\n",
    "        \n",
    "        # validate\n",
    "        model.eval()\n",
    "        for batch in valid_dataloader:\n",
    "            features, label = batch\n",
    "            x_a = Variable(features['audio'].float().type(DTYPE), requires_grad=False)\n",
    "            x_v = Variable(features['vision'].float().type(DTYPE), requires_grad=False)\n",
    "            x_t = Variable(features['text'].float().type(DTYPE), requires_grad=False)\n",
    "            y = Variable(label.view(-1, 1).float().type(DTYPE), requires_grad=False)\n",
    "\n",
    "            output = model([x_a, x_v, x_t])\n",
    "        \n",
    "        valid_error = nn.functional.mse_loss(y, output)\n",
    "        print(\"Validation MSE {:4f}\".format(valid_error.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "9dc1552b-948b-491d-9fc5-40965ca0dd48",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "linear(): argument 'input' (position 1) must be Tensor, not list",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/xr/p4pqzm995nbcl9w6d8lrb5zh0000gn/T/ipykernel_13693/1519775591.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m train_CMU_mosi(batch_size=32, epochs=100, lr=.001, max_rank=20, rank_adaptive=True,  \n\u001b[0m\u001b[1;32m      2\u001b[0m                    warmup_epochs=50, kl_multiplier=1e-4, no_kl_epochs=5)\n",
      "\u001b[0;32m/var/folders/xr/p4pqzm995nbcl9w6d8lrb5zh0000gn/T/ipykernel_13693/2491498774.py\u001b[0m in \u001b[0;36mtrain_CMU_mosi\u001b[0;34m(batch_size, epochs, lr, max_rank, rank_adaptive, warmup_epochs, kl_multiplier, no_kl_epochs)\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDTYPE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequires_grad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx_a\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_v\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_t\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/tensor_fusion/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/xr/p4pqzm995nbcl9w6d8lrb5zh0000gn/T/ipykernel_13693/107090118.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0mz_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDTYPE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequires_grad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor_fusion_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mz_audio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz_video\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz_text\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/tensor_fusion/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Projects/bayesian-tensor-rank-determination/tensor_layers/tensor_layers/layers.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, rank_update)\u001b[0m\n\u001b[1;32m     37\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_rank_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_full\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mout_features\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0min_features\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mupdate_rank_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/tensor_fusion/lib/python3.9/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1846\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhas_torch_function_variadic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1847\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1848\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1850\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: linear(): argument 'input' (position 1) must be Tensor, not list"
     ]
    }
   ],
   "source": [
    "train_CMU_mosi(batch_size=32, epochs=100, lr=.001, max_rank=20, rank_adaptive=True,  \n",
    "                   warmup_epochs=50, kl_multiplier=1e-4, no_kl_epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "176021d0-2b99-4c1e-9763-ad1feffb6ea7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "Training Loss 2.1427\n",
      "Validation MSE 0.009098\n",
      "Epoch 2\n",
      "Training Loss 1.5855\n",
      "Validation MSE 0.008205\n",
      "Epoch 3\n",
      "Training Loss 1.3448\n",
      "Validation MSE 0.006969\n",
      "Epoch 4\n",
      "Training Loss 1.1076\n",
      "Validation MSE 0.006593\n",
      "Epoch 5\n",
      "Training Loss 0.9306\n",
      "Validation MSE 0.006355\n",
      "Epoch 6\n",
      "Training Loss 0.8845\n",
      "Validation MSE 0.006940\n",
      "Epoch 7\n",
      "Training Loss 0.7350\n",
      "Validation MSE 0.006779\n",
      "Epoch 8\n",
      "Training Loss 0.6082\n",
      "Validation MSE 0.006832\n",
      "Epoch 9\n",
      "Training Loss 0.5367\n",
      "Validation MSE 0.006847\n",
      "Epoch 10\n",
      "Training Loss 0.4709\n",
      "Validation MSE 0.007109\n",
      "Epoch 11\n",
      "Training Loss 0.4176\n",
      "Validation MSE 0.006958\n",
      "Epoch 12\n",
      "Training Loss 0.3145\n",
      "Validation MSE 0.007326\n",
      "Epoch 13\n",
      "Training Loss 0.2755\n",
      "Validation MSE 0.006999\n",
      "Epoch 14\n",
      "Training Loss 0.2453\n",
      "Validation MSE 0.007049\n",
      "Epoch 15\n",
      "Training Loss 0.2315\n",
      "Validation MSE 0.007468\n",
      "Epoch 16\n",
      "Training Loss 0.2451\n",
      "Validation MSE 0.006876\n",
      "Epoch 17\n",
      "Training Loss 0.1809\n",
      "Validation MSE 0.007385\n",
      "Epoch 18\n",
      "Training Loss 0.1557\n",
      "Validation MSE 0.007211\n",
      "Epoch 19\n",
      "Training Loss 0.1444\n",
      "Validation MSE 0.007193\n",
      "Epoch 20\n",
      "Training Loss 0.1511\n",
      "Validation MSE 0.006888\n",
      "Epoch 21\n",
      "Training Loss 0.1314\n",
      "Validation MSE 0.006823\n",
      "Epoch 22\n",
      "Training Loss 0.1233\n",
      "Validation MSE 0.007060\n",
      "Epoch 23\n",
      "Training Loss 0.1180\n",
      "Validation MSE 0.006642\n",
      "Epoch 24\n",
      "Training Loss 0.0988\n",
      "Validation MSE 0.006955\n",
      "Epoch 25\n",
      "Training Loss 0.0951\n",
      "Validation MSE 0.006986\n",
      "Epoch 26\n",
      "Training Loss 0.0852\n",
      "Validation MSE 0.007194\n",
      "Epoch 27\n",
      "Training Loss 0.0816\n",
      "Validation MSE 0.006910\n",
      "Epoch 28\n",
      "Training Loss 0.0847\n",
      "Validation MSE 0.006791\n",
      "Epoch 29\n",
      "Training Loss 0.0841\n",
      "Validation MSE 0.006853\n",
      "Epoch 30\n",
      "Training Loss 0.0751\n",
      "Validation MSE 0.006909\n",
      "Epoch 31\n",
      "Training Loss 0.0725\n",
      "Validation MSE 0.006641\n",
      "Epoch 32\n",
      "Training Loss 0.0703\n",
      "Validation MSE 0.006808\n",
      "Epoch 33\n",
      "Training Loss 0.0702\n",
      "Validation MSE 0.006892\n",
      "Epoch 34\n",
      "Training Loss 0.0813\n",
      "Validation MSE 0.006916\n",
      "Epoch 35\n",
      "Training Loss 0.0750\n",
      "Validation MSE 0.006788\n",
      "Epoch 36\n",
      "Training Loss 0.0718\n",
      "Validation MSE 0.007048\n",
      "Epoch 37\n",
      "Training Loss 0.0621\n",
      "Validation MSE 0.006578\n",
      "Epoch 38\n",
      "Training Loss 0.0623\n",
      "Validation MSE 0.006763\n",
      "Epoch 39\n",
      "Training Loss 0.0582\n",
      "Validation MSE 0.006580\n",
      "Epoch 40\n",
      "Training Loss 0.0626\n",
      "Validation MSE 0.006512\n",
      "Epoch 41\n",
      "Training Loss 0.0539\n",
      "Validation MSE 0.006797\n",
      "Epoch 42\n",
      "Training Loss 0.0639\n",
      "Validation MSE 0.006842\n",
      "Epoch 43\n",
      "Training Loss 0.0630\n",
      "Validation MSE 0.006600\n",
      "Epoch 44\n",
      "Training Loss 0.0589\n",
      "Validation MSE 0.006663\n",
      "Epoch 45\n",
      "Training Loss 0.0534\n",
      "Validation MSE 0.006830\n",
      "Epoch 46\n",
      "Training Loss 0.0541\n",
      "Validation MSE 0.006811\n",
      "Epoch 47\n",
      "Training Loss 0.0555\n",
      "Validation MSE 0.006762\n",
      "Epoch 48\n",
      "Training Loss 0.0575\n",
      "Validation MSE 0.006460\n",
      "Epoch 49\n",
      "Training Loss 0.0609\n",
      "Validation MSE 0.006861\n",
      "Epoch 50\n",
      "Training Loss 0.0645\n",
      "Validation MSE 0.006768\n",
      "Epoch 51\n",
      "Training Loss 0.0523\n",
      "Validation MSE 0.006590\n",
      "Epoch 52\n",
      "Training Loss 0.0555\n",
      "Validation MSE 0.006778\n",
      "Epoch 53\n",
      "Training Loss 0.0552\n",
      "Validation MSE 0.006791\n",
      "Epoch 54\n",
      "Training Loss 0.0496\n",
      "Validation MSE 0.006871\n",
      "Epoch 55\n",
      "Training Loss 0.0416\n",
      "Validation MSE 0.006607\n",
      "Epoch 56\n",
      "Training Loss 0.0526\n",
      "Validation MSE 0.006864\n",
      "Epoch 57\n",
      "Training Loss 0.0519\n",
      "Validation MSE 0.006606\n",
      "Epoch 58\n",
      "Training Loss 0.0507\n",
      "Validation MSE 0.006809\n",
      "Epoch 59\n",
      "Training Loss 0.0476\n",
      "Validation MSE 0.006735\n",
      "Epoch 60\n",
      "Training Loss 0.0469\n",
      "Validation MSE 0.006666\n",
      "Epoch 61\n",
      "Training Loss 0.0521\n",
      "Validation MSE 0.006594\n",
      "Epoch 62\n",
      "Training Loss 0.0454\n",
      "Validation MSE 0.006617\n",
      "Epoch 63\n",
      "Training Loss 0.0424\n",
      "Validation MSE 0.006504\n",
      "Epoch 64\n",
      "Training Loss 0.0411\n",
      "Validation MSE 0.006518\n",
      "Epoch 65\n",
      "Training Loss 0.0476\n",
      "Validation MSE 0.006601\n",
      "Epoch 66\n",
      "Training Loss 0.0461\n",
      "Validation MSE 0.006503\n",
      "Epoch 67\n",
      "Training Loss 0.0427\n",
      "Validation MSE 0.006762\n",
      "Epoch 68\n",
      "Training Loss 0.0390\n",
      "Validation MSE 0.006428\n",
      "Epoch 69\n",
      "Training Loss 0.0397\n",
      "Validation MSE 0.006744\n",
      "Epoch 70\n",
      "Training Loss 0.0417\n",
      "Validation MSE 0.006752\n",
      "Epoch 71\n",
      "Training Loss 0.0444\n",
      "Validation MSE 0.006526\n",
      "Epoch 72\n",
      "Training Loss 0.0449\n",
      "Validation MSE 0.006662\n",
      "Epoch 73\n",
      "Training Loss 0.0419\n",
      "Validation MSE 0.006689\n",
      "Epoch 74\n",
      "Training Loss 0.0395\n",
      "Validation MSE 0.006606\n",
      "Epoch 75\n",
      "Training Loss 0.0430\n",
      "Validation MSE 0.006658\n",
      "Epoch 76\n",
      "Training Loss 0.0443\n",
      "Validation MSE 0.006476\n",
      "Epoch 77\n",
      "Training Loss 0.0414\n",
      "Validation MSE 0.006622\n",
      "Epoch 78\n",
      "Training Loss 0.0377\n",
      "Validation MSE 0.006814\n",
      "Epoch 79\n",
      "Training Loss 0.0384\n",
      "Validation MSE 0.006670\n",
      "Epoch 80\n",
      "Training Loss 0.0358\n",
      "Validation MSE 0.006714\n",
      "Epoch 81\n",
      "Training Loss 0.0357\n",
      "Validation MSE 0.006827\n",
      "Epoch 82\n",
      "Training Loss 0.0427\n",
      "Validation MSE 0.006750\n",
      "Epoch 83\n",
      "Training Loss 0.0421\n",
      "Validation MSE 0.006613\n",
      "Epoch 84\n",
      "Training Loss 0.0417\n",
      "Validation MSE 0.006714\n",
      "Epoch 85\n",
      "Training Loss 0.0397\n",
      "Validation MSE 0.006670\n",
      "Epoch 86\n",
      "Training Loss 0.0417\n",
      "Validation MSE 0.006705\n",
      "Epoch 87\n",
      "Training Loss 0.0413\n",
      "Validation MSE 0.006712\n",
      "Epoch 88\n",
      "Training Loss 0.0378\n",
      "Validation MSE 0.006673\n",
      "Epoch 89\n",
      "Training Loss 0.0387\n",
      "Validation MSE 0.006465\n",
      "Epoch 90\n",
      "Training Loss 0.0347\n",
      "Validation MSE 0.006451\n",
      "Epoch 91\n",
      "Training Loss 0.0318\n",
      "Validation MSE 0.006681\n",
      "Epoch 92\n",
      "Training Loss 0.0346\n",
      "Validation MSE 0.006520\n",
      "Epoch 93\n",
      "Training Loss 0.0363\n",
      "Validation MSE 0.006615\n",
      "Epoch 94\n",
      "Training Loss 0.0346\n",
      "Validation MSE 0.006593\n",
      "Epoch 95\n",
      "Training Loss 0.0361\n",
      "Validation MSE 0.006570\n",
      "Epoch 96\n",
      "Training Loss 0.0321\n",
      "Validation MSE 0.006673\n",
      "Epoch 97\n",
      "Training Loss 0.0336\n",
      "Validation MSE 0.006870\n",
      "Epoch 98\n",
      "Training Loss 0.0401\n",
      "Validation MSE 0.006525\n",
      "Epoch 99\n",
      "Training Loss 0.0352\n",
      "Validation MSE 0.006396\n",
      "Epoch 100\n",
      "Training Loss 0.0369\n",
      "Validation MSE 0.006496\n"
     ]
    }
   ],
   "source": [
    "train_CMU_mosi(batch_size=32, epochs=100, lr=.001, max_rank=20, rank_adaptive=False,  \n",
    "                   warmup_epochs=50, kl_multiplier=1e-4, no_kl_epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c74f6a4e-6c3e-4181-8532-52f070a83afc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
