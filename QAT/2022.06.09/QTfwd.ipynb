{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import qtorch\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.is_available = lambda : False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No CUDA runtime is found, using CUDA_HOME='/usr'\n"
     ]
    }
   ],
   "source": [
    "from qtorch import FloatingPoint\n",
    "from qtorch.quant import Quantizer\n",
    "\n",
    "# define floating point format\n",
    "bit_8 = FloatingPoint(exp=5, man=2)\n",
    "# create a quantizer\n",
    "factor_Q = Quantizer(forward_number=bit_8, forward_rounding=\"nearest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class QTensorFusion(nn.Module):\n",
    "\n",
    "    def __init__(self, input_sizes, output_size, dropout=0.0, bias=True, device=None, dtype=None):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.input_sizes = input_sizes\n",
    "        self.output_size = output_size\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # initialize weight tensor\n",
    "        tensorized_shape = input_sizes + (output_size,)\n",
    "        self.weight_tensor = nn.Parameter(torch.empty(tensorized_shape, device=device, dtype=dtype))\n",
    "        nn.init.xavier_normal_(self.weight_tensor)\n",
    "\n",
    "        # initialize bias\n",
    "        if bias:\n",
    "            self.bias = nn.Parameter(torch.full((output_size,), 0.1, device=device, dtype=dtype))\n",
    "        else:\n",
    "            self.bias = None\n",
    "\n",
    "    def forward(self, inputs):\n",
    "\n",
    "        fusion_tensor = inputs[0]\n",
    "        for x in inputs[1:]:\n",
    "            fusion_tensor = factor_Q(torch.einsum('n...,na->n...a', fusion_tensor, x))\n",
    "        \n",
    "        fusion_tensor = self.dropout(fusion_tensor)\n",
    "\n",
    "        output = factor_Q(torch.einsum('n...,...o->no', fusion_tensor, self.weight_tensor))\n",
    "\n",
    "        if self.bias is not None:\n",
    "            output = factor_Q(output + self.bias)\n",
    "        \n",
    "\n",
    "        output = F.relu(output)\n",
    "        output = self.dropout(output)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/christian_lee/anaconda3/envs/tensor_fusion/lib/python3.8/site-packages/tltorch/factorized_tensors/core.py:145: UserWarning: Creating a subclass of FactorizedTensor TensorizedTensor with no name.\n",
      "  warnings.warn(f'Creating a subclass of FactorizedTensor {cls.__name__} with no name.')\n"
     ]
    }
   ],
   "source": [
    "from tensor_fusion.module import TensorFusion\n",
    "\n",
    "fusion_layer = TensorFusion((10, 20, 30), 10)\n",
    "x1 = torch.rand((4,10))\n",
    "x2 = torch.rand((4,20))\n",
    "x3 = torch.rand((4,30))\n",
    "y = fusion_layer([x1, x2, x3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000, 0.0860, 0.0023, 0.0323, 0.0000, 0.0000, 0.0000, 0.2301, 0.0045,\n",
       "         0.3891],\n",
       "        [0.0000, 0.3390, 0.1816, 0.0022, 0.0000, 0.0000, 0.1642, 0.1613, 0.0999,\n",
       "         0.0976],\n",
       "        [0.1569, 0.3022, 0.2595, 0.0000, 0.1094, 0.0000, 0.2626, 0.0370, 0.2038,\n",
       "         0.2950],\n",
       "        [0.3092, 0.5297, 0.3148, 0.0000, 0.2240, 0.0000, 0.0000, 0.4008, 0.0000,\n",
       "         0.0000]], grad_fn=<ReluBackward0>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_fusion_layer = QTensorFusion((10, 20, 30), 10)\n",
    "q_fusion_layer.weight_tensor = fusion_layer.weight_tensor\n",
    "q_fusion_layer.bias = fusion_layer.bias\n",
    "x1 = torch.rand((4,10))\n",
    "x2 = torch.rand((4,20))\n",
    "x3 = torch.rand((4,30))\n",
    "y = fusion_layer([x1, x2, x3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000, 0.1949, 0.1740, 0.0000, 0.0779, 0.2369, 0.1970, 0.3585, 0.0000,\n",
       "         0.1788],\n",
       "        [0.1584, 0.0937, 0.0861, 0.1034, 0.0461, 0.0000, 0.1266, 0.0000, 0.0000,\n",
       "         0.3539],\n",
       "        [0.0000, 0.2110, 0.2696, 0.0000, 0.0000, 0.0000, 0.0700, 0.2734, 0.3019,\n",
       "         0.2641],\n",
       "        [0.1048, 0.2091, 0.1398, 0.2077, 0.3056, 0.1765, 0.2426, 0.1022, 0.0000,\n",
       "         0.2376]], grad_fn=<ReluBackward0>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def tt_times_matrix_fwd(tensor, matrix, return_saved_tensors):\n",
    "    \"\"\"\n",
    "    This function takes the input tensor \"tensor\", the input matrix \"matrix\"\n",
    "    and returns tensor times matrix as well as any extra tensors you decide to save\n",
    "    for the backward pass\n",
    "    \"\"\"\n",
    "    #Author Alvin Liu\n",
    "\n",
    "    ndims = tensor.order\n",
    "    d = int(ndims / 2)\n",
    "    tt_shape = tensor.shape\n",
    "    tt_ranks = tensor.rank[1:-1]\n",
    "    tt_shape_row = tt_shape[:d]\n",
    "    tt_shape_col = tt_shape[d:]\n",
    "    tt_rows = np.prod(tt_shape_row)\n",
    "    tt_cols = np.prod(tt_shape_col)\n",
    "    matrix_rows = matrix.shape[0]\n",
    "    matrix_cols = matrix.shape[1]\n",
    "    if tt_rows is not None and matrix_rows is not None:\n",
    "        if tt_rows != matrix_rows:\n",
    "            raise ValueError(\n",
    "                'Arguments shapes should align got %s and %s instead.' %\n",
    "                ((tt_rows, tt_cols), (matrix_rows, matrix_cols)))\n",
    "\n",
    "    # Matrix: M * K, tensor: M * N = (i_0, i_1, ..., i_d-1) * (j_0, j_1, ..., j_d-1)\n",
    "    # The shape of data is 1 * i_0 * (i_1, i_2, ..., i_d-1, K)\n",
    "    data = matrix\n",
    "    data = data.reshape(1, tt_shape_row[0], -1)\n",
    "    saved_tensors = [matrix] if return_saved_tensors else None\n",
    "\n",
    "    for k in range(d):\n",
    "        # The shape of data is r_k * i_k * (i_k+1, ..., i_d-1, K)\n",
    "        # The shape of curr_core (core_k) is r_k * i_k * r_k+1\n",
    "        # After einsum() the shape of data is r_k+1 * (i_k+1, ..., i_d-1, K)\n",
    "        curr_core = tensor.factors[k]\n",
    "        data = torch.einsum('ria, rib->ba', [data, curr_core])\n",
    "\n",
    "        if k < d - 1:\n",
    "            # After reshape the data, the shape is r_k+1 * i_k+1 * (i_k+2, ..., i_d-1, K)\n",
    "            data = data.reshape(tt_ranks[k], tt_shape_row[k + 1], -1)\n",
    "\n",
    "        saved_tensors.append(data) if return_saved_tensors else None\n",
    "\n",
    "    # Now the shape of data is r_d * K\n",
    "    for k in range(d):\n",
    "        # The shape of data is r_d+k * (K, j_0, ..., j_k-1)\n",
    "        # The shape of curr_core (core_d+k) is r_d+k * j_k * r_d+k+1\n",
    "        # After einsum() the shape of data is r_d+k+1 * (K, j_0, ..., j_k-1) * j_k\n",
    "        curr_core = tensor.factors[k + d]\n",
    "        data = torch.einsum('ra, rjb->baj', [data, curr_core])\n",
    "\n",
    "        if k < d - 1:\n",
    "            saved_tensors.append(data.reshape(data.shape[0], matrix_cols, -1)) if return_saved_tensors else None\n",
    "            # After reshape the data, the shape is r_d+k+1 * (K, j_0, ..., j_k)\n",
    "            data = data.reshape(tt_ranks[k + d], -1)\n",
    "\n",
    "    # The shape of data is 1 * (K, j_0, ..., j_d-2) * j_d-1\n",
    "    # The shape of output is K * (j_0, ..., j_d-1)\n",
    "    output = data.reshape(matrix_cols, tt_cols)\n",
    "\n",
    "    if return_saved_tensors:\n",
    "        return output, saved_tensors\n",
    "    else:\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensor_fusion.low_rank_tensor import TT\n",
    "weight_tensor = TT(128, 64, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix = torch.rand(5, 128)\n",
    "out = tt_times_matrix_fwd(weight_tensor.tensor, matrix.T, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-2.8147e-01, -4.6852e-01,  2.6628e+00, -7.1536e-01, -9.8626e-02,\n",
       "          4.9763e-01,  2.9270e-01, -7.0394e-01, -3.2957e-01, -4.6288e-01,\n",
       "          1.6270e-01, -3.6290e-01,  2.3649e-01,  7.0094e-01,  1.3807e+00,\n",
       "          3.5507e-01, -7.4049e-02, -4.8794e-01, -2.2668e-01, -7.4344e-01,\n",
       "          1.1270e-01,  3.2861e-01,  3.2756e-01, -2.7821e-01, -4.0464e-01,\n",
       "         -3.8620e-02,  1.1700e+00, -5.8933e-01, -3.2322e-01, -5.5988e-01,\n",
       "          1.0103e-01, -5.2083e-01, -4.8731e-01, -3.8730e-01,  1.1940e+00,\n",
       "         -6.7260e-01,  9.0283e-02,  1.4722e-01, -1.1062e+00, -1.0455e-01,\n",
       "          2.5446e-01, -1.6950e-01, -7.0476e-01,  1.8621e-01, -2.3740e-01,\n",
       "         -6.5926e-01,  1.5572e-01, -5.6310e-01,  8.0796e-01,  9.5150e-01,\n",
       "         -8.8637e-01,  1.2115e+00, -4.0965e-01, -1.4050e-01,  2.5246e+00,\n",
       "         -1.0110e-01, -1.0198e+00, -1.7574e-01,  2.3754e+00, -1.0628e+00,\n",
       "          5.5995e-01,  1.1560e+00, -2.2987e-01,  1.0955e+00],\n",
       "        [-1.8364e-01, -2.5672e-01,  8.0541e-01, -3.4443e-01,  5.7230e-02,\n",
       "          1.7397e-01, -2.3330e-01, -2.0730e-01, -8.8104e-02, -1.2647e-01,\n",
       "         -9.6425e-02, -5.3798e-02, -2.1412e-02,  1.0190e-01,  5.2728e-01,\n",
       "         -6.4927e-02,  2.8561e-02,  7.5569e-02,  2.1497e-01,  1.0628e-01,\n",
       "          2.2593e-03,  5.8520e-02,  2.4494e-01, -1.2966e-01, -3.9358e-01,\n",
       "         -1.3682e-01,  5.2340e-01, -3.1081e-01,  7.1557e-02,  3.3019e-01,\n",
       "          3.7227e-01,  1.2096e-01, -1.3362e-01, -5.1166e-01, -1.8403e-02,\n",
       "         -6.9643e-01,  9.2858e-02,  2.0197e-01, -1.6839e-01, -1.0591e-01,\n",
       "          2.4870e-01,  1.1923e-01, -1.2268e-01,  8.2198e-02, -2.4264e-01,\n",
       "         -5.3759e-01, -6.2485e-02, -3.8294e-01,  1.4447e-01,  2.5621e-01,\n",
       "          9.8673e-02,  2.1480e-01, -1.6283e-01, -3.7823e-03,  6.4992e-01,\n",
       "         -7.5169e-02, -2.9828e-01, -1.4809e-01,  7.6294e-01, -3.9219e-01,\n",
       "          1.2481e-01,  1.0468e-01, -1.6515e-01,  1.9784e-01],\n",
       "        [-7.9821e-02, -3.9520e-01,  3.8239e-01, -6.8798e-01, -8.0169e-02,\n",
       "          2.2765e-01,  2.5839e-01, -1.8534e-01,  2.2570e-01, -7.2302e-05,\n",
       "          1.8725e-01, -9.7233e-02, -1.4314e-01, -5.9682e-01, -2.8921e-01,\n",
       "         -2.4347e-01,  3.6879e-01,  8.1017e-01,  4.5031e-01,  1.0443e+00,\n",
       "         -1.7456e-01,  2.2861e-02,  1.5326e+00, -3.6253e-01, -1.6592e+00,\n",
       "         -5.6797e-01,  2.4290e+00, -1.3518e+00,  4.8910e-01,  1.5277e+00,\n",
       "          1.0275e+00,  8.3085e-01, -5.0966e-01, -8.4554e-01,  7.9353e-01,\n",
       "         -1.0003e+00,  3.7057e-01,  3.3112e-01, -1.2478e+00, -2.3884e-01,\n",
       "          1.0734e-01,  6.5676e-03, -7.4573e-01,  2.4084e-01, -3.6118e-01,\n",
       "         -2.9619e-01,  9.4175e-01, -6.4462e-01,  6.4017e-01,  8.7018e-01,\n",
       "         -1.1589e+00,  1.0813e+00, -2.6152e-01, -1.5569e-01,  1.9347e+00,\n",
       "         -2.2876e-02, -1.1181e+00, -1.8253e-01,  2.4145e+00, -1.1046e+00,\n",
       "          3.3083e-01,  7.4190e-01, -2.7536e-01,  6.4677e-01],\n",
       "        [ 3.4065e-02, -4.4747e-01, -5.4554e-01, -3.3834e-01,  2.4700e-01,\n",
       "          1.9259e-01,  6.4859e-02, -1.3970e-01, -1.6243e-01,  1.9166e-01,\n",
       "          1.5456e-01, -1.9129e-02, -1.2741e-01,  2.9778e-01,  6.5236e-01,\n",
       "         -7.3180e-02, -1.7088e-02, -3.1656e-02,  2.9855e-02, -5.3866e-02,\n",
       "          7.1667e-02,  1.1857e-01,  1.8446e-01, -1.6420e-01, -5.4472e-01,\n",
       "         -1.4553e-01,  8.4171e-01, -4.7270e-01, -5.6248e-02,  1.2937e-01,\n",
       "          3.9500e-01, -1.0162e-01, -2.6108e-01, -2.5993e-01,  8.4864e-01,\n",
       "         -2.8248e-01,  1.3261e-01,  1.2520e-01, -6.2976e-01, -1.4576e-01,\n",
       "         -1.9968e-02, -1.0519e-01, -4.9518e-01,  1.2781e-01, -3.2518e-02,\n",
       "          1.8601e-01,  6.8629e-01, -9.8811e-02,  1.2308e-01,  1.6807e-01,\n",
       "          4.4650e-01,  1.7632e-01, -1.3828e-01,  5.2133e-02,  6.2855e-01,\n",
       "         -1.6105e-01, -2.8626e-01, -1.5885e-01,  5.2872e-01, -3.0386e-01,\n",
       "          2.0565e-01,  4.3111e-01,  2.1295e-01,  3.6029e-01],\n",
       "        [-1.8391e-01,  1.7005e-01,  1.6664e+00,  1.6950e-01, -7.1417e-02,\n",
       "          7.8000e-02, -1.5904e-01, -2.6162e-01, -3.9823e-01, -4.1066e-01,\n",
       "         -1.7206e-02, -2.1332e-01,  2.6565e-01,  7.1192e-01,  8.8419e-01,\n",
       "          3.4139e-01, -2.0041e-01, -5.1880e-01, -1.8866e-01, -7.2623e-01,\n",
       "          6.4358e-02,  5.0484e-02, -6.7878e-01,  1.3859e-01,  8.9760e-01,\n",
       "          2.9427e-01, -1.1253e+00,  6.3828e-01, -3.0115e-01, -9.8349e-01,\n",
       "         -6.4932e-01, -5.1052e-01,  8.5245e-02,  3.3339e-02, -3.4034e-01,\n",
       "         -3.8616e-02, -1.1806e-01, -7.9033e-02,  1.6381e-01,  1.3856e-01,\n",
       "          4.1306e-01,  1.3553e-01, -2.4361e-01,  2.0585e-01, -2.2245e-02,\n",
       "         -4.2289e-01, -6.9738e-01, -2.7718e-02,  5.6512e-02,  1.4942e-01,\n",
       "          9.4820e-02,  1.0384e-01, -1.2544e-01, -5.1490e-02,  2.1704e-01,\n",
       "          3.2245e-02,  8.2231e-02, -3.7015e-02,  5.8075e-02, -2.2248e-02,\n",
       "          6.8459e-02, -9.6705e-02, -3.1653e-01,  9.3232e-02]],\n",
       "       grad_fn=<ReshapeAliasBackward0>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def q_tt_times_matrix_fwd(tensor, matrix, return_saved_tensors):\n",
    "    \"\"\"\n",
    "    This function takes the input tensor \"tensor\", the input matrix \"matrix\"\n",
    "    and returns tensor times matrix as well as any extra tensors you decide to save\n",
    "    for the backward pass\n",
    "    \"\"\"\n",
    "    #Author Alvin Liu\n",
    "\n",
    "    ndims = tensor.order\n",
    "    d = int(ndims / 2)\n",
    "    tt_shape = tensor.shape\n",
    "    tt_ranks = tensor.rank[1:-1]\n",
    "    tt_shape_row = tt_shape[:d]\n",
    "    tt_shape_col = tt_shape[d:]\n",
    "    tt_rows = np.prod(tt_shape_row)\n",
    "    tt_cols = np.prod(tt_shape_col)\n",
    "    matrix_rows = matrix.shape[0]\n",
    "    matrix_cols = matrix.shape[1]\n",
    "    if tt_rows is not None and matrix_rows is not None:\n",
    "        if tt_rows != matrix_rows:\n",
    "            raise ValueError(\n",
    "                'Arguments shapes should align got %s and %s instead.' %\n",
    "                ((tt_rows, tt_cols), (matrix_rows, matrix_cols)))\n",
    "\n",
    "    # Matrix: M * K, tensor: M * N = (i_0, i_1, ..., i_d-1) * (j_0, j_1, ..., j_d-1)\n",
    "    # The shape of data is 1 * i_0 * (i_1, i_2, ..., i_d-1, K)\n",
    "    data = matrix\n",
    "    data = data.reshape(1, tt_shape_row[0], -1)\n",
    "    saved_tensors = [matrix] if return_saved_tensors else None\n",
    "\n",
    "    for k in range(d):\n",
    "        # The shape of data is r_k * i_k * (i_k+1, ..., i_d-1, K)\n",
    "        # The shape of curr_core (core_k) is r_k * i_k * r_k+1\n",
    "        # After einsum() the shape of data is r_k+1 * (i_k+1, ..., i_d-1, K)\n",
    "        curr_core = tensor.factors[k]\n",
    "        data = factor_Q(torch.einsum('ria, rib->ba', [data, curr_core]))\n",
    "\n",
    "        if k < d - 1:\n",
    "            # After reshape the data, the shape is r_k+1 * i_k+1 * (i_k+2, ..., i_d-1, K)\n",
    "            data = data.reshape(tt_ranks[k], tt_shape_row[k + 1], -1)\n",
    "\n",
    "        saved_tensors.append(data) if return_saved_tensors else None\n",
    "\n",
    "    # Now the shape of data is r_d * K\n",
    "    for k in range(d):\n",
    "        # The shape of data is r_d+k * (K, j_0, ..., j_k-1)\n",
    "        # The shape of curr_core (core_d+k) is r_d+k * j_k * r_d+k+1\n",
    "        # After einsum() the shape of data is r_d+k+1 * (K, j_0, ..., j_k-1) * j_k\n",
    "        curr_core = tensor.factors[k + d]\n",
    "        data = factor_Q(torch.einsum('ra, rjb->baj', [data, curr_core]))\n",
    "\n",
    "        if k < d - 1:\n",
    "            saved_tensors.append(data.reshape(data.shape[0], matrix_cols, -1)) if return_saved_tensors else None\n",
    "            # After reshape the data, the shape is r_d+k+1 * (K, j_0, ..., j_k)\n",
    "            data = data.reshape(tt_ranks[k + d], -1)\n",
    "\n",
    "    # The shape of data is 1 * (K, j_0, ..., j_d-2) * j_d-1\n",
    "    # The shape of output is K * (j_0, ..., j_d-1)\n",
    "    output = data.reshape(matrix_cols, tt_cols)\n",
    "\n",
    "    if return_saved_tensors:\n",
    "        return output, saved_tensors\n",
    "    else:\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = q_tt_times_matrix_fwd(weight_tensor.tensor, matrix.T, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-3.1250e-01, -4.3750e-01,  3.0000e+00, -6.2500e-01, -1.9531e-02,\n",
       "          5.0000e-01,  1.5625e-01, -6.2500e-01, -3.7500e-01, -4.3750e-01,\n",
       "          1.5625e-01, -3.7500e-01,  2.5000e-01,  7.5000e-01,  1.5000e+00,\n",
       "          3.1250e-01, -1.5625e-01, -6.2500e-01, -3.1250e-01, -8.7500e-01,\n",
       "          1.2500e-01,  3.7500e-01,  2.5000e-01, -3.1250e-01, -3.7500e-01,\n",
       "         -2.7344e-02,  1.0000e+00, -5.0000e-01, -4.3750e-01, -7.5000e-01,\n",
       "          6.2500e-02, -6.2500e-01, -5.0000e-01, -4.3750e-01,  1.2500e+00,\n",
       "         -8.7500e-01,  1.0938e-01,  1.5625e-01, -1.2500e+00, -1.2500e-01,\n",
       "          2.5000e-01, -1.8750e-01, -6.2500e-01,  1.5625e-01, -2.5000e-01,\n",
       "         -7.5000e-01,  6.2500e-02, -6.2500e-01,  7.5000e-01,  8.7500e-01,\n",
       "         -6.2500e-01,  1.2500e+00, -4.3750e-01, -1.8750e-01,  2.5000e+00,\n",
       "         -1.8750e-01, -1.0000e+00, -2.1875e-01,  2.5000e+00, -1.0000e+00,\n",
       "          5.0000e-01,  1.2500e+00, -1.0938e-01,  1.0000e+00],\n",
       "        [-2.1875e-01, -2.1875e-01,  1.0000e+00, -3.1250e-01,  1.9531e-02,\n",
       "          1.5625e-01, -2.1875e-01, -2.5000e-01, -1.5625e-01, -1.5625e-01,\n",
       "         -7.8125e-02, -7.8125e-02,  3.0518e-04,  1.8750e-01,  6.2500e-01,\n",
       "         -1.1719e-02,  1.5625e-02,  2.7344e-02,  1.8750e-01,  3.9062e-02,\n",
       "          4.8828e-03,  6.2500e-02,  1.5625e-01, -7.8125e-02, -2.5000e-01,\n",
       "         -1.0938e-01,  3.7500e-01, -2.5000e-01,  4.6875e-02,  2.1875e-01,\n",
       "          2.5000e-01,  6.2500e-02, -1.2500e-01, -5.0000e-01, -6.2500e-02,\n",
       "         -7.5000e-01,  9.3750e-02,  1.8750e-01, -2.5000e-01, -1.0938e-01,\n",
       "          3.1250e-01,  1.2500e-01, -2.1875e-01,  1.2500e-01, -2.5000e-01,\n",
       "         -6.2500e-01, -9.3750e-02, -4.3750e-01,  1.2500e-01,  2.5000e-01,\n",
       "          1.0938e-01,  2.1875e-01, -1.8750e-01, -2.7344e-02,  6.2500e-01,\n",
       "         -5.4688e-02, -2.5000e-01, -1.5625e-01,  6.2500e-01, -3.7500e-01,\n",
       "          1.2500e-01,  7.8125e-02, -2.1875e-01,  1.8750e-01],\n",
       "        [-6.2500e-02, -3.7500e-01,  3.7500e-01, -6.2500e-01, -6.2500e-02,\n",
       "          3.1250e-01,  3.1250e-01, -1.8750e-01,  1.8750e-01, -3.9062e-03,\n",
       "          2.1875e-01, -1.0938e-01, -1.2500e-01, -5.0000e-01, -1.5625e-01,\n",
       "         -2.1875e-01,  3.7500e-01,  7.5000e-01,  3.7500e-01,  1.0000e+00,\n",
       "         -1.2500e-01, -1.9531e-02,  1.5000e+00, -4.3750e-01, -1.7500e+00,\n",
       "         -5.0000e-01,  2.5000e+00, -1.2500e+00,  4.3750e-01,  1.5000e+00,\n",
       "          1.0000e+00,  7.5000e-01, -5.0000e-01, -8.7500e-01,  1.0000e+00,\n",
       "         -1.0000e+00,  3.1250e-01,  3.1250e-01, -1.2500e+00, -2.5000e-01,\n",
       "          9.3750e-02, -2.3438e-02, -7.5000e-01,  2.1875e-01, -3.1250e-01,\n",
       "         -2.1875e-01,  8.7500e-01, -5.0000e-01,  6.2500e-01,  8.7500e-01,\n",
       "         -1.0000e+00,  1.0000e+00, -2.5000e-01, -7.8125e-02,  2.0000e+00,\n",
       "          7.8125e-02, -1.0000e+00, -2.1875e-01,  2.0000e+00, -1.0000e+00,\n",
       "          3.7500e-01,  7.5000e-01, -2.5000e-01,  7.5000e-01],\n",
       "        [-1.9531e-02, -5.0000e-01, -6.2500e-01, -5.0000e-01,  3.1250e-01,\n",
       "          2.1875e-01, -3.1250e-02, -1.5625e-01, -1.0938e-01,  2.1875e-01,\n",
       "          3.1250e-02,  3.9062e-02, -1.8750e-01,  1.8750e-01,  6.2500e-01,\n",
       "         -1.5625e-01, -3.0518e-04,  3.9062e-02,  1.2500e-01,  4.6875e-02,\n",
       "          6.2500e-02,  1.0938e-01,  3.1250e-01, -2.1875e-01, -7.5000e-01,\n",
       "         -2.1875e-01,  1.0000e+00, -6.2500e-01, -6.8359e-03,  3.1250e-01,\n",
       "          5.0000e-01, -1.3672e-02, -2.5000e-01, -3.7500e-01,  7.5000e-01,\n",
       "         -4.3750e-01,  1.5625e-01,  1.5625e-01, -6.2500e-01, -2.1875e-01,\n",
       "         -4.6875e-02, -9.3750e-02, -3.1250e-01,  6.2500e-02, -9.3750e-02,\n",
       "          7.8125e-02,  6.2500e-01, -1.8750e-01,  1.0938e-01,  1.2500e-01,\n",
       "          5.0000e-01,  1.2500e-01, -1.2500e-01,  7.8125e-02,  6.2500e-01,\n",
       "         -1.5625e-01, -3.1250e-01, -1.5625e-01,  6.2500e-01, -3.7500e-01,\n",
       "          1.8750e-01,  4.3750e-01,  2.5000e-01,  3.1250e-01],\n",
       "        [-1.5625e-01,  1.8750e-01,  1.5000e+00,  1.8750e-01, -7.8125e-02,\n",
       "          3.1250e-02, -1.0938e-01, -2.1875e-01, -3.1250e-01, -3.1250e-01,\n",
       "         -7.8125e-02, -1.5625e-01,  2.5000e-01,  6.2500e-01,  7.5000e-01,\n",
       "          3.1250e-01, -1.8750e-01, -4.3750e-01, -1.2500e-01, -6.2500e-01,\n",
       "          4.6875e-02,  2.3438e-02, -6.2500e-01,  1.2500e-01,  8.7500e-01,\n",
       "          3.1250e-01, -1.2500e+00,  7.5000e-01, -2.5000e-01, -8.7500e-01,\n",
       "         -6.2500e-01, -4.3750e-01,  1.2500e-01,  5.4688e-02, -5.0000e-01,\n",
       "         -3.1250e-02, -1.5625e-01, -1.0938e-01,  3.1250e-01,  9.3750e-02,\n",
       "          3.7500e-01,  1.0938e-01, -5.4688e-02,  1.0938e-01, -2.3438e-02,\n",
       "         -5.0000e-01, -8.7500e-01, -2.3438e-02,  2.7344e-02,  7.8125e-02,\n",
       "          6.2500e-02,  3.9062e-02, -9.3750e-02, -3.9062e-02,  9.3750e-02,\n",
       "          3.9062e-02,  1.0938e-01, -1.3672e-02, -3.1250e-02,  1.9531e-02,\n",
       "          3.1250e-02, -1.2500e-01, -2.5000e-01,  3.1250e-02]],\n",
       "       grad_fn=<ReshapeAliasBackward0>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8d381c0d5fdc2ad83ad0668835ab5719aa4babc32c7351cb80dde318b7d35d97"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 ('tensor_fusion')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
