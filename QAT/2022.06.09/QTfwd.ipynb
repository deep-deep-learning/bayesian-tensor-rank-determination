{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import qtorch\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.is_available = lambda : False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No CUDA runtime is found, using CUDA_HOME='/usr'\n"
     ]
    }
   ],
   "source": [
    "from qtorch import FloatingPoint\n",
    "from qtorch.quant import Quantizer\n",
    "\n",
    "# define floating point format\n",
    "bit_8 = FloatingPoint(exp=5, man=2)\n",
    "# create a quantizer\n",
    "factor_Q = Quantizer(forward_number=bit_8, forward_rounding=\"nearest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class QTensorFusion(nn.Module):\n",
    "\n",
    "    def __init__(self, input_sizes, output_size, dropout=0.0, bias=True, device=None, dtype=None):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.input_sizes = input_sizes\n",
    "        self.output_size = output_size\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # initialize weight tensor\n",
    "        tensorized_shape = input_sizes + (output_size,)\n",
    "        self.weight_tensor = nn.Parameter(torch.empty(tensorized_shape, device=device, dtype=dtype))\n",
    "        nn.init.xavier_normal_(self.weight_tensor)\n",
    "\n",
    "        # initialize bias\n",
    "        if bias:\n",
    "            self.bias = nn.Parameter(torch.full((output_size,), 0.1, device=device, dtype=dtype))\n",
    "        else:\n",
    "            self.bias = None\n",
    "\n",
    "    def forward(self, inputs):\n",
    "\n",
    "        fusion_tensor = inputs[0]\n",
    "        for x in inputs[1:]:\n",
    "            fusion_tensor = factor_Q(torch.einsum('n...,na->n...a', fusion_tensor, x))\n",
    "        \n",
    "        fusion_tensor = self.dropout(fusion_tensor)\n",
    "\n",
    "        output = factor_Q(torch.einsum('n...,...o->no', fusion_tensor, self.weight_tensor))\n",
    "\n",
    "        if self.bias is not None:\n",
    "            output = factor_Q(output + self.bias)\n",
    "        \n",
    "\n",
    "        output = F.relu(output)\n",
    "        output = self.dropout(output)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensor_fusion.module import TensorFusion\n",
    "\n",
    "fusion_layer = TensorFusion((10, 20, 30), 10)\n",
    "x1 = torch.rand((4,10))\n",
    "x2 = torch.rand((4,20))\n",
    "x3 = torch.rand((4,30))\n",
    "y = fusion_layer([x1, x2, x3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1153, 0.1978, 0.0000, 0.0276, 0.2336, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.2107],\n",
       "        [0.0935, 0.0000, 0.0000, 0.0228, 0.1642, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000],\n",
       "        [0.0000, 0.0000, 0.0781, 0.0237, 0.0000, 0.1887, 0.0000, 0.0356, 0.0616,\n",
       "         0.0000],\n",
       "        [0.0000, 0.0000, 0.0676, 0.1536, 0.0000, 0.1821, 0.0000, 0.0000, 0.0000,\n",
       "         0.2708]], grad_fn=<ReluBackward0>)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_fusion_layer = QTensorFusion((10, 20, 30), 10)\n",
    "q_fusion_layer.weight_tensor = fusion_layer.weight_tensor\n",
    "q_fusion_layer.bias = fusion_layer.bias\n",
    "x1 = torch.rand((4,10))\n",
    "x2 = torch.rand((4,20))\n",
    "x3 = torch.rand((4,30))\n",
    "y = fusion_layer([x1, x2, x3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000, 0.0715, 0.0807, 0.0000, 0.0000, 0.1102, 0.2239, 0.0000, 0.2434,\n",
       "         0.1205],\n",
       "        [0.2324, 0.2206, 0.0000, 0.0000, 0.1947, 0.0288, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000],\n",
       "        [0.0900, 0.0000, 0.0000, 0.0990, 0.0000, 0.4148, 0.0000, 0.0000, 0.1515,\n",
       "         0.0943],\n",
       "        [0.0000, 0.0376, 0.1363, 0.2232, 0.2081, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000]], grad_fn=<ReluBackward0>)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8d381c0d5fdc2ad83ad0668835ab5719aa4babc32c7351cb80dde318b7d35d97"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 ('tensor_fusion')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
